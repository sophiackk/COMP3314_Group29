nohup: ignoring input
Seed: 42
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.5_alpha0.0_96_96Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.5_alpha0.0.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           96                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.0_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.9964172
	speed: 0.0425s/iter; left time: 86.2462s
	iters: 200, epoch: 1 | loss: 1.0330465
	speed: 0.0282s/iter; left time: 54.5035s
Epoch: 1 cost time: 5.949129581451416
Epoch: 1, Steps: 213 | Train Loss: 1.0345474 Vali Loss: 1.0402997 Test Loss: 1.0232433
Validation loss decreased (inf --> 1.040300).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0462786
	speed: 0.0141s/iter; left time: 25.7005s
	iters: 200, epoch: 2 | loss: 1.0339957
	speed: 0.0131s/iter; left time: 22.4785s
Epoch: 2 cost time: 2.871999502182007
Epoch: 2, Steps: 213 | Train Loss: 1.0237598 Vali Loss: 1.0402977 Test Loss: 1.0244259
Validation loss decreased (1.040300 --> 1.040298).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.9964721
	speed: 0.0196s/iter; left time: 31.4756s
	iters: 200, epoch: 3 | loss: 1.0178320
	speed: 0.0183s/iter; left time: 27.5638s
Epoch: 3 cost time: 3.9171648025512695
Epoch: 3, Steps: 213 | Train Loss: 1.0199568 Vali Loss: 1.0390313 Test Loss: 1.0247734
Validation loss decreased (1.040298 --> 1.039031).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0397471
	speed: 0.0170s/iter; left time: 23.7093s
	iters: 200, epoch: 4 | loss: 1.0203471
	speed: 0.0156s/iter; left time: 20.1915s
Epoch: 4 cost time: 3.415898561477661
Epoch: 4, Steps: 213 | Train Loss: 1.0171207 Vali Loss: 1.0398190 Test Loss: 1.0255948
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0346251
	speed: 0.0193s/iter; left time: 22.7374s
	iters: 200, epoch: 5 | loss: 1.0108280
	speed: 0.0180s/iter; left time: 19.4726s
Epoch: 5 cost time: 3.8326416015625
Epoch: 5, Steps: 213 | Train Loss: 1.0149089 Vali Loss: 1.0394393 Test Loss: 1.0263170
EarlyStopping counter: 2 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0281875
	speed: 0.0161s/iter; left time: 15.5352s
	iters: 200, epoch: 6 | loss: 1.0099145
	speed: 0.0136s/iter; left time: 11.7799s
Epoch: 6 cost time: 2.966071605682373
Epoch: 6, Steps: 213 | Train Loss: 1.0135093 Vali Loss: 1.0390725 Test Loss: 1.0269461
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0413451
	speed: 0.0127s/iter; left time: 9.5312s
	iters: 200, epoch: 7 | loss: 0.9967873
	speed: 0.0112s/iter; left time: 7.3142s
Epoch: 7 cost time: 2.5878283977508545
Epoch: 7, Steps: 213 | Train Loss: 1.0127612 Vali Loss: 1.0398943 Test Loss: 1.0271745
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 1.0045025
	speed: 0.0204s/iter; left time: 11.0169s
	iters: 200, epoch: 8 | loss: 1.0156951
	speed: 0.0174s/iter; left time: 7.6682s
Epoch: 8 cost time: 3.7262706756591797
Epoch: 8, Steps: 213 | Train Loss: 1.0125304 Vali Loss: 1.0397989 Test Loss: 1.0272793
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.0_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0247734785079956, mae:0.8073770403862
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.0_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0356368
	speed: 0.0165s/iter; left time: 33.5528s
	iters: 200, epoch: 1 | loss: 1.0404474
	speed: 0.0170s/iter; left time: 32.7679s
Epoch: 1 cost time: 3.7807605266571045
Epoch: 1, Steps: 213 | Train Loss: 1.0349532 Vali Loss: 1.0402147 Test Loss: 1.0227721
Validation loss decreased (inf --> 1.040215).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0164557
	speed: 0.0133s/iter; left time: 24.2064s
	iters: 200, epoch: 2 | loss: 1.0359726
	speed: 0.0125s/iter; left time: 21.4684s
Epoch: 2 cost time: 2.7661819458007812
Epoch: 2, Steps: 213 | Train Loss: 1.0249131 Vali Loss: 1.0390524 Test Loss: 1.0222285
Validation loss decreased (1.040215 --> 1.039052).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.9923510
	speed: 0.0168s/iter; left time: 27.0101s
	iters: 200, epoch: 3 | loss: 0.9921627
	speed: 0.0164s/iter; left time: 24.6473s
Epoch: 3 cost time: 3.5190560817718506
Epoch: 3, Steps: 213 | Train Loss: 1.0221864 Vali Loss: 1.0396885 Test Loss: 1.0222467
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0053129
	speed: 0.0191s/iter; left time: 26.5578s
	iters: 200, epoch: 4 | loss: 0.9849509
	speed: 0.0169s/iter; left time: 21.8478s
Epoch: 4 cost time: 3.6734721660614014
Epoch: 4, Steps: 213 | Train Loss: 1.0187292 Vali Loss: 1.0385662 Test Loss: 1.0233842
Validation loss decreased (1.039052 --> 1.038566).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.9794180
	speed: 0.0210s/iter; left time: 24.7999s
	iters: 200, epoch: 5 | loss: 1.0163996
	speed: 0.0202s/iter; left time: 21.7699s
Epoch: 5 cost time: 4.359449625015259
Epoch: 5, Steps: 213 | Train Loss: 1.0164478 Vali Loss: 1.0404363 Test Loss: 1.0234845
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.9817506
	speed: 0.0136s/iter; left time: 13.1109s
	iters: 200, epoch: 6 | loss: 1.0115006
	speed: 0.0135s/iter; left time: 11.6985s
Epoch: 6 cost time: 2.9239351749420166
Epoch: 6, Steps: 213 | Train Loss: 1.0151869 Vali Loss: 1.0417304 Test Loss: 1.0237539
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0222211
	speed: 0.0236s/iter; left time: 17.7859s
	iters: 200, epoch: 7 | loss: 1.0219550
	speed: 0.0227s/iter; left time: 14.8132s
Epoch: 7 cost time: 4.8289971351623535
Epoch: 7, Steps: 213 | Train Loss: 1.0141386 Vali Loss: 1.0407388 Test Loss: 1.0238528
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 1.0039592
	speed: 0.0213s/iter; left time: 11.5145s
	iters: 200, epoch: 8 | loss: 1.0314410
	speed: 0.0181s/iter; left time: 7.9668s
Epoch: 8 cost time: 3.916837692260742
Epoch: 8, Steps: 213 | Train Loss: 1.0135696 Vali Loss: 1.0393727 Test Loss: 1.0239686
EarlyStopping counter: 4 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 1.0150298
	speed: 0.0195s/iter; left time: 6.3780s
	iters: 200, epoch: 9 | loss: 1.0063536
	speed: 0.0176s/iter; left time: 4.0026s
Epoch: 9 cost time: 3.826510190963745
Epoch: 9, Steps: 213 | Train Loss: 1.0134971 Vali Loss: 1.0419980 Test Loss: 1.0240101
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.0_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0233843326568604, mae:0.80692058801651
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.0_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0280030
	speed: 0.0182s/iter; left time: 37.0556s
	iters: 200, epoch: 1 | loss: 1.0187466
	speed: 0.0151s/iter; left time: 29.1783s
Epoch: 1 cost time: 3.212538242340088
Epoch: 1, Steps: 213 | Train Loss: 1.0363720 Vali Loss: 1.0405480 Test Loss: 1.0236354
Validation loss decreased (inf --> 1.040548).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0357975
	speed: 0.0173s/iter; left time: 31.4985s
	iters: 200, epoch: 2 | loss: 1.0375289
	speed: 0.0161s/iter; left time: 27.6081s
Epoch: 2 cost time: 3.526947021484375
Epoch: 2, Steps: 213 | Train Loss: 1.0243176 Vali Loss: 1.0383905 Test Loss: 1.0225664
Validation loss decreased (1.040548 --> 1.038391).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0126340
	speed: 0.0177s/iter; left time: 28.4195s
	iters: 200, epoch: 3 | loss: 1.0273271
	speed: 0.0165s/iter; left time: 24.7975s
Epoch: 3 cost time: 3.559880018234253
Epoch: 3, Steps: 213 | Train Loss: 1.0203748 Vali Loss: 1.0381056 Test Loss: 1.0223237
Validation loss decreased (1.038391 --> 1.038106).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0129972
	speed: 0.0187s/iter; left time: 26.0326s
	iters: 200, epoch: 4 | loss: 1.0211608
	speed: 0.0171s/iter; left time: 22.0687s
Epoch: 4 cost time: 3.6313748359680176
Epoch: 4, Steps: 213 | Train Loss: 1.0172613 Vali Loss: 1.0377028 Test Loss: 1.0232778
Validation loss decreased (1.038106 --> 1.037703).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0065494
	speed: 0.0183s/iter; left time: 21.5195s
	iters: 200, epoch: 5 | loss: 1.0369310
	speed: 0.0146s/iter; left time: 15.7295s
Epoch: 5 cost time: 3.074892044067383
Epoch: 5, Steps: 213 | Train Loss: 1.0152260 Vali Loss: 1.0377325 Test Loss: 1.0241343
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0039804
	speed: 0.0211s/iter; left time: 20.3536s
	iters: 200, epoch: 6 | loss: 1.0492156
	speed: 0.0194s/iter; left time: 16.7833s
Epoch: 6 cost time: 4.22837233543396
Epoch: 6, Steps: 213 | Train Loss: 1.0140032 Vali Loss: 1.0377120 Test Loss: 1.0244825
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0073860
	speed: 0.0219s/iter; left time: 16.4672s
	iters: 200, epoch: 7 | loss: 1.0047455
	speed: 0.0189s/iter; left time: 12.3293s
Epoch: 7 cost time: 4.021318197250366
Epoch: 7, Steps: 213 | Train Loss: 1.0133143 Vali Loss: 1.0395936 Test Loss: 1.0245551
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 1.0085022
	speed: 0.0189s/iter; left time: 10.2267s
	iters: 200, epoch: 8 | loss: 1.0181818
	speed: 0.0166s/iter; left time: 7.3005s
Epoch: 8 cost time: 3.571157932281494
Epoch: 8, Steps: 213 | Train Loss: 1.0126965 Vali Loss: 1.0394617 Test Loss: 1.0247023
EarlyStopping counter: 4 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 1.0131791
	speed: 0.0194s/iter; left time: 6.3366s
	iters: 200, epoch: 9 | loss: 1.0020262
	speed: 0.0152s/iter; left time: 3.4434s
Epoch: 9 cost time: 3.2499802112579346
Epoch: 9, Steps: 213 | Train Loss: 1.0125830 Vali Loss: 1.0409274 Test Loss: 1.0247773
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.0_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0232778787612915, mae:0.806665301322937
Seed: 42
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.5_alpha0.0_96_192Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.5_alpha0.0.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           192                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.0_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0251603
	speed: 0.0402s/iter; left time: 80.4837s
	iters: 200, epoch: 1 | loss: 1.0305853
	speed: 0.0295s/iter; left time: 56.1665s
Epoch: 1 cost time: 6.092750787734985
Epoch: 1, Steps: 210 | Train Loss: 1.0376854 Vali Loss: 1.0476454 Test Loss: 1.0223302
Validation loss decreased (inf --> 1.047645).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0294108
	speed: 0.0266s/iter; left time: 47.5892s
	iters: 200, epoch: 2 | loss: 1.0239394
	speed: 0.0210s/iter; left time: 35.4418s
Epoch: 2 cost time: 4.331782102584839
Epoch: 2, Steps: 210 | Train Loss: 1.0289169 Vali Loss: 1.0479419 Test Loss: 1.0232126
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.9967018
	speed: 0.0175s/iter; left time: 27.7042s
	iters: 200, epoch: 3 | loss: 1.0078740
	speed: 0.0140s/iter; left time: 20.6705s
Epoch: 3 cost time: 3.0434558391571045
Epoch: 3, Steps: 210 | Train Loss: 1.0268047 Vali Loss: 1.0493010 Test Loss: 1.0236651
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0235937
	speed: 0.0248s/iter; left time: 34.0305s
	iters: 200, epoch: 4 | loss: 1.0227674
	speed: 0.0211s/iter; left time: 26.8120s
Epoch: 4 cost time: 4.6003258228302
Epoch: 4, Steps: 210 | Train Loss: 1.0248085 Vali Loss: 1.0488206 Test Loss: 1.0250049
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0298766
	speed: 0.0223s/iter; left time: 25.8955s
	iters: 200, epoch: 5 | loss: 1.0149096
	speed: 0.0217s/iter; left time: 23.0032s
Epoch: 5 cost time: 5.125818252563477
Epoch: 5, Steps: 210 | Train Loss: 1.0234914 Vali Loss: 1.0494288 Test Loss: 1.0251967
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0367321
	speed: 0.0194s/iter; left time: 18.4779s
	iters: 200, epoch: 6 | loss: 1.0250267
	speed: 0.0150s/iter; left time: 12.7964s
Epoch: 6 cost time: 3.26593017578125
Epoch: 6, Steps: 210 | Train Loss: 1.0225907 Vali Loss: 1.0500461 Test Loss: 1.0256960
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.0_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0223301649093628, mae:0.8059184551239014
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.0_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0471358
	speed: 0.0268s/iter; left time: 53.5275s
	iters: 200, epoch: 1 | loss: 1.0108074
	speed: 0.0219s/iter; left time: 41.5564s
Epoch: 1 cost time: 4.608291387557983
Epoch: 1, Steps: 210 | Train Loss: 1.0398079 Vali Loss: 1.0480251 Test Loss: 1.0227937
Validation loss decreased (inf --> 1.048025).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0291604
	speed: 0.0143s/iter; left time: 25.6496s
	iters: 200, epoch: 2 | loss: 1.0404299
	speed: 0.0128s/iter; left time: 21.7036s
Epoch: 2 cost time: 2.7711246013641357
Epoch: 2, Steps: 210 | Train Loss: 1.0295991 Vali Loss: 1.0478827 Test Loss: 1.0247605
Validation loss decreased (1.048025 --> 1.047883).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0059609
	speed: 0.0176s/iter; left time: 27.7763s
	iters: 200, epoch: 3 | loss: 1.0349613
	speed: 0.0177s/iter; left time: 26.2203s
Epoch: 3 cost time: 3.80993390083313
Epoch: 3, Steps: 210 | Train Loss: 1.0266497 Vali Loss: 1.0508419 Test Loss: 1.0264163
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0042620
	speed: 0.0134s/iter; left time: 18.3288s
	iters: 200, epoch: 4 | loss: 1.0096319
	speed: 0.0122s/iter; left time: 15.5457s
Epoch: 4 cost time: 2.65700364112854
Epoch: 4, Steps: 210 | Train Loss: 1.0234405 Vali Loss: 1.0529391 Test Loss: 1.0287412
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0122981
	speed: 0.0234s/iter; left time: 27.2238s
	iters: 200, epoch: 5 | loss: 1.0288136
	speed: 0.0183s/iter; left time: 19.4278s
Epoch: 5 cost time: 3.8367342948913574
Epoch: 5, Steps: 210 | Train Loss: 1.0212931 Vali Loss: 1.0517462 Test Loss: 1.0311685
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0170794
	speed: 0.0162s/iter; left time: 15.3842s
	iters: 200, epoch: 6 | loss: 1.0369015
	speed: 0.0134s/iter; left time: 11.3913s
Epoch: 6 cost time: 2.870941400527954
Epoch: 6, Steps: 210 | Train Loss: 1.0200391 Vali Loss: 1.0535066 Test Loss: 1.0321914
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0123060
	speed: 0.0148s/iter; left time: 10.9410s
	iters: 200, epoch: 7 | loss: 1.0281466
	speed: 0.0137s/iter; left time: 8.8135s
Epoch: 7 cost time: 2.973970651626587
Epoch: 7, Steps: 210 | Train Loss: 1.0193909 Vali Loss: 1.0525943 Test Loss: 1.0327913
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.0_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.024760365486145, mae:0.8067020177841187
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.0_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0208547
	speed: 0.0192s/iter; left time: 38.4119s
	iters: 200, epoch: 1 | loss: 1.0391288
	speed: 0.0171s/iter; left time: 32.5378s
Epoch: 1 cost time: 3.6346914768218994
Epoch: 1, Steps: 210 | Train Loss: 1.0367312 Vali Loss: 1.0475137 Test Loss: 1.0217297
Validation loss decreased (inf --> 1.047514).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0590880
	speed: 0.0191s/iter; left time: 34.1689s
	iters: 200, epoch: 2 | loss: 1.0331137
	speed: 0.0158s/iter; left time: 26.7432s
Epoch: 2 cost time: 3.465031385421753
Epoch: 2, Steps: 210 | Train Loss: 1.0295629 Vali Loss: 1.0489619 Test Loss: 1.0223517
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0462623
	speed: 0.0178s/iter; left time: 28.1784s
	iters: 200, epoch: 3 | loss: 1.0380025
	speed: 0.0175s/iter; left time: 25.8700s
Epoch: 3 cost time: 3.8251469135284424
Epoch: 3, Steps: 210 | Train Loss: 1.0273488 Vali Loss: 1.0483577 Test Loss: 1.0230669
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0516547
	speed: 0.0164s/iter; left time: 22.4996s
	iters: 200, epoch: 4 | loss: 1.0338991
	speed: 0.0150s/iter; left time: 19.0129s
Epoch: 4 cost time: 3.209049940109253
Epoch: 4, Steps: 210 | Train Loss: 1.0254976 Vali Loss: 1.0511577 Test Loss: 1.0240405
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0383211
	speed: 0.0205s/iter; left time: 23.7427s
	iters: 200, epoch: 5 | loss: 1.0245361
	speed: 0.0211s/iter; left time: 22.3812s
Epoch: 5 cost time: 4.496269464492798
Epoch: 5, Steps: 210 | Train Loss: 1.0240884 Vali Loss: 1.0519072 Test Loss: 1.0247332
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0525939
	speed: 0.0182s/iter; left time: 17.2801s
	iters: 200, epoch: 6 | loss: 1.0190914
	speed: 0.0193s/iter; left time: 16.4276s
Epoch: 6 cost time: 4.095319747924805
Epoch: 6, Steps: 210 | Train Loss: 1.0231658 Vali Loss: 1.0505646 Test Loss: 1.0250961
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.0_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0217297077178955, mae:0.8055899739265442
Seed: 42
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.5_alpha0.0_96_336Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.5_alpha0.0.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           336                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.0_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0312587
	speed: 0.0380s/iter; left time: 74.5993s
	iters: 200, epoch: 1 | loss: 1.0278058
	speed: 0.0293s/iter; left time: 54.4466s
Epoch: 1 cost time: 6.055437326431274
Epoch: 1, Steps: 206 | Train Loss: 1.0363208 Vali Loss: 1.0578617 Test Loss: 1.0367062
Validation loss decreased (inf --> 1.057862).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0272136
	speed: 0.0161s/iter; left time: 28.3401s
	iters: 200, epoch: 2 | loss: 1.0288938
	speed: 0.0153s/iter; left time: 25.3720s
Epoch: 2 cost time: 3.284558057785034
Epoch: 2, Steps: 206 | Train Loss: 1.0289047 Vali Loss: 1.0601616 Test Loss: 1.0377586
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0198400
	speed: 0.0180s/iter; left time: 27.8673s
	iters: 200, epoch: 3 | loss: 1.0300225
	speed: 0.0136s/iter; left time: 19.6996s
Epoch: 3 cost time: 2.868525266647339
Epoch: 3, Steps: 206 | Train Loss: 1.0267376 Vali Loss: 1.0618593 Test Loss: 1.0391897
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0235603
	speed: 0.0178s/iter; left time: 23.9217s
	iters: 200, epoch: 4 | loss: 1.0233054
	speed: 0.0145s/iter; left time: 18.0205s
Epoch: 4 cost time: 3.0773539543151855
Epoch: 4, Steps: 206 | Train Loss: 1.0247656 Vali Loss: 1.0630145 Test Loss: 1.0434960
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0400729
	speed: 0.0189s/iter; left time: 21.4560s
	iters: 200, epoch: 5 | loss: 1.0293088
	speed: 0.0158s/iter; left time: 16.4096s
Epoch: 5 cost time: 3.4217076301574707
Epoch: 5, Steps: 206 | Train Loss: 1.0227395 Vali Loss: 1.0624243 Test Loss: 1.0485419
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0309290
	speed: 0.0159s/iter; left time: 14.8336s
	iters: 200, epoch: 6 | loss: 1.0173014
	speed: 0.0176s/iter; left time: 14.6389s
Epoch: 6 cost time: 3.6774001121520996
Epoch: 6, Steps: 206 | Train Loss: 1.0214442 Vali Loss: 1.0620577 Test Loss: 1.0503836
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.0_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0367062091827393, mae:0.8095923662185669
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.0_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0323268
	speed: 0.0134s/iter; left time: 26.3744s
	iters: 200, epoch: 1 | loss: 1.0402023
	speed: 0.0131s/iter; left time: 24.3491s
Epoch: 1 cost time: 2.7908928394317627
Epoch: 1, Steps: 206 | Train Loss: 1.0365682 Vali Loss: 1.0571128 Test Loss: 1.0360514
Validation loss decreased (inf --> 1.057113).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0081648
	speed: 0.0240s/iter; left time: 42.0796s
	iters: 200, epoch: 2 | loss: 1.0347826
	speed: 0.0240s/iter; left time: 39.7758s
Epoch: 2 cost time: 5.042453050613403
Epoch: 2, Steps: 206 | Train Loss: 1.0290394 Vali Loss: 1.0600573 Test Loss: 1.0369995
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0071449
	speed: 0.0177s/iter; left time: 27.3946s
	iters: 200, epoch: 3 | loss: 1.0407023
	speed: 0.0150s/iter; left time: 21.7498s
Epoch: 3 cost time: 3.2200863361358643
Epoch: 3, Steps: 206 | Train Loss: 1.0269871 Vali Loss: 1.0614727 Test Loss: 1.0383601
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0397470
	speed: 0.0205s/iter; left time: 27.5029s
	iters: 200, epoch: 4 | loss: 1.0401903
	speed: 0.0157s/iter; left time: 19.4874s
Epoch: 4 cost time: 3.319728374481201
Epoch: 4, Steps: 206 | Train Loss: 1.0251106 Vali Loss: 1.0626651 Test Loss: 1.0400407
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0158226
	speed: 0.0124s/iter; left time: 14.1262s
	iters: 200, epoch: 5 | loss: 1.0030582
	speed: 0.0109s/iter; left time: 11.2962s
Epoch: 5 cost time: 2.293757915496826
Epoch: 5, Steps: 206 | Train Loss: 1.0238348 Vali Loss: 1.0626907 Test Loss: 1.0407479
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0085248
	speed: 0.0137s/iter; left time: 12.7102s
	iters: 200, epoch: 6 | loss: 1.0442843
	speed: 0.0132s/iter; left time: 10.9534s
Epoch: 6 cost time: 2.853121042251587
Epoch: 6, Steps: 206 | Train Loss: 1.0232428 Vali Loss: 1.0628771 Test Loss: 1.0412263
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.0_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0360515117645264, mae:0.8093985319137573
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.0_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0260533
	speed: 0.0247s/iter; left time: 48.5313s
	iters: 200, epoch: 1 | loss: 1.0377845
	speed: 0.0215s/iter; left time: 40.0953s
Epoch: 1 cost time: 4.4876344203948975
Epoch: 1, Steps: 206 | Train Loss: 1.0379995 Vali Loss: 1.0590147 Test Loss: 1.0359963
Validation loss decreased (inf --> 1.059015).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0249193
	speed: 0.0225s/iter; left time: 39.5298s
	iters: 200, epoch: 2 | loss: 1.0177455
	speed: 0.0217s/iter; left time: 35.8856s
Epoch: 2 cost time: 4.5264341831207275
Epoch: 2, Steps: 206 | Train Loss: 1.0297219 Vali Loss: 1.0593238 Test Loss: 1.0368886
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0175796
	speed: 0.0166s/iter; left time: 25.6535s
	iters: 200, epoch: 3 | loss: 1.0357928
	speed: 0.0150s/iter; left time: 21.6779s
Epoch: 3 cost time: 3.179612398147583
Epoch: 3, Steps: 206 | Train Loss: 1.0275684 Vali Loss: 1.0609467 Test Loss: 1.0391676
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0413588
	speed: 0.0155s/iter; left time: 20.7846s
	iters: 200, epoch: 4 | loss: 1.0345664
	speed: 0.0155s/iter; left time: 19.2213s
Epoch: 4 cost time: 3.326404094696045
Epoch: 4, Steps: 206 | Train Loss: 1.0262838 Vali Loss: 1.0607589 Test Loss: 1.0409185
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0358905
	speed: 0.0174s/iter; left time: 19.8077s
	iters: 200, epoch: 5 | loss: 1.0102984
	speed: 0.0166s/iter; left time: 17.2143s
Epoch: 5 cost time: 3.5182137489318848
Epoch: 5, Steps: 206 | Train Loss: 1.0251761 Vali Loss: 1.0617527 Test Loss: 1.0419378
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0269578
	speed: 0.0177s/iter; left time: 16.4937s
	iters: 200, epoch: 6 | loss: 1.0115991
	speed: 0.0182s/iter; left time: 15.1405s
Epoch: 6 cost time: 3.915830612182617
Epoch: 6, Steps: 206 | Train Loss: 1.0246580 Vali Loss: 1.0615442 Test Loss: 1.0425758
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.0_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.035996437072754, mae:0.8093281984329224
Seed: 42
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.5_alpha0.0_96_720Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.5_alpha0.0.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           720                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.0_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0292808
	speed: 0.0379s/iter; left time: 69.7097s
Epoch: 1 cost time: 5.740440845489502
Epoch: 1, Steps: 194 | Train Loss: 1.0386673 Vali Loss: 1.0545846 Test Loss: 1.0288825
Validation loss decreased (inf --> 1.054585).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0494003
	speed: 0.0175s/iter; left time: 28.8275s
Epoch: 2 cost time: 3.0507125854492188
Epoch: 2, Steps: 194 | Train Loss: 1.0319803 Vali Loss: 1.0527934 Test Loss: 1.0290593
Validation loss decreased (1.054585 --> 1.052793).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0345459
	speed: 0.0190s/iter; left time: 27.6322s
Epoch: 3 cost time: 3.2849068641662598
Epoch: 3, Steps: 194 | Train Loss: 1.0292464 Vali Loss: 1.0574605 Test Loss: 1.0319926
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0259869
	speed: 0.0189s/iter; left time: 23.7462s
Epoch: 4 cost time: 3.16780424118042
Epoch: 4, Steps: 194 | Train Loss: 1.0258266 Vali Loss: 1.0625497 Test Loss: 1.0363072
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0208776
	speed: 0.0162s/iter; left time: 17.2379s
Epoch: 5 cost time: 3.0567500591278076
Epoch: 5, Steps: 194 | Train Loss: 1.0241629 Vali Loss: 1.0622036 Test Loss: 1.0359284
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0417770
	speed: 0.0196s/iter; left time: 17.0461s
Epoch: 6 cost time: 3.7528088092803955
Epoch: 6, Steps: 194 | Train Loss: 1.0234605 Vali Loss: 1.0626577 Test Loss: 1.0364915
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0231426
	speed: 0.0213s/iter; left time: 14.4334s
Epoch: 7 cost time: 4.275238752365112
Epoch: 7, Steps: 194 | Train Loss: 1.0229584 Vali Loss: 1.0631617 Test Loss: 1.0367268
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.0_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0290592908859253, mae:0.8050318360328674
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.0_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0357709
	speed: 0.0198s/iter; left time: 36.3646s
Epoch: 1 cost time: 3.2138707637786865
Epoch: 1, Steps: 194 | Train Loss: 1.0413670 Vali Loss: 1.0567293 Test Loss: 1.0289528
Validation loss decreased (inf --> 1.056729).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0328113
	speed: 0.0131s/iter; left time: 21.6214s
Epoch: 2 cost time: 2.711045980453491
Epoch: 2, Steps: 194 | Train Loss: 1.0326425 Vali Loss: 1.0557797 Test Loss: 1.0287594
Validation loss decreased (1.056729 --> 1.055780).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0203484
	speed: 0.0209s/iter; left time: 30.3121s
Epoch: 3 cost time: 3.337859869003296
Epoch: 3, Steps: 194 | Train Loss: 1.0313619 Vali Loss: 1.0545626 Test Loss: 1.0290271
Validation loss decreased (1.055780 --> 1.054563).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0303614
	speed: 0.0159s/iter; left time: 20.0295s
Epoch: 4 cost time: 3.3454036712646484
Epoch: 4, Steps: 194 | Train Loss: 1.0304649 Vali Loss: 1.0541798 Test Loss: 1.0296015
Validation loss decreased (1.054563 --> 1.054180).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0215964
	speed: 0.0212s/iter; left time: 22.5670s
Epoch: 5 cost time: 3.658170223236084
Epoch: 5, Steps: 194 | Train Loss: 1.0297497 Vali Loss: 1.0542177 Test Loss: 1.0298604
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0332075
	speed: 0.0178s/iter; left time: 15.4825s
Epoch: 6 cost time: 2.8359971046447754
Epoch: 6, Steps: 194 | Train Loss: 1.0289724 Vali Loss: 1.0548233 Test Loss: 1.0305867
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0296261
	speed: 0.0155s/iter; left time: 10.4896s
Epoch: 7 cost time: 2.7462899684906006
Epoch: 7, Steps: 194 | Train Loss: 1.0285749 Vali Loss: 1.0547550 Test Loss: 1.0305790
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 1.0197265
	speed: 0.0181s/iter; left time: 8.7637s
Epoch: 8 cost time: 3.2121732234954834
Epoch: 8, Steps: 194 | Train Loss: 1.0284218 Vali Loss: 1.0544255 Test Loss: 1.0305791
EarlyStopping counter: 4 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 1.0342289
	speed: 0.0235s/iter; left time: 6.7960s
Epoch: 9 cost time: 4.261513948440552
Epoch: 9, Steps: 194 | Train Loss: 1.0282083 Vali Loss: 1.0545280 Test Loss: 1.0306791
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.0_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0296012163162231, mae:0.8052957057952881
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.0_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0456319
	speed: 0.0218s/iter; left time: 40.1653s
Epoch: 1 cost time: 4.2642598152160645
Epoch: 1, Steps: 194 | Train Loss: 1.0423907 Vali Loss: 1.0553542 Test Loss: 1.0294451
Validation loss decreased (inf --> 1.055354).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0409070
	speed: 0.0156s/iter; left time: 25.6342s
Epoch: 2 cost time: 3.2534544467926025
Epoch: 2, Steps: 194 | Train Loss: 1.0326089 Vali Loss: 1.0556914 Test Loss: 1.0284426
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0282959
	speed: 0.0187s/iter; left time: 27.1771s
Epoch: 3 cost time: 4.216282606124878
Epoch: 3, Steps: 194 | Train Loss: 1.0311358 Vali Loss: 1.0532057 Test Loss: 1.0292470
Validation loss decreased (1.055354 --> 1.053206).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0176259
	speed: 0.0195s/iter; left time: 24.5872s
Epoch: 4 cost time: 4.056280612945557
Epoch: 4, Steps: 194 | Train Loss: 1.0293542 Vali Loss: 1.0527779 Test Loss: 1.0294293
Validation loss decreased (1.053206 --> 1.052778).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0428053
	speed: 0.0159s/iter; left time: 16.8994s
Epoch: 5 cost time: 3.3700268268585205
Epoch: 5, Steps: 194 | Train Loss: 1.0272099 Vali Loss: 1.0541539 Test Loss: 1.0303535
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0464749
	speed: 0.0199s/iter; left time: 17.3200s
Epoch: 6 cost time: 3.0552568435668945
Epoch: 6, Steps: 194 | Train Loss: 1.0261204 Vali Loss: 1.0551338 Test Loss: 1.0307972
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0266054
	speed: 0.0125s/iter; left time: 8.4706s
Epoch: 7 cost time: 2.28903865814209
Epoch: 7, Steps: 194 | Train Loss: 1.0255392 Vali Loss: 1.0556599 Test Loss: 1.0308934
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 1.0249642
	speed: 0.0190s/iter; left time: 9.1649s
Epoch: 8 cost time: 3.2092912197113037
Epoch: 8, Steps: 194 | Train Loss: 1.0252837 Vali Loss: 1.0559030 Test Loss: 1.0312393
EarlyStopping counter: 4 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 1.0414104
	speed: 0.0153s/iter; left time: 4.4240s
Epoch: 9 cost time: 3.7567875385284424
Epoch: 9, Steps: 194 | Train Loss: 1.0249968 Vali Loss: 1.0559908 Test Loss: 1.0311478
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.0_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0294291973114014, mae:0.8050748109817505
Seed: 123
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.5_alpha0.0_96_96Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.5_alpha0.0.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           96                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.0_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.9879923
	speed: 0.0335s/iter; left time: 68.1054s
	iters: 200, epoch: 1 | loss: 1.0425050
	speed: 0.0250s/iter; left time: 48.1787s
Epoch: 1 cost time: 5.281457424163818
Epoch: 1, Steps: 213 | Train Loss: 1.0348519 Vali Loss: 1.0398022 Test Loss: 1.0229480
Validation loss decreased (inf --> 1.039802).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0498550
	speed: 0.0198s/iter; left time: 35.9189s
	iters: 200, epoch: 2 | loss: 1.0225646
	speed: 0.0170s/iter; left time: 29.2400s
Epoch: 2 cost time: 3.6900885105133057
Epoch: 2, Steps: 213 | Train Loss: 1.0235330 Vali Loss: 1.0379732 Test Loss: 1.0230054
Validation loss decreased (1.039802 --> 1.037973).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0019792
	speed: 0.0184s/iter; left time: 29.5547s
	iters: 200, epoch: 3 | loss: 0.9873981
	speed: 0.0152s/iter; left time: 22.8813s
Epoch: 3 cost time: 3.343045234680176
Epoch: 3, Steps: 213 | Train Loss: 1.0192073 Vali Loss: 1.0371081 Test Loss: 1.0238769
Validation loss decreased (1.037973 --> 1.037108).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0147929
	speed: 0.0152s/iter; left time: 21.1269s
	iters: 200, epoch: 4 | loss: 0.9881014
	speed: 0.0188s/iter; left time: 24.3402s
Epoch: 4 cost time: 4.009457588195801
Epoch: 4, Steps: 213 | Train Loss: 1.0154353 Vali Loss: 1.0384787 Test Loss: 1.0243012
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.9816750
	speed: 0.0239s/iter; left time: 28.1920s
	iters: 200, epoch: 5 | loss: 1.0207689
	speed: 0.0197s/iter; left time: 21.2276s
Epoch: 5 cost time: 4.304520130157471
Epoch: 5, Steps: 213 | Train Loss: 1.0128918 Vali Loss: 1.0386727 Test Loss: 1.0244901
EarlyStopping counter: 2 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0390544
	speed: 0.0178s/iter; left time: 17.2186s
	iters: 200, epoch: 6 | loss: 1.0007956
	speed: 0.0155s/iter; left time: 13.4541s
Epoch: 6 cost time: 3.3705341815948486
Epoch: 6, Steps: 213 | Train Loss: 1.0112508 Vali Loss: 1.0390095 Test Loss: 1.0249469
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0043944
	speed: 0.0151s/iter; left time: 11.3722s
	iters: 200, epoch: 7 | loss: 1.0037689
	speed: 0.0130s/iter; left time: 8.5144s
Epoch: 7 cost time: 2.983285903930664
Epoch: 7, Steps: 213 | Train Loss: 1.0104716 Vali Loss: 1.0409611 Test Loss: 1.0254315
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.9686138
	speed: 0.0169s/iter; left time: 9.1173s
	iters: 200, epoch: 8 | loss: 1.0222025
	speed: 0.0157s/iter; left time: 6.8877s
Epoch: 8 cost time: 3.3980398178100586
Epoch: 8, Steps: 213 | Train Loss: 1.0100042 Vali Loss: 1.0398977 Test Loss: 1.0256318
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.0_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0238769054412842, mae:0.807000458240509
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.0_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0517359
	speed: 0.0191s/iter; left time: 38.8485s
	iters: 200, epoch: 1 | loss: 1.0254430
	speed: 0.0171s/iter; left time: 33.0894s
Epoch: 1 cost time: 3.9298408031463623
Epoch: 1, Steps: 213 | Train Loss: 1.0332628 Vali Loss: 1.0382394 Test Loss: 1.0218441
Validation loss decreased (inf --> 1.038239).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0291570
	speed: 0.0183s/iter; left time: 33.3100s
	iters: 200, epoch: 2 | loss: 1.0127741
	speed: 0.0184s/iter; left time: 31.6224s
Epoch: 2 cost time: 4.002755403518677
Epoch: 2, Steps: 213 | Train Loss: 1.0237680 Vali Loss: 1.0378674 Test Loss: 1.0200064
Validation loss decreased (1.038239 --> 1.037867).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0213236
	speed: 0.0169s/iter; left time: 27.0617s
	iters: 200, epoch: 3 | loss: 0.9998660
	speed: 0.0145s/iter; left time: 21.8579s
Epoch: 3 cost time: 3.1539387702941895
Epoch: 3, Steps: 213 | Train Loss: 1.0195178 Vali Loss: 1.0357811 Test Loss: 1.0208408
Validation loss decreased (1.037867 --> 1.035781).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0443593
	speed: 0.0203s/iter; left time: 28.2357s
	iters: 200, epoch: 4 | loss: 0.9908645
	speed: 0.0177s/iter; left time: 22.9227s
Epoch: 4 cost time: 3.7717833518981934
Epoch: 4, Steps: 213 | Train Loss: 1.0158410 Vali Loss: 1.0359734 Test Loss: 1.0221964
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0066774
	speed: 0.0171s/iter; left time: 20.1286s
	iters: 200, epoch: 5 | loss: 1.0275788
	speed: 0.0172s/iter; left time: 18.6035s
Epoch: 5 cost time: 3.9054336547851562
Epoch: 5, Steps: 213 | Train Loss: 1.0132640 Vali Loss: 1.0360477 Test Loss: 1.0236661
EarlyStopping counter: 2 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0462822
	speed: 0.0163s/iter; left time: 15.7699s
	iters: 200, epoch: 6 | loss: 1.0290546
	speed: 0.0165s/iter; left time: 14.3118s
Epoch: 6 cost time: 3.7201216220855713
Epoch: 6, Steps: 213 | Train Loss: 1.0117958 Vali Loss: 1.0369130 Test Loss: 1.0240119
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0013492
	speed: 0.0134s/iter; left time: 10.0905s
	iters: 200, epoch: 7 | loss: 1.0070076
	speed: 0.0112s/iter; left time: 7.2888s
Epoch: 7 cost time: 2.479710102081299
Epoch: 7, Steps: 213 | Train Loss: 1.0106579 Vali Loss: 1.0367568 Test Loss: 1.0242690
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 1.0213416
	speed: 0.0161s/iter; left time: 8.6672s
	iters: 200, epoch: 8 | loss: 1.0048622
	speed: 0.0157s/iter; left time: 6.8914s
Epoch: 8 cost time: 3.402907133102417
Epoch: 8, Steps: 213 | Train Loss: 1.0102769 Vali Loss: 1.0367208 Test Loss: 1.0244217
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.0_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0208408832550049, mae:0.8061106204986572
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.0_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0232273
	speed: 0.0224s/iter; left time: 45.4481s
	iters: 200, epoch: 1 | loss: 1.0061412
	speed: 0.0179s/iter; left time: 34.6063s
Epoch: 1 cost time: 3.8445773124694824
Epoch: 1, Steps: 213 | Train Loss: 1.0365410 Vali Loss: 1.0397252 Test Loss: 1.0236775
Validation loss decreased (inf --> 1.039725).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0510637
	speed: 0.0208s/iter; left time: 37.7321s
	iters: 200, epoch: 2 | loss: 1.0416933
	speed: 0.0169s/iter; left time: 28.9714s
Epoch: 2 cost time: 3.5746681690216064
Epoch: 2, Steps: 213 | Train Loss: 1.0243403 Vali Loss: 1.0408874 Test Loss: 1.0235891
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0386271
	speed: 0.0192s/iter; left time: 30.8542s
	iters: 200, epoch: 3 | loss: 1.0334928
	speed: 0.0141s/iter; left time: 21.1773s
Epoch: 3 cost time: 3.032451629638672
Epoch: 3, Steps: 213 | Train Loss: 1.0206450 Vali Loss: 1.0394486 Test Loss: 1.0243623
Validation loss decreased (1.039725 --> 1.039449).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0227826
	speed: 0.0154s/iter; left time: 21.3939s
	iters: 200, epoch: 4 | loss: 0.9923047
	speed: 0.0152s/iter; left time: 19.5785s
Epoch: 4 cost time: 3.3533194065093994
Epoch: 4, Steps: 213 | Train Loss: 1.0174740 Vali Loss: 1.0398520 Test Loss: 1.0249281
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0173713
	speed: 0.0180s/iter; left time: 21.2584s
	iters: 200, epoch: 5 | loss: 0.9834663
	speed: 0.0192s/iter; left time: 20.7279s
Epoch: 5 cost time: 4.1053056716918945
Epoch: 5, Steps: 213 | Train Loss: 1.0148551 Vali Loss: 1.0403374 Test Loss: 1.0254511
EarlyStopping counter: 2 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0005589
	speed: 0.0214s/iter; left time: 20.6775s
	iters: 200, epoch: 6 | loss: 1.0289961
	speed: 0.0184s/iter; left time: 15.9055s
Epoch: 6 cost time: 3.9769976139068604
Epoch: 6, Steps: 213 | Train Loss: 1.0135600 Vali Loss: 1.0402600 Test Loss: 1.0259656
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0068936
	speed: 0.0177s/iter; left time: 13.3491s
	iters: 200, epoch: 7 | loss: 1.0075138
	speed: 0.0150s/iter; left time: 9.7914s
Epoch: 7 cost time: 3.2435882091522217
Epoch: 7, Steps: 213 | Train Loss: 1.0127916 Vali Loss: 1.0406893 Test Loss: 1.0258374
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.9941031
	speed: 0.0160s/iter; left time: 8.6412s
	iters: 200, epoch: 8 | loss: 1.0058906
	speed: 0.0152s/iter; left time: 6.6847s
Epoch: 8 cost time: 3.235431432723999
Epoch: 8, Steps: 213 | Train Loss: 1.0125231 Vali Loss: 1.0416384 Test Loss: 1.0259820
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.0_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0243624448776245, mae:0.8074142932891846
Seed: 123
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.5_alpha0.0_96_192Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.5_alpha0.0.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           192                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.0_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0392917
	speed: 0.0319s/iter; left time: 63.9230s
	iters: 200, epoch: 1 | loss: 1.0058143
	speed: 0.0231s/iter; left time: 43.9763s
Epoch: 1 cost time: 4.7896435260772705
Epoch: 1, Steps: 210 | Train Loss: 1.0376074 Vali Loss: 1.0496004 Test Loss: 1.0221316
Validation loss decreased (inf --> 1.049600).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0318089
	speed: 0.0136s/iter; left time: 24.2791s
	iters: 200, epoch: 2 | loss: 1.0239226
	speed: 0.0139s/iter; left time: 23.5818s
Epoch: 2 cost time: 3.023301839828491
Epoch: 2, Steps: 210 | Train Loss: 1.0291898 Vali Loss: 1.0471722 Test Loss: 1.0226442
Validation loss decreased (1.049600 --> 1.047172).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0478427
	speed: 0.0237s/iter; left time: 37.4255s
	iters: 200, epoch: 3 | loss: 1.0304761
	speed: 0.0196s/iter; left time: 28.9638s
Epoch: 3 cost time: 4.1601104736328125
Epoch: 3, Steps: 210 | Train Loss: 1.0270145 Vali Loss: 1.0485450 Test Loss: 1.0233260
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0447109
	speed: 0.0214s/iter; left time: 29.3828s
	iters: 200, epoch: 4 | loss: 1.0204644
	speed: 0.0186s/iter; left time: 23.6517s
Epoch: 4 cost time: 4.005820989608765
Epoch: 4, Steps: 210 | Train Loss: 1.0248115 Vali Loss: 1.0497143 Test Loss: 1.0242121
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0429043
	speed: 0.0146s/iter; left time: 16.9001s
	iters: 200, epoch: 5 | loss: 1.0251000
	speed: 0.0120s/iter; left time: 12.7831s
Epoch: 5 cost time: 2.584773540496826
Epoch: 5, Steps: 210 | Train Loss: 1.0232868 Vali Loss: 1.0498860 Test Loss: 1.0248489
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0219949
	speed: 0.0173s/iter; left time: 16.4798s
	iters: 200, epoch: 6 | loss: 1.0160232
	speed: 0.0157s/iter; left time: 13.3373s
Epoch: 6 cost time: 3.3522377014160156
Epoch: 6, Steps: 210 | Train Loss: 1.0223157 Vali Loss: 1.0499619 Test Loss: 1.0250769
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0248849
	speed: 0.0192s/iter; left time: 14.2241s
	iters: 200, epoch: 7 | loss: 1.0327206
	speed: 0.0181s/iter; left time: 11.5831s
Epoch: 7 cost time: 3.8110554218292236
Epoch: 7, Steps: 210 | Train Loss: 1.0218232 Vali Loss: 1.0501364 Test Loss: 1.0252808
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.0_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0226441621780396, mae:0.8058840036392212
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.0_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0260108
	speed: 0.0225s/iter; left time: 45.0575s
	iters: 200, epoch: 1 | loss: 1.0359392
	speed: 0.0188s/iter; left time: 35.7747s
Epoch: 1 cost time: 3.976478099822998
Epoch: 1, Steps: 210 | Train Loss: 1.0396586 Vali Loss: 1.0492733 Test Loss: 1.0229952
Validation loss decreased (inf --> 1.049273).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0216329
	speed: 0.0156s/iter; left time: 27.9650s
	iters: 200, epoch: 2 | loss: 1.0109313
	speed: 0.0134s/iter; left time: 22.5805s
Epoch: 2 cost time: 2.887619972229004
Epoch: 2, Steps: 210 | Train Loss: 1.0294688 Vali Loss: 1.0489278 Test Loss: 1.0235438
Validation loss decreased (1.049273 --> 1.048928).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0375879
	speed: 0.0202s/iter; left time: 31.8857s
	iters: 200, epoch: 3 | loss: 1.0193987
	speed: 0.0181s/iter; left time: 26.7675s
Epoch: 3 cost time: 3.8115766048431396
Epoch: 3, Steps: 210 | Train Loss: 1.0271063 Vali Loss: 1.0504375 Test Loss: 1.0242295
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0206943
	speed: 0.0184s/iter; left time: 25.2713s
	iters: 200, epoch: 4 | loss: 0.9967923
	speed: 0.0170s/iter; left time: 21.5661s
Epoch: 4 cost time: 3.559530735015869
Epoch: 4, Steps: 210 | Train Loss: 1.0253029 Vali Loss: 1.0514303 Test Loss: 1.0248924
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0267050
	speed: 0.0186s/iter; left time: 21.5738s
	iters: 200, epoch: 5 | loss: 1.0207806
	speed: 0.0171s/iter; left time: 18.1809s
Epoch: 5 cost time: 3.7084157466888428
Epoch: 5, Steps: 210 | Train Loss: 1.0238054 Vali Loss: 1.0516193 Test Loss: 1.0253953
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0091496
	speed: 0.0151s/iter; left time: 14.4061s
	iters: 200, epoch: 6 | loss: 1.0314479
	speed: 0.0133s/iter; left time: 11.2959s
Epoch: 6 cost time: 2.8763153553009033
Epoch: 6, Steps: 210 | Train Loss: 1.0230349 Vali Loss: 1.0510348 Test Loss: 1.0259130
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0317516
	speed: 0.0186s/iter; left time: 13.7776s
	iters: 200, epoch: 7 | loss: 1.0102284
	speed: 0.0168s/iter; left time: 10.7881s
Epoch: 7 cost time: 3.632312774658203
Epoch: 7, Steps: 210 | Train Loss: 1.0224059 Vali Loss: 1.0507872 Test Loss: 1.0259832
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.0_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0235437154769897, mae:0.806238055229187
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.0_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0571848
	speed: 0.0196s/iter; left time: 39.1978s
	iters: 200, epoch: 1 | loss: 1.0355856
	speed: 0.0172s/iter; left time: 32.6279s
Epoch: 1 cost time: 3.6951136589050293
Epoch: 1, Steps: 210 | Train Loss: 1.0386383 Vali Loss: 1.0491425 Test Loss: 1.0223758
Validation loss decreased (inf --> 1.049142).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0365584
	speed: 0.0168s/iter; left time: 30.1733s
	iters: 200, epoch: 2 | loss: 1.0163718
	speed: 0.0182s/iter; left time: 30.7425s
Epoch: 2 cost time: 3.867973804473877
Epoch: 2, Steps: 210 | Train Loss: 1.0294258 Vali Loss: 1.0514505 Test Loss: 1.0225008
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0311439
	speed: 0.0165s/iter; left time: 26.0657s
	iters: 200, epoch: 3 | loss: 1.0268848
	speed: 0.0136s/iter; left time: 20.1546s
Epoch: 3 cost time: 2.9291632175445557
Epoch: 3, Steps: 210 | Train Loss: 1.0271725 Vali Loss: 1.0515242 Test Loss: 1.0230757
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0220864
	speed: 0.0205s/iter; left time: 28.1013s
	iters: 200, epoch: 4 | loss: 1.0221331
	speed: 0.0164s/iter; left time: 20.8113s
Epoch: 4 cost time: 3.5183212757110596
Epoch: 4, Steps: 210 | Train Loss: 1.0250069 Vali Loss: 1.0505313 Test Loss: 1.0253948
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0127704
	speed: 0.0213s/iter; left time: 24.7518s
	iters: 200, epoch: 5 | loss: 1.0257810
	speed: 0.0187s/iter; left time: 19.8922s
Epoch: 5 cost time: 4.01461386680603
Epoch: 5, Steps: 210 | Train Loss: 1.0229752 Vali Loss: 1.0533764 Test Loss: 1.0263118
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0270380
	speed: 0.0197s/iter; left time: 18.7124s
	iters: 200, epoch: 6 | loss: 1.0155883
	speed: 0.0160s/iter; left time: 13.5831s
Epoch: 6 cost time: 3.3356149196624756
Epoch: 6, Steps: 210 | Train Loss: 1.0215256 Vali Loss: 1.0536854 Test Loss: 1.0274208
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.0_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0223758220672607, mae:0.8058701157569885
Seed: 123
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.5_alpha0.0_96_336Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.5_alpha0.0.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           336                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.0_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0147241
	speed: 0.0264s/iter; left time: 51.6901s
	iters: 200, epoch: 1 | loss: 1.0296342
	speed: 0.0198s/iter; left time: 36.8517s
Epoch: 1 cost time: 4.142529249191284
Epoch: 1, Steps: 206 | Train Loss: 1.0370353 Vali Loss: 1.0575430 Test Loss: 1.0359844
Validation loss decreased (inf --> 1.057543).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0432582
	speed: 0.0188s/iter; left time: 32.9617s
	iters: 200, epoch: 2 | loss: 1.0338162
	speed: 0.0164s/iter; left time: 27.1456s
Epoch: 2 cost time: 3.4780168533325195
Epoch: 2, Steps: 206 | Train Loss: 1.0291658 Vali Loss: 1.0582184 Test Loss: 1.0370884
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0380210
	speed: 0.0132s/iter; left time: 20.4809s
	iters: 200, epoch: 3 | loss: 1.0339550
	speed: 0.0121s/iter; left time: 17.4793s
Epoch: 3 cost time: 2.5266356468200684
Epoch: 3, Steps: 206 | Train Loss: 1.0272440 Vali Loss: 1.0611324 Test Loss: 1.0377799
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0144734
	speed: 0.0216s/iter; left time: 29.0304s
	iters: 200, epoch: 4 | loss: 1.0401841
	speed: 0.0199s/iter; left time: 24.7097s
Epoch: 4 cost time: 4.1434361934661865
Epoch: 4, Steps: 206 | Train Loss: 1.0258070 Vali Loss: 1.0623938 Test Loss: 1.0392028
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0259107
	speed: 0.0233s/iter; left time: 26.4484s
	iters: 200, epoch: 5 | loss: 1.0162374
	speed: 0.0191s/iter; left time: 19.8025s
Epoch: 5 cost time: 3.963270664215088
Epoch: 5, Steps: 206 | Train Loss: 1.0246653 Vali Loss: 1.0629520 Test Loss: 1.0400617
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0268847
	speed: 0.0219s/iter; left time: 20.3942s
	iters: 200, epoch: 6 | loss: 1.0319772
	speed: 0.0181s/iter; left time: 15.0640s
Epoch: 6 cost time: 3.783660650253296
Epoch: 6, Steps: 206 | Train Loss: 1.0237088 Vali Loss: 1.0632942 Test Loss: 1.0410646
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.0_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0359843969345093, mae:0.8093756437301636
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.0_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0286871
	speed: 0.0150s/iter; left time: 29.4064s
	iters: 200, epoch: 1 | loss: 1.0385613
	speed: 0.0127s/iter; left time: 23.7161s
Epoch: 1 cost time: 2.775383472442627
Epoch: 1, Steps: 206 | Train Loss: 1.0359542 Vali Loss: 1.0580829 Test Loss: 1.0361608
Validation loss decreased (inf --> 1.058083).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0316073
	speed: 0.0178s/iter; left time: 31.1825s
	iters: 200, epoch: 2 | loss: 1.0186980
	speed: 0.0156s/iter; left time: 25.8902s
Epoch: 2 cost time: 3.3434674739837646
Epoch: 2, Steps: 206 | Train Loss: 1.0285856 Vali Loss: 1.0611498 Test Loss: 1.0382352
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0298741
	speed: 0.0166s/iter; left time: 25.6731s
	iters: 200, epoch: 3 | loss: 1.0360876
	speed: 0.0153s/iter; left time: 22.2268s
Epoch: 3 cost time: 3.240410566329956
Epoch: 3, Steps: 206 | Train Loss: 1.0261750 Vali Loss: 1.0614383 Test Loss: 1.0409982
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0337417
	speed: 0.0146s/iter; left time: 19.6707s
	iters: 200, epoch: 4 | loss: 1.0204855
	speed: 0.0141s/iter; left time: 17.4972s
Epoch: 4 cost time: 3.0503242015838623
Epoch: 4, Steps: 206 | Train Loss: 1.0241898 Vali Loss: 1.0613890 Test Loss: 1.0433898
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0169709
	speed: 0.0147s/iter; left time: 16.7535s
	iters: 200, epoch: 5 | loss: 1.0236685
	speed: 0.0143s/iter; left time: 14.7931s
Epoch: 5 cost time: 3.039818525314331
Epoch: 5, Steps: 206 | Train Loss: 1.0229896 Vali Loss: 1.0608435 Test Loss: 1.0450354
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0087363
	speed: 0.0168s/iter; left time: 15.6208s
	iters: 200, epoch: 6 | loss: 1.0307825
	speed: 0.0165s/iter; left time: 13.6776s
Epoch: 6 cost time: 3.4749526977539062
Epoch: 6, Steps: 206 | Train Loss: 1.0217532 Vali Loss: 1.0592817 Test Loss: 1.0468582
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.0_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0361608266830444, mae:0.8093572854995728
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.0_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0350327
	speed: 0.0186s/iter; left time: 36.4914s
	iters: 200, epoch: 1 | loss: 1.0322639
	speed: 0.0201s/iter; left time: 37.4331s
Epoch: 1 cost time: 4.208784818649292
Epoch: 1, Steps: 206 | Train Loss: 1.0359428 Vali Loss: 1.0583007 Test Loss: 1.0357369
Validation loss decreased (inf --> 1.058301).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0345031
	speed: 0.0151s/iter; left time: 26.5839s
	iters: 200, epoch: 2 | loss: 1.0245634
	speed: 0.0153s/iter; left time: 25.2976s
Epoch: 2 cost time: 3.215987205505371
Epoch: 2, Steps: 206 | Train Loss: 1.0288890 Vali Loss: 1.0604780 Test Loss: 1.0371701
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0448054
	speed: 0.0126s/iter; left time: 19.5018s
	iters: 200, epoch: 3 | loss: 1.0245078
	speed: 0.0132s/iter; left time: 19.1765s
Epoch: 3 cost time: 2.8056640625
Epoch: 3, Steps: 206 | Train Loss: 1.0268856 Vali Loss: 1.0618032 Test Loss: 1.0380883
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0273556
	speed: 0.0156s/iter; left time: 20.9992s
	iters: 200, epoch: 4 | loss: 1.0452067
	speed: 0.0155s/iter; left time: 19.2665s
Epoch: 4 cost time: 3.2945024967193604
Epoch: 4, Steps: 206 | Train Loss: 1.0253507 Vali Loss: 1.0642203 Test Loss: 1.0399306
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0372460
	speed: 0.0164s/iter; left time: 18.6240s
	iters: 200, epoch: 5 | loss: 1.0266275
	speed: 0.0154s/iter; left time: 15.9838s
Epoch: 5 cost time: 3.242612361907959
Epoch: 5, Steps: 206 | Train Loss: 1.0241179 Vali Loss: 1.0654917 Test Loss: 1.0412886
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0456998
	speed: 0.0224s/iter; left time: 20.8108s
	iters: 200, epoch: 6 | loss: 1.0211201
	speed: 0.0171s/iter; left time: 14.1883s
Epoch: 6 cost time: 3.620617151260376
Epoch: 6, Steps: 206 | Train Loss: 1.0235796 Vali Loss: 1.0651126 Test Loss: 1.0420121
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.0_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0357367992401123, mae:0.8092601299285889
Seed: 123
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.5_alpha0.0_96_720Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.5_alpha0.0.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           720                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.0_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0282953
	speed: 0.0291s/iter; left time: 53.5269s
Epoch: 1 cost time: 4.031841278076172
Epoch: 1, Steps: 194 | Train Loss: 1.0387491 Vali Loss: 1.0557686 Test Loss: 1.0284778
Validation loss decreased (inf --> 1.055769).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0411077
	speed: 0.0157s/iter; left time: 25.7759s
Epoch: 2 cost time: 2.5672404766082764
Epoch: 2, Steps: 194 | Train Loss: 1.0322583 Vali Loss: 1.0547581 Test Loss: 1.0285622
Validation loss decreased (1.055769 --> 1.054758).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0437773
	speed: 0.0229s/iter; left time: 33.3201s
Epoch: 3 cost time: 4.091269254684448
Epoch: 3, Steps: 194 | Train Loss: 1.0308664 Vali Loss: 1.0539889 Test Loss: 1.0288949
Validation loss decreased (1.054758 --> 1.053989).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0315889
	speed: 0.0202s/iter; left time: 25.4505s
Epoch: 4 cost time: 3.7436842918395996
Epoch: 4, Steps: 194 | Train Loss: 1.0297398 Vali Loss: 1.0529748 Test Loss: 1.0297498
Validation loss decreased (1.053989 --> 1.052975).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0392556
	speed: 0.0192s/iter; left time: 20.4283s
Epoch: 5 cost time: 3.813997268676758
Epoch: 5, Steps: 194 | Train Loss: 1.0283822 Vali Loss: 1.0533646 Test Loss: 1.0305780
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0156484
	speed: 0.0152s/iter; left time: 13.2526s
Epoch: 6 cost time: 2.719480276107788
Epoch: 6, Steps: 194 | Train Loss: 1.0271294 Vali Loss: 1.0558436 Test Loss: 1.0323900
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0241196
	speed: 0.0197s/iter; left time: 13.3435s
Epoch: 7 cost time: 3.425156354904175
Epoch: 7, Steps: 194 | Train Loss: 1.0264004 Vali Loss: 1.0570793 Test Loss: 1.0333658
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 1.0243034
	speed: 0.0199s/iter; left time: 9.6238s
Epoch: 8 cost time: 3.408900022506714
Epoch: 8, Steps: 194 | Train Loss: 1.0259006 Vali Loss: 1.0571904 Test Loss: 1.0332143
EarlyStopping counter: 4 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 1.0207431
	speed: 0.0198s/iter; left time: 5.7090s
Epoch: 9 cost time: 3.373128890991211
Epoch: 9, Steps: 194 | Train Loss: 1.0257223 Vali Loss: 1.0574956 Test Loss: 1.0337878
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.0_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0297497510910034, mae:0.8052036762237549
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.0_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0366488
	speed: 0.0191s/iter; left time: 35.2467s
Epoch: 1 cost time: 3.2401530742645264
Epoch: 1, Steps: 194 | Train Loss: 1.0390681 Vali Loss: 1.0558170 Test Loss: 1.0285892
Validation loss decreased (inf --> 1.055817).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0400281
	speed: 0.0170s/iter; left time: 27.9792s
Epoch: 2 cost time: 3.2164928913116455
Epoch: 2, Steps: 194 | Train Loss: 1.0321142 Vali Loss: 1.0549607 Test Loss: 1.0283661
Validation loss decreased (1.055817 --> 1.054961).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0217093
	speed: 0.0213s/iter; left time: 30.9714s
Epoch: 3 cost time: 3.5084264278411865
Epoch: 3, Steps: 194 | Train Loss: 1.0304329 Vali Loss: 1.0544512 Test Loss: 1.0290276
Validation loss decreased (1.054961 --> 1.054451).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0245370
	speed: 0.0243s/iter; left time: 30.5752s
Epoch: 4 cost time: 3.9086804389953613
Epoch: 4, Steps: 194 | Train Loss: 1.0293003 Vali Loss: 1.0543343 Test Loss: 1.0295558
Validation loss decreased (1.054451 --> 1.054334).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0284282
	speed: 0.0171s/iter; left time: 18.2255s
Epoch: 5 cost time: 3.040522336959839
Epoch: 5, Steps: 194 | Train Loss: 1.0282464 Vali Loss: 1.0555845 Test Loss: 1.0302999
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0380384
	speed: 0.0138s/iter; left time: 12.0556s
Epoch: 6 cost time: 2.4921205043792725
Epoch: 6, Steps: 194 | Train Loss: 1.0274373 Vali Loss: 1.0563864 Test Loss: 1.0306886
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0325273
	speed: 0.0188s/iter; left time: 12.6966s
Epoch: 7 cost time: 3.0368850231170654
Epoch: 7, Steps: 194 | Train Loss: 1.0269427 Vali Loss: 1.0573571 Test Loss: 1.0314428
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 1.0301645
	speed: 0.0236s/iter; left time: 11.4011s
Epoch: 8 cost time: 3.8008134365081787
Epoch: 8, Steps: 194 | Train Loss: 1.0267520 Vali Loss: 1.0575411 Test Loss: 1.0313298
EarlyStopping counter: 4 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 1.0310507
	speed: 0.0172s/iter; left time: 4.9814s
Epoch: 9 cost time: 3.2309656143188477
Epoch: 9, Steps: 194 | Train Loss: 1.0265886 Vali Loss: 1.0578761 Test Loss: 1.0314947
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.0_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0295555591583252, mae:0.8050926923751831
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.0_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0322492
	speed: 0.0195s/iter; left time: 35.8741s
Epoch: 1 cost time: 3.2690391540527344
Epoch: 1, Steps: 194 | Train Loss: 1.0393428 Vali Loss: 1.0555177 Test Loss: 1.0290390
Validation loss decreased (inf --> 1.055518).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0320747
	speed: 0.0170s/iter; left time: 27.9881s
Epoch: 2 cost time: 3.1406354904174805
Epoch: 2, Steps: 194 | Train Loss: 1.0321541 Vali Loss: 1.0537233 Test Loss: 1.0289063
Validation loss decreased (1.055518 --> 1.053723).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0240064
	speed: 0.0176s/iter; left time: 25.6160s
Epoch: 3 cost time: 3.24393630027771
Epoch: 3, Steps: 194 | Train Loss: 1.0306017 Vali Loss: 1.0526515 Test Loss: 1.0297363
Validation loss decreased (1.053723 --> 1.052652).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0435200
	speed: 0.0172s/iter; left time: 21.7108s
Epoch: 4 cost time: 3.2831623554229736
Epoch: 4, Steps: 194 | Train Loss: 1.0286870 Vali Loss: 1.0547398 Test Loss: 1.0317732
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0301211
	speed: 0.0134s/iter; left time: 14.2343s
Epoch: 5 cost time: 2.852750301361084
Epoch: 5, Steps: 194 | Train Loss: 1.0263363 Vali Loss: 1.0560418 Test Loss: 1.0311866
EarlyStopping counter: 2 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0138338
	speed: 0.0116s/iter; left time: 10.0996s
Epoch: 6 cost time: 1.9628396034240723
Epoch: 6, Steps: 194 | Train Loss: 1.0250172 Vali Loss: 1.0563750 Test Loss: 1.0314827
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0216151
	speed: 0.0119s/iter; left time: 8.0578s
Epoch: 7 cost time: 2.372523069381714
Epoch: 7, Steps: 194 | Train Loss: 1.0246763 Vali Loss: 1.0571533 Test Loss: 1.0317554
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 1.0272454
	speed: 0.0195s/iter; left time: 9.4342s
Epoch: 8 cost time: 3.361537456512451
Epoch: 8, Steps: 194 | Train Loss: 1.0242885 Vali Loss: 1.0571225 Test Loss: 1.0314908
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.0_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0297361612319946, mae:0.8053267598152161
Seed: 456
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.5_alpha0.0_96_96Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.5_alpha0.0.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           96                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.0_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0326341
	speed: 0.0295s/iter; left time: 59.9363s
	iters: 200, epoch: 1 | loss: 1.0329568
	speed: 0.0263s/iter; left time: 50.7462s
Epoch: 1 cost time: 5.5411696434021
Epoch: 1, Steps: 213 | Train Loss: 1.0344585 Vali Loss: 1.0406635 Test Loss: 1.0240681
Validation loss decreased (inf --> 1.040663).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.9913184
	speed: 0.0197s/iter; left time: 35.8705s
	iters: 200, epoch: 2 | loss: 1.0287396
	speed: 0.0180s/iter; left time: 30.8788s
Epoch: 2 cost time: 3.8424489498138428
Epoch: 2, Steps: 213 | Train Loss: 1.0243333 Vali Loss: 1.0396101 Test Loss: 1.0244623
Validation loss decreased (1.040663 --> 1.039610).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0277951
	speed: 0.0202s/iter; left time: 32.3553s
	iters: 200, epoch: 3 | loss: 0.9924539
	speed: 0.0182s/iter; left time: 27.3944s
Epoch: 3 cost time: 3.8710761070251465
Epoch: 3, Steps: 213 | Train Loss: 1.0206307 Vali Loss: 1.0393522 Test Loss: 1.0259933
Validation loss decreased (1.039610 --> 1.039352).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.9825057
	speed: 0.0131s/iter; left time: 18.2091s
	iters: 200, epoch: 4 | loss: 1.0233939
	speed: 0.0113s/iter; left time: 14.5370s
Epoch: 4 cost time: 2.4632081985473633
Epoch: 4, Steps: 213 | Train Loss: 1.0173239 Vali Loss: 1.0399994 Test Loss: 1.0266193
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0035222
	speed: 0.0164s/iter; left time: 19.3559s
	iters: 200, epoch: 5 | loss: 1.0151024
	speed: 0.0169s/iter; left time: 18.2872s
Epoch: 5 cost time: 3.7340264320373535
Epoch: 5, Steps: 213 | Train Loss: 1.0150198 Vali Loss: 1.0399117 Test Loss: 1.0276649
EarlyStopping counter: 2 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.9791819
	speed: 0.0204s/iter; left time: 19.7161s
	iters: 200, epoch: 6 | loss: 1.0468116
	speed: 0.0178s/iter; left time: 15.4080s
Epoch: 6 cost time: 3.8397817611694336
Epoch: 6, Steps: 213 | Train Loss: 1.0136054 Vali Loss: 1.0405071 Test Loss: 1.0280635
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.9810241
	speed: 0.0179s/iter; left time: 13.4653s
	iters: 200, epoch: 7 | loss: 1.0064597
	speed: 0.0159s/iter; left time: 10.3722s
Epoch: 7 cost time: 3.5079259872436523
Epoch: 7, Steps: 213 | Train Loss: 1.0127786 Vali Loss: 1.0400995 Test Loss: 1.0282853
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.9972998
	speed: 0.0180s/iter; left time: 9.7094s
	iters: 200, epoch: 8 | loss: 1.0060514
	speed: 0.0153s/iter; left time: 6.7341s
Epoch: 8 cost time: 3.3633265495300293
Epoch: 8, Steps: 213 | Train Loss: 1.0124577 Vali Loss: 1.0409902 Test Loss: 1.0283688
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.0_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0259933471679688, mae:0.8079304099082947
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.0_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0066432
	speed: 0.0166s/iter; left time: 33.7153s
	iters: 200, epoch: 1 | loss: 1.0534846
	speed: 0.0165s/iter; left time: 31.9137s
Epoch: 1 cost time: 3.485100507736206
Epoch: 1, Steps: 213 | Train Loss: 1.0343653 Vali Loss: 1.0410135 Test Loss: 1.0222954
Validation loss decreased (inf --> 1.041013).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0715748
	speed: 0.0225s/iter; left time: 40.9093s
	iters: 200, epoch: 2 | loss: 1.0467017
	speed: 0.0217s/iter; left time: 37.2816s
Epoch: 2 cost time: 4.59675931930542
Epoch: 2, Steps: 213 | Train Loss: 1.0241372 Vali Loss: 1.0405376 Test Loss: 1.0220580
Validation loss decreased (1.041013 --> 1.040538).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0719897
	speed: 0.0222s/iter; left time: 35.5848s
	iters: 200, epoch: 3 | loss: 1.0248094
	speed: 0.0184s/iter; left time: 27.7640s
Epoch: 3 cost time: 4.017524003982544
Epoch: 3, Steps: 213 | Train Loss: 1.0204881 Vali Loss: 1.0374900 Test Loss: 1.0224720
Validation loss decreased (1.040538 --> 1.037490).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0155257
	speed: 0.0221s/iter; left time: 30.7708s
	iters: 200, epoch: 4 | loss: 0.9974439
	speed: 0.0176s/iter; left time: 22.7453s
Epoch: 4 cost time: 3.747316598892212
Epoch: 4, Steps: 213 | Train Loss: 1.0172992 Vali Loss: 1.0382810 Test Loss: 1.0229027
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0108930
	speed: 0.0142s/iter; left time: 16.7603s
	iters: 200, epoch: 5 | loss: 1.0169089
	speed: 0.0133s/iter; left time: 14.3211s
Epoch: 5 cost time: 2.8978168964385986
Epoch: 5, Steps: 213 | Train Loss: 1.0154242 Vali Loss: 1.0384792 Test Loss: 1.0233793
EarlyStopping counter: 2 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0286539
	speed: 0.0185s/iter; left time: 17.8872s
	iters: 200, epoch: 6 | loss: 1.0175558
	speed: 0.0177s/iter; left time: 15.3528s
Epoch: 6 cost time: 4.04196572303772
Epoch: 6, Steps: 213 | Train Loss: 1.0141750 Vali Loss: 1.0380578 Test Loss: 1.0233982
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0142816
	speed: 0.0219s/iter; left time: 16.4595s
	iters: 200, epoch: 7 | loss: 1.0152416
	speed: 0.0204s/iter; left time: 13.3400s
Epoch: 7 cost time: 4.3697733879089355
Epoch: 7, Steps: 213 | Train Loss: 1.0133531 Vali Loss: 1.0390891 Test Loss: 1.0235473
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 1.0393778
	speed: 0.0247s/iter; left time: 13.3354s
	iters: 200, epoch: 8 | loss: 1.0226270
	speed: 0.0196s/iter; left time: 8.6073s
Epoch: 8 cost time: 4.1125829219818115
Epoch: 8, Steps: 213 | Train Loss: 1.0132337 Vali Loss: 1.0375869 Test Loss: 1.0236187
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.0_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0224722623825073, mae:0.806582510471344
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.0_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0111351
	speed: 0.0140s/iter; left time: 28.3505s
	iters: 200, epoch: 1 | loss: 1.0318575
	speed: 0.0134s/iter; left time: 25.7813s
Epoch: 1 cost time: 2.9763951301574707
Epoch: 1, Steps: 213 | Train Loss: 1.0340701 Vali Loss: 1.0410049 Test Loss: 1.0227939
Validation loss decreased (inf --> 1.041005).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0790701
	speed: 0.0177s/iter; left time: 32.0956s
	iters: 200, epoch: 2 | loss: 1.0286705
	speed: 0.0195s/iter; left time: 33.5059s
Epoch: 2 cost time: 4.219934463500977
Epoch: 2, Steps: 213 | Train Loss: 1.0238386 Vali Loss: 1.0388416 Test Loss: 1.0233169
Validation loss decreased (1.041005 --> 1.038842).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0275598
	speed: 0.0204s/iter; left time: 32.8109s
	iters: 200, epoch: 3 | loss: 1.0116209
	speed: 0.0181s/iter; left time: 27.1718s
Epoch: 3 cost time: 3.9397263526916504
Epoch: 3, Steps: 213 | Train Loss: 1.0200028 Vali Loss: 1.0398555 Test Loss: 1.0227954
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0322429
	speed: 0.0169s/iter; left time: 23.5545s
	iters: 200, epoch: 4 | loss: 1.0006351
	speed: 0.0150s/iter; left time: 19.4050s
Epoch: 4 cost time: 3.2844908237457275
Epoch: 4, Steps: 213 | Train Loss: 1.0163838 Vali Loss: 1.0381765 Test Loss: 1.0226742
Validation loss decreased (1.038842 --> 1.038177).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0353768
	speed: 0.0171s/iter; left time: 20.1220s
	iters: 200, epoch: 5 | loss: 1.0014219
	speed: 0.0163s/iter; left time: 17.5802s
Epoch: 5 cost time: 3.5183820724487305
Epoch: 5, Steps: 213 | Train Loss: 1.0139157 Vali Loss: 1.0412832 Test Loss: 1.0234262
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0125358
	speed: 0.0190s/iter; left time: 18.3852s
	iters: 200, epoch: 6 | loss: 1.0024972
	speed: 0.0184s/iter; left time: 15.9717s
Epoch: 6 cost time: 3.9805421829223633
Epoch: 6, Steps: 213 | Train Loss: 1.0123902 Vali Loss: 1.0391350 Test Loss: 1.0236117
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0160351
	speed: 0.0226s/iter; left time: 17.0224s
	iters: 200, epoch: 7 | loss: 1.0359393
	speed: 0.0180s/iter; left time: 11.7839s
Epoch: 7 cost time: 3.8701744079589844
Epoch: 7, Steps: 213 | Train Loss: 1.0117120 Vali Loss: 1.0399368 Test Loss: 1.0237981
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 1.0160128
	speed: 0.0140s/iter; left time: 7.5678s
	iters: 200, epoch: 8 | loss: 1.0251906
	speed: 0.0128s/iter; left time: 5.6245s
Epoch: 8 cost time: 2.8211333751678467
Epoch: 8, Steps: 213 | Train Loss: 1.0113176 Vali Loss: 1.0405548 Test Loss: 1.0238700
EarlyStopping counter: 4 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.9843657
	speed: 0.0178s/iter; left time: 5.8290s
	iters: 200, epoch: 9 | loss: 1.0133790
	speed: 0.0140s/iter; left time: 3.1887s
Epoch: 9 cost time: 3.0391602516174316
Epoch: 9, Steps: 213 | Train Loss: 1.0111666 Vali Loss: 1.0397213 Test Loss: 1.0238957
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.0_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0226742029190063, mae:0.8066924214363098
Seed: 456
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.5_alpha0.0_96_192Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.5_alpha0.0.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           192                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.0_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0172939
	speed: 0.0290s/iter; left time: 57.9376s
	iters: 200, epoch: 1 | loss: 1.0422090
	speed: 0.0214s/iter; left time: 40.7685s
Epoch: 1 cost time: 4.480939865112305
Epoch: 1, Steps: 210 | Train Loss: 1.0379350 Vali Loss: 1.0467689 Test Loss: 1.0228382
Validation loss decreased (inf --> 1.046769).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0208473
	speed: 0.0199s/iter; left time: 35.6991s
	iters: 200, epoch: 2 | loss: 1.0242486
	speed: 0.0194s/iter; left time: 32.8543s
Epoch: 2 cost time: 4.165462493896484
Epoch: 2, Steps: 210 | Train Loss: 1.0293348 Vali Loss: 1.0493706 Test Loss: 1.0230210
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0173240
	speed: 0.0229s/iter; left time: 36.1499s
	iters: 200, epoch: 3 | loss: 1.0323660
	speed: 0.0196s/iter; left time: 29.0338s
Epoch: 3 cost time: 4.207017660140991
Epoch: 3, Steps: 210 | Train Loss: 1.0273300 Vali Loss: 1.0497388 Test Loss: 1.0243223
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0295488
	speed: 0.0169s/iter; left time: 23.2296s
	iters: 200, epoch: 4 | loss: 1.0357118
	speed: 0.0164s/iter; left time: 20.9000s
Epoch: 4 cost time: 3.6514673233032227
Epoch: 4, Steps: 210 | Train Loss: 1.0256396 Vali Loss: 1.0508597 Test Loss: 1.0252506
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0183463
	speed: 0.0162s/iter; left time: 18.7952s
	iters: 200, epoch: 5 | loss: 1.0299352
	speed: 0.0162s/iter; left time: 17.1845s
Epoch: 5 cost time: 3.4629554748535156
Epoch: 5, Steps: 210 | Train Loss: 1.0243280 Vali Loss: 1.0507867 Test Loss: 1.0260481
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0330684
	speed: 0.0174s/iter; left time: 16.5542s
	iters: 200, epoch: 6 | loss: 1.0262650
	speed: 0.0160s/iter; left time: 13.6489s
Epoch: 6 cost time: 3.4150049686431885
Epoch: 6, Steps: 210 | Train Loss: 1.0232600 Vali Loss: 1.0502217 Test Loss: 1.0264741
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.0_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0228381156921387, mae:0.8060451745986938
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.0_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0392873
	speed: 0.0181s/iter; left time: 36.2133s
	iters: 200, epoch: 1 | loss: 1.0391228
	speed: 0.0185s/iter; left time: 35.2138s
Epoch: 1 cost time: 3.9666335582733154
Epoch: 1, Steps: 210 | Train Loss: 1.0374999 Vali Loss: 1.0458405 Test Loss: 1.0220501
Validation loss decreased (inf --> 1.045841).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0140674
	speed: 0.0189s/iter; left time: 33.9108s
	iters: 200, epoch: 2 | loss: 1.0317688
	speed: 0.0159s/iter; left time: 26.8337s
Epoch: 2 cost time: 3.408106803894043
Epoch: 2, Steps: 210 | Train Loss: 1.0294340 Vali Loss: 1.0480273 Test Loss: 1.0227385
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0070598
	speed: 0.0166s/iter; left time: 26.1664s
	iters: 200, epoch: 3 | loss: 1.0166731
	speed: 0.0141s/iter; left time: 20.8409s
Epoch: 3 cost time: 3.0215935707092285
Epoch: 3, Steps: 210 | Train Loss: 1.0272955 Vali Loss: 1.0467279 Test Loss: 1.0239097
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0301449
	speed: 0.0223s/iter; left time: 30.6100s
	iters: 200, epoch: 4 | loss: 1.0309975
	speed: 0.0177s/iter; left time: 22.4359s
Epoch: 4 cost time: 3.717960834503174
Epoch: 4, Steps: 210 | Train Loss: 1.0252585 Vali Loss: 1.0468187 Test Loss: 1.0255042
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0398594
	speed: 0.0179s/iter; left time: 20.7308s
	iters: 200, epoch: 5 | loss: 1.0039973
	speed: 0.0155s/iter; left time: 16.4901s
Epoch: 5 cost time: 3.253880500793457
Epoch: 5, Steps: 210 | Train Loss: 1.0237973 Vali Loss: 1.0466437 Test Loss: 1.0264617
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0138857
	speed: 0.0243s/iter; left time: 23.1141s
	iters: 200, epoch: 6 | loss: 0.9897488
	speed: 0.0188s/iter; left time: 16.0326s
Epoch: 6 cost time: 3.9645907878875732
Epoch: 6, Steps: 210 | Train Loss: 1.0227487 Vali Loss: 1.0466384 Test Loss: 1.0269860
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.0_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0220500230789185, mae:0.8057648539543152
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.0_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0185465
	speed: 0.0192s/iter; left time: 38.4546s
	iters: 200, epoch: 1 | loss: 1.0268183
	speed: 0.0175s/iter; left time: 33.3382s
Epoch: 1 cost time: 3.738668918609619
Epoch: 1, Steps: 210 | Train Loss: 1.0388954 Vali Loss: 1.0482376 Test Loss: 1.0220422
Validation loss decreased (inf --> 1.048238).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0189558
	speed: 0.0248s/iter; left time: 44.4082s
	iters: 200, epoch: 2 | loss: 1.0431542
	speed: 0.0193s/iter; left time: 32.5773s
Epoch: 2 cost time: 4.096170902252197
Epoch: 2, Steps: 210 | Train Loss: 1.0291548 Vali Loss: 1.0484214 Test Loss: 1.0227046
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0183638
	speed: 0.0170s/iter; left time: 26.9439s
	iters: 200, epoch: 3 | loss: 1.0383837
	speed: 0.0169s/iter; left time: 25.0715s
Epoch: 3 cost time: 3.665897846221924
Epoch: 3, Steps: 210 | Train Loss: 1.0266317 Vali Loss: 1.0478206 Test Loss: 1.0231720
Validation loss decreased (1.048238 --> 1.047821).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0130699
	speed: 0.0146s/iter; left time: 19.9600s
	iters: 200, epoch: 4 | loss: 1.0051383
	speed: 0.0120s/iter; left time: 15.2414s
Epoch: 4 cost time: 2.619875431060791
Epoch: 4, Steps: 210 | Train Loss: 1.0243141 Vali Loss: 1.0510355 Test Loss: 1.0245165
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0195520
	speed: 0.0196s/iter; left time: 22.7835s
	iters: 200, epoch: 5 | loss: 1.0229239
	speed: 0.0174s/iter; left time: 18.4285s
Epoch: 5 cost time: 3.7192673683166504
Epoch: 5, Steps: 210 | Train Loss: 1.0226085 Vali Loss: 1.0506287 Test Loss: 1.0248762
EarlyStopping counter: 2 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0151453
	speed: 0.0196s/iter; left time: 18.6580s
	iters: 200, epoch: 6 | loss: 1.0451044
	speed: 0.0179s/iter; left time: 15.2115s
Epoch: 6 cost time: 3.830533266067505
Epoch: 6, Steps: 210 | Train Loss: 1.0214893 Vali Loss: 1.0503980 Test Loss: 1.0257391
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0073050
	speed: 0.0180s/iter; left time: 13.3484s
	iters: 200, epoch: 7 | loss: 1.0062864
	speed: 0.0165s/iter; left time: 10.5695s
Epoch: 7 cost time: 3.4506428241729736
Epoch: 7, Steps: 210 | Train Loss: 1.0208669 Vali Loss: 1.0497543 Test Loss: 1.0260528
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 1.0124439
	speed: 0.0145s/iter; left time: 7.6731s
	iters: 200, epoch: 8 | loss: 1.0299040
	speed: 0.0130s/iter; left time: 5.6071s
Epoch: 8 cost time: 2.834141254425049
Epoch: 8, Steps: 210 | Train Loss: 1.0206946 Vali Loss: 1.0505018 Test Loss: 1.0261656
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.0_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0231719017028809, mae:0.8060482144355774
Seed: 456
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.5_alpha0.0_96_336Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.5_alpha0.0.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           336                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.0_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0505366
	speed: 0.0318s/iter; left time: 62.3232s
	iters: 200, epoch: 1 | loss: 1.0291575
	speed: 0.0223s/iter; left time: 41.5538s
Epoch: 1 cost time: 4.671098709106445
Epoch: 1, Steps: 206 | Train Loss: 1.0369401 Vali Loss: 1.0582416 Test Loss: 1.0366299
Validation loss decreased (inf --> 1.058242).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0280162
	speed: 0.0171s/iter; left time: 29.9333s
	iters: 200, epoch: 2 | loss: 1.0231808
	speed: 0.0138s/iter; left time: 22.7910s
Epoch: 2 cost time: 2.957341432571411
Epoch: 2, Steps: 206 | Train Loss: 1.0292238 Vali Loss: 1.0617326 Test Loss: 1.0377511
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0139346
	speed: 0.0199s/iter; left time: 30.7630s
	iters: 200, epoch: 3 | loss: 1.0295405
	speed: 0.0173s/iter; left time: 25.0105s
Epoch: 3 cost time: 3.6272480487823486
Epoch: 3, Steps: 206 | Train Loss: 1.0275013 Vali Loss: 1.0626378 Test Loss: 1.0389656
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0282080
	speed: 0.0172s/iter; left time: 23.0864s
	iters: 200, epoch: 4 | loss: 1.0252647
	speed: 0.0152s/iter; left time: 18.9400s
Epoch: 4 cost time: 3.2524325847625732
Epoch: 4, Steps: 206 | Train Loss: 1.0261851 Vali Loss: 1.0621809 Test Loss: 1.0404689
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0095122
	speed: 0.0168s/iter; left time: 19.0487s
	iters: 200, epoch: 5 | loss: 1.0226718
	speed: 0.0156s/iter; left time: 16.1576s
Epoch: 5 cost time: 3.2550506591796875
Epoch: 5, Steps: 206 | Train Loss: 1.0250476 Vali Loss: 1.0630860 Test Loss: 1.0415783
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0312743
	speed: 0.0197s/iter; left time: 18.3230s
	iters: 200, epoch: 6 | loss: 1.0321572
	speed: 0.0168s/iter; left time: 13.9920s
Epoch: 6 cost time: 3.6019084453582764
Epoch: 6, Steps: 206 | Train Loss: 1.0245207 Vali Loss: 1.0629237 Test Loss: 1.0426973
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.0_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0366299152374268, mae:0.8095698952674866
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.0_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0387715
	speed: 0.0158s/iter; left time: 31.0810s
	iters: 200, epoch: 1 | loss: 1.0367401
	speed: 0.0143s/iter; left time: 26.7045s
Epoch: 1 cost time: 3.034517288208008
Epoch: 1, Steps: 206 | Train Loss: 1.0398832 Vali Loss: 1.0586818 Test Loss: 1.0366799
Validation loss decreased (inf --> 1.058682).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0297754
	speed: 0.0204s/iter; left time: 35.8248s
	iters: 200, epoch: 2 | loss: 1.0169265
	speed: 0.0180s/iter; left time: 29.7717s
Epoch: 2 cost time: 3.788470983505249
Epoch: 2, Steps: 206 | Train Loss: 1.0303285 Vali Loss: 1.0570580 Test Loss: 1.0391268
Validation loss decreased (1.058682 --> 1.057058).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0370985
	speed: 0.0184s/iter; left time: 28.4362s
	iters: 200, epoch: 3 | loss: 1.0408179
	speed: 0.0165s/iter; left time: 23.9040s
Epoch: 3 cost time: 3.5020172595977783
Epoch: 3, Steps: 206 | Train Loss: 1.0281270 Vali Loss: 1.0589274 Test Loss: 1.0396810
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0290020
	speed: 0.0202s/iter; left time: 27.1751s
	iters: 200, epoch: 4 | loss: 1.0127215
	speed: 0.0163s/iter; left time: 20.2681s
Epoch: 4 cost time: 3.4192678928375244
Epoch: 4, Steps: 206 | Train Loss: 1.0263591 Vali Loss: 1.0603712 Test Loss: 1.0426319
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0277362
	speed: 0.0121s/iter; left time: 13.7150s
	iters: 200, epoch: 5 | loss: 1.0230528
	speed: 0.0118s/iter; left time: 12.2608s
Epoch: 5 cost time: 2.527467727661133
Epoch: 5, Steps: 206 | Train Loss: 1.0246290 Vali Loss: 1.0612928 Test Loss: 1.0454133
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0328015
	speed: 0.0154s/iter; left time: 14.3579s
	iters: 200, epoch: 6 | loss: 1.0264674
	speed: 0.0137s/iter; left time: 11.3905s
Epoch: 6 cost time: 2.9098479747772217
Epoch: 6, Steps: 206 | Train Loss: 1.0235611 Vali Loss: 1.0610597 Test Loss: 1.0463076
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0083382
	speed: 0.0174s/iter; left time: 12.5844s
	iters: 200, epoch: 7 | loss: 1.0267045
	speed: 0.0174s/iter; left time: 10.9024s
Epoch: 7 cost time: 3.77205491065979
Epoch: 7, Steps: 206 | Train Loss: 1.0230414 Vali Loss: 1.0610822 Test Loss: 1.0473108
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.0_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0391267538070679, mae:0.8105180859565735
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.0_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0405124
	speed: 0.0174s/iter; left time: 34.1675s
	iters: 200, epoch: 1 | loss: 1.0295759
	speed: 0.0139s/iter; left time: 25.7784s
Epoch: 1 cost time: 2.9191272258758545
Epoch: 1, Steps: 206 | Train Loss: 1.0377479 Vali Loss: 1.0581546 Test Loss: 1.0355093
Validation loss decreased (inf --> 1.058155).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0187010
	speed: 0.0146s/iter; left time: 25.6723s
	iters: 200, epoch: 2 | loss: 1.0326221
	speed: 0.0127s/iter; left time: 21.0817s
Epoch: 2 cost time: 2.6825714111328125
Epoch: 2, Steps: 206 | Train Loss: 1.0295745 Vali Loss: 1.0575407 Test Loss: 1.0366501
Validation loss decreased (1.058155 --> 1.057541).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0344794
	speed: 0.0229s/iter; left time: 35.5378s
	iters: 200, epoch: 3 | loss: 1.0387805
	speed: 0.0181s/iter; left time: 26.2346s
Epoch: 3 cost time: 3.8218579292297363
Epoch: 3, Steps: 206 | Train Loss: 1.0280795 Vali Loss: 1.0593860 Test Loss: 1.0374304
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0310165
	speed: 0.0188s/iter; left time: 25.2743s
	iters: 200, epoch: 4 | loss: 1.0314941
	speed: 0.0162s/iter; left time: 20.1875s
Epoch: 4 cost time: 3.433579683303833
Epoch: 4, Steps: 206 | Train Loss: 1.0267452 Vali Loss: 1.0619951 Test Loss: 1.0378343
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0338748
	speed: 0.0206s/iter; left time: 23.4473s
	iters: 200, epoch: 5 | loss: 1.0124362
	speed: 0.0186s/iter; left time: 19.3389s
Epoch: 5 cost time: 3.894305467605591
Epoch: 5, Steps: 206 | Train Loss: 1.0254565 Vali Loss: 1.0622782 Test Loss: 1.0389761
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0332989
	speed: 0.0227s/iter; left time: 21.1705s
	iters: 200, epoch: 6 | loss: 1.0489178
	speed: 0.0176s/iter; left time: 14.6511s
Epoch: 6 cost time: 3.6720359325408936
Epoch: 6, Steps: 206 | Train Loss: 1.0249659 Vali Loss: 1.0628194 Test Loss: 1.0393872
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0365176
	speed: 0.0179s/iter; left time: 12.9423s
	iters: 200, epoch: 7 | loss: 1.0007148
	speed: 0.0161s/iter; left time: 10.0713s
Epoch: 7 cost time: 3.4403510093688965
Epoch: 7, Steps: 206 | Train Loss: 1.0246522 Vali Loss: 1.0633504 Test Loss: 1.0396633
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.0_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0366500616073608, mae:0.8096216320991516
Seed: 456
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.5_alpha0.0_96_720Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.5_alpha0.0.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           720                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.0_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0316539
	speed: 0.0273s/iter; left time: 50.3511s
Epoch: 1 cost time: 3.815997362136841
Epoch: 1, Steps: 194 | Train Loss: 1.0390441 Vali Loss: 1.0552398 Test Loss: 1.0286030
Validation loss decreased (inf --> 1.055240).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0244790
	speed: 0.0186s/iter; left time: 30.6674s
Epoch: 2 cost time: 3.350592851638794
Epoch: 2, Steps: 194 | Train Loss: 1.0323149 Vali Loss: 1.0547035 Test Loss: 1.0284588
Validation loss decreased (1.055240 --> 1.054703).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0419592
	speed: 0.0193s/iter; left time: 28.1144s
Epoch: 3 cost time: 3.3709557056427
Epoch: 3, Steps: 194 | Train Loss: 1.0308124 Vali Loss: 1.0532634 Test Loss: 1.0298826
Validation loss decreased (1.054703 --> 1.053263).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0457718
	speed: 0.0183s/iter; left time: 23.0379s
Epoch: 4 cost time: 3.2014219760894775
Epoch: 4, Steps: 194 | Train Loss: 1.0284763 Vali Loss: 1.0546772 Test Loss: 1.0316967
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0351894
	speed: 0.0143s/iter; left time: 15.2751s
Epoch: 5 cost time: 2.5061514377593994
Epoch: 5, Steps: 194 | Train Loss: 1.0261779 Vali Loss: 1.0569587 Test Loss: 1.0333626
EarlyStopping counter: 2 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0281911
	speed: 0.0172s/iter; left time: 15.0095s
Epoch: 6 cost time: 2.860842704772949
Epoch: 6, Steps: 194 | Train Loss: 1.0251278 Vali Loss: 1.0582216 Test Loss: 1.0351967
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0327696
	speed: 0.0172s/iter; left time: 11.6706s
Epoch: 7 cost time: 3.034668445587158
Epoch: 7, Steps: 194 | Train Loss: 1.0245942 Vali Loss: 1.0571098 Test Loss: 1.0339010
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 1.0220773
	speed: 0.0185s/iter; left time: 8.9357s
Epoch: 8 cost time: 3.510202407836914
Epoch: 8, Steps: 194 | Train Loss: 1.0245093 Vali Loss: 1.0574210 Test Loss: 1.0344808
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.0_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0298823118209839, mae:0.8053338527679443
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.0_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0439186
	speed: 0.0189s/iter; left time: 34.7591s
Epoch: 1 cost time: 3.216644048690796
Epoch: 1, Steps: 194 | Train Loss: 1.0396030 Vali Loss: 1.0556945 Test Loss: 1.0287586
Validation loss decreased (inf --> 1.055694).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0252404
	speed: 0.0137s/iter; left time: 22.6385s
Epoch: 2 cost time: 2.6631529331207275
Epoch: 2, Steps: 194 | Train Loss: 1.0323379 Vali Loss: 1.0557500 Test Loss: 1.0287865
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0182395
	speed: 0.0184s/iter; left time: 26.6907s
Epoch: 3 cost time: 3.3273444175720215
Epoch: 3, Steps: 194 | Train Loss: 1.0306197 Vali Loss: 1.0543565 Test Loss: 1.0302367
Validation loss decreased (1.055694 --> 1.054356).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0210365
	speed: 0.0241s/iter; left time: 30.3310s
Epoch: 4 cost time: 4.614858627319336
Epoch: 4, Steps: 194 | Train Loss: 1.0290836 Vali Loss: 1.0553479 Test Loss: 1.0308073
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0337825
	speed: 0.0174s/iter; left time: 18.5149s
Epoch: 5 cost time: 3.0493500232696533
Epoch: 5, Steps: 194 | Train Loss: 1.0271196 Vali Loss: 1.0560977 Test Loss: 1.0309979
EarlyStopping counter: 2 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0240585
	speed: 0.0177s/iter; left time: 15.4063s
Epoch: 6 cost time: 2.845391273498535
Epoch: 6, Steps: 194 | Train Loss: 1.0257744 Vali Loss: 1.0578681 Test Loss: 1.0318686
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0175186
	speed: 0.0155s/iter; left time: 10.5205s
Epoch: 7 cost time: 2.9524922370910645
Epoch: 7, Steps: 194 | Train Loss: 1.0251466 Vali Loss: 1.0580536 Test Loss: 1.0319581
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 1.0229475
	speed: 0.0184s/iter; left time: 8.9085s
Epoch: 8 cost time: 3.1265475749969482
Epoch: 8, Steps: 194 | Train Loss: 1.0247891 Vali Loss: 1.0586858 Test Loss: 1.0321350
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.0_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0302366018295288, mae:0.8054187893867493
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.0_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0421838
	speed: 0.0164s/iter; left time: 30.2734s
Epoch: 1 cost time: 3.1865086555480957
Epoch: 1, Steps: 194 | Train Loss: 1.0386852 Vali Loss: 1.0551529 Test Loss: 1.0289516
Validation loss decreased (inf --> 1.055153).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0396202
	speed: 0.0166s/iter; left time: 27.3714s
Epoch: 2 cost time: 3.0087950229644775
Epoch: 2, Steps: 194 | Train Loss: 1.0317081 Vali Loss: 1.0529729 Test Loss: 1.0299112
Validation loss decreased (1.055153 --> 1.052973).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0350511
	speed: 0.0156s/iter; left time: 22.6804s
Epoch: 3 cost time: 2.8199143409729004
Epoch: 3, Steps: 194 | Train Loss: 1.0283199 Vali Loss: 1.0566450 Test Loss: 1.0309480
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0206467
	speed: 0.0159s/iter; left time: 20.0342s
Epoch: 4 cost time: 2.5454955101013184
Epoch: 4, Steps: 194 | Train Loss: 1.0253520 Vali Loss: 1.0571947 Test Loss: 1.0316302
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0156522
	speed: 0.0161s/iter; left time: 17.1443s
Epoch: 5 cost time: 2.7218430042266846
Epoch: 5, Steps: 194 | Train Loss: 1.0240880 Vali Loss: 1.0612301 Test Loss: 1.0335543
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0287628
	speed: 0.0150s/iter; left time: 13.0421s
Epoch: 6 cost time: 3.0666446685791016
Epoch: 6, Steps: 194 | Train Loss: 1.0232939 Vali Loss: 1.0608366 Test Loss: 1.0336478
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0159585
	speed: 0.0185s/iter; left time: 12.5160s
Epoch: 7 cost time: 3.9992711544036865
Epoch: 7, Steps: 194 | Train Loss: 1.0229913 Vali Loss: 1.0607349 Test Loss: 1.0339284
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.0_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0299111604690552, mae:0.8052796125411987
Seed: 42
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.95_alpha0.0_96_96Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.95_alpha0.0.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           96                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.0_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.6714519
	speed: 0.0276s/iter; left time: 56.0934s
	iters: 200, epoch: 1 | loss: 0.6699924
	speed: 0.0186s/iter; left time: 35.8907s
Epoch: 1 cost time: 3.9396591186523438
Epoch: 1, Steps: 213 | Train Loss: 0.7071307 Vali Loss: 0.7430485 Test Loss: 0.6709384
Validation loss decreased (inf --> 0.743048).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.6459705
	speed: 0.0183s/iter; left time: 33.2548s
	iters: 200, epoch: 2 | loss: 0.6408365
	speed: 0.0158s/iter; left time: 27.1532s
Epoch: 2 cost time: 3.528989553451538
Epoch: 2, Steps: 213 | Train Loss: 0.6665359 Vali Loss: 0.7299236 Test Loss: 0.6610107
Validation loss decreased (0.743048 --> 0.729924).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.6412328
	speed: 0.0143s/iter; left time: 22.8834s
	iters: 200, epoch: 3 | loss: 0.6559953
	speed: 0.0135s/iter; left time: 20.3598s
Epoch: 3 cost time: 3.021324872970581
Epoch: 3, Steps: 213 | Train Loss: 0.6389399 Vali Loss: 0.7446579 Test Loss: 0.6752985
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.6120035
	speed: 0.0151s/iter; left time: 21.0410s
	iters: 200, epoch: 4 | loss: 0.6428652
	speed: 0.0139s/iter; left time: 17.9882s
Epoch: 4 cost time: 3.163303852081299
Epoch: 4, Steps: 213 | Train Loss: 0.6201462 Vali Loss: 0.7713026 Test Loss: 0.6815638
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.5990282
	speed: 0.0141s/iter; left time: 16.5727s
	iters: 200, epoch: 5 | loss: 0.6120490
	speed: 0.0133s/iter; left time: 14.3228s
Epoch: 5 cost time: 2.9056971073150635
Epoch: 5, Steps: 213 | Train Loss: 0.6110315 Vali Loss: 0.7543443 Test Loss: 0.6842987
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.5922054
	speed: 0.0219s/iter; left time: 21.1786s
	iters: 200, epoch: 6 | loss: 0.5854034
	speed: 0.0223s/iter; left time: 19.2727s
Epoch: 6 cost time: 4.76451849937439
Epoch: 6, Steps: 213 | Train Loss: 0.6060194 Vali Loss: 0.7726387 Test Loss: 0.6869802
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.5899813
	speed: 0.0231s/iter; left time: 17.3934s
	iters: 200, epoch: 7 | loss: 0.5914435
	speed: 0.0189s/iter; left time: 12.3599s
Epoch: 7 cost time: 4.104002237319946
Epoch: 7, Steps: 213 | Train Loss: 0.6043515 Vali Loss: 0.7712371 Test Loss: 0.6881672
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.0_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.6610109210014343, mae:0.6507939100265503
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.0_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.7217253
	speed: 0.0259s/iter; left time: 52.6155s
	iters: 200, epoch: 1 | loss: 0.6561188
	speed: 0.0194s/iter; left time: 37.5119s
Epoch: 1 cost time: 4.1236186027526855
Epoch: 1, Steps: 213 | Train Loss: 0.7026793 Vali Loss: 0.6997033 Test Loss: 0.6627781
Validation loss decreased (inf --> 0.699703).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.6555876
	speed: 0.0213s/iter; left time: 38.6441s
	iters: 200, epoch: 2 | loss: 0.6844587
	speed: 0.0166s/iter; left time: 28.5247s
Epoch: 2 cost time: 3.621333360671997
Epoch: 2, Steps: 213 | Train Loss: 0.6627523 Vali Loss: 0.7569495 Test Loss: 0.6781591
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.6279863
	speed: 0.0231s/iter; left time: 37.0167s
	iters: 200, epoch: 3 | loss: 0.6330383
	speed: 0.0189s/iter; left time: 28.4115s
Epoch: 3 cost time: 4.065508842468262
Epoch: 3, Steps: 213 | Train Loss: 0.6357885 Vali Loss: 0.7624289 Test Loss: 0.6779862
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.6499424
	speed: 0.0192s/iter; left time: 26.6591s
	iters: 200, epoch: 4 | loss: 0.6036631
	speed: 0.0169s/iter; left time: 21.8084s
Epoch: 4 cost time: 3.682356834411621
Epoch: 4, Steps: 213 | Train Loss: 0.6208070 Vali Loss: 0.7893042 Test Loss: 0.6947748
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.6176078
	speed: 0.0210s/iter; left time: 24.7567s
	iters: 200, epoch: 5 | loss: 0.5880291
	speed: 0.0196s/iter; left time: 21.1291s
Epoch: 5 cost time: 4.130069971084595
Epoch: 5, Steps: 213 | Train Loss: 0.6127170 Vali Loss: 0.8131339 Test Loss: 0.7039170
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.6462584
	speed: 0.0143s/iter; left time: 13.8218s
	iters: 200, epoch: 6 | loss: 0.5989310
	speed: 0.0139s/iter; left time: 12.0632s
Epoch: 6 cost time: 3.0632288455963135
Epoch: 6, Steps: 213 | Train Loss: 0.6088009 Vali Loss: 0.8148592 Test Loss: 0.7033175
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.0_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.6627782583236694, mae:0.6516327261924744
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.0_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.7235784
	speed: 0.0211s/iter; left time: 42.8672s
	iters: 200, epoch: 1 | loss: 0.6879035
	speed: 0.0215s/iter; left time: 41.5771s
Epoch: 1 cost time: 4.640382766723633
Epoch: 1, Steps: 213 | Train Loss: 0.7020161 Vali Loss: 0.7177253 Test Loss: 0.6544189
Validation loss decreased (inf --> 0.717725).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.6657819
	speed: 0.0184s/iter; left time: 33.4764s
	iters: 200, epoch: 2 | loss: 0.6669339
	speed: 0.0166s/iter; left time: 28.4426s
Epoch: 2 cost time: 3.6316187381744385
Epoch: 2, Steps: 213 | Train Loss: 0.6577542 Vali Loss: 0.7362161 Test Loss: 0.6715154
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.6294844
	speed: 0.0181s/iter; left time: 29.0279s
	iters: 200, epoch: 3 | loss: 0.6139916
	speed: 0.0158s/iter; left time: 23.8344s
Epoch: 3 cost time: 3.4442691802978516
Epoch: 3, Steps: 213 | Train Loss: 0.6273568 Vali Loss: 0.7376054 Test Loss: 0.6870747
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.6290801
	speed: 0.0161s/iter; left time: 22.4476s
	iters: 200, epoch: 4 | loss: 0.6291744
	speed: 0.0156s/iter; left time: 20.1360s
Epoch: 4 cost time: 3.3745124340057373
Epoch: 4, Steps: 213 | Train Loss: 0.6111171 Vali Loss: 0.7594167 Test Loss: 0.6898223
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.6280538
	speed: 0.0179s/iter; left time: 21.1474s
	iters: 200, epoch: 5 | loss: 0.5861812
	speed: 0.0152s/iter; left time: 16.3940s
Epoch: 5 cost time: 3.2596657276153564
Epoch: 5, Steps: 213 | Train Loss: 0.6027730 Vali Loss: 0.7657231 Test Loss: 0.6934124
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.6182836
	speed: 0.0147s/iter; left time: 14.1928s
	iters: 200, epoch: 6 | loss: 0.5979297
	speed: 0.0127s/iter; left time: 10.9606s
Epoch: 6 cost time: 2.7861177921295166
Epoch: 6, Steps: 213 | Train Loss: 0.5997574 Vali Loss: 0.7697448 Test Loss: 0.6936181
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.0_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.6544190645217896, mae:0.6476901173591614
Seed: 42
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.95_alpha0.0_96_192Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.95_alpha0.0.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           192                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.0_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.7486768
	speed: 0.0400s/iter; left time: 80.1365s
	iters: 200, epoch: 1 | loss: 0.7792295
	speed: 0.0297s/iter; left time: 56.3900s
Epoch: 1 cost time: 6.181735277175903
Epoch: 1, Steps: 210 | Train Loss: 0.7882134 Vali Loss: 0.8814043 Test Loss: 0.7534776
Validation loss decreased (inf --> 0.881404).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.7168917
	speed: 0.0216s/iter; left time: 38.7624s
	iters: 200, epoch: 2 | loss: 0.7157143
	speed: 0.0196s/iter; left time: 33.0960s
Epoch: 2 cost time: 4.1517205238342285
Epoch: 2, Steps: 210 | Train Loss: 0.7286421 Vali Loss: 0.9545947 Test Loss: 0.7940499
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.6731892
	speed: 0.0188s/iter; left time: 29.7453s
	iters: 200, epoch: 3 | loss: 0.6628920
	speed: 0.0155s/iter; left time: 22.9261s
Epoch: 3 cost time: 3.320051908493042
Epoch: 3, Steps: 210 | Train Loss: 0.6867969 Vali Loss: 0.9652450 Test Loss: 0.8006713
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.6709267
	speed: 0.0210s/iter; left time: 28.8547s
	iters: 200, epoch: 4 | loss: 0.6385635
	speed: 0.0211s/iter; left time: 26.8292s
Epoch: 4 cost time: 4.610895156860352
Epoch: 4, Steps: 210 | Train Loss: 0.6652289 Vali Loss: 0.9600973 Test Loss: 0.7707968
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.6485795
	speed: 0.0248s/iter; left time: 28.7441s
	iters: 200, epoch: 5 | loss: 0.6489326
	speed: 0.0208s/iter; left time: 22.0762s
Epoch: 5 cost time: 4.492727518081665
Epoch: 5, Steps: 210 | Train Loss: 0.6543487 Vali Loss: 0.9710336 Test Loss: 0.7803389
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.6640239
	speed: 0.0227s/iter; left time: 21.5946s
	iters: 200, epoch: 6 | loss: 0.6577363
	speed: 0.0174s/iter; left time: 14.8173s
Epoch: 6 cost time: 3.6464409828186035
Epoch: 6, Steps: 210 | Train Loss: 0.6492743 Vali Loss: 0.9704695 Test Loss: 0.7881296
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.0_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.7534775137901306, mae:0.689044177532196
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.0_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.8515937
	speed: 0.0147s/iter; left time: 29.3174s
	iters: 200, epoch: 1 | loss: 0.7332853
	speed: 0.0142s/iter; left time: 27.0410s
Epoch: 1 cost time: 3.205594778060913
Epoch: 1, Steps: 210 | Train Loss: 0.8002233 Vali Loss: 0.9328035 Test Loss: 0.7690380
Validation loss decreased (inf --> 0.932803).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.7020500
	speed: 0.0201s/iter; left time: 36.0380s
	iters: 200, epoch: 2 | loss: 0.6901517
	speed: 0.0199s/iter; left time: 33.7124s
Epoch: 2 cost time: 4.2602434158325195
Epoch: 2, Steps: 210 | Train Loss: 0.7260421 Vali Loss: 0.8966351 Test Loss: 0.7660971
Validation loss decreased (0.932803 --> 0.896635).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.6782339
	speed: 0.0192s/iter; left time: 30.4150s
	iters: 200, epoch: 3 | loss: 0.6781198
	speed: 0.0165s/iter; left time: 24.3860s
Epoch: 3 cost time: 3.4861643314361572
Epoch: 3, Steps: 210 | Train Loss: 0.6834193 Vali Loss: 0.8896754 Test Loss: 0.8037949
Validation loss decreased (0.896635 --> 0.889675).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.6752710
	speed: 0.0182s/iter; left time: 24.8940s
	iters: 200, epoch: 4 | loss: 0.6500396
	speed: 0.0158s/iter; left time: 20.0494s
Epoch: 4 cost time: 3.4048516750335693
Epoch: 4, Steps: 210 | Train Loss: 0.6582682 Vali Loss: 0.8105468 Test Loss: 0.7920654
Validation loss decreased (0.889675 --> 0.810547).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.6499510
	speed: 0.0166s/iter; left time: 19.2913s
	iters: 200, epoch: 5 | loss: 0.6622149
	speed: 0.0138s/iter; left time: 14.6743s
Epoch: 5 cost time: 2.9462146759033203
Epoch: 5, Steps: 210 | Train Loss: 0.6456120 Vali Loss: 0.8427538 Test Loss: 0.7805526
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.6355313
	speed: 0.0160s/iter; left time: 15.2616s
	iters: 200, epoch: 6 | loss: 0.6445464
	speed: 0.0141s/iter; left time: 11.9595s
Epoch: 6 cost time: 3.051332950592041
Epoch: 6, Steps: 210 | Train Loss: 0.6387367 Vali Loss: 0.8255130 Test Loss: 0.7790610
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.6544443
	speed: 0.0220s/iter; left time: 16.2664s
	iters: 200, epoch: 7 | loss: 0.6276851
	speed: 0.0185s/iter; left time: 11.8808s
Epoch: 7 cost time: 3.9662559032440186
Epoch: 7, Steps: 210 | Train Loss: 0.6348149 Vali Loss: 0.8319722 Test Loss: 0.7835207
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.6276171
	speed: 0.0162s/iter; left time: 8.6225s
	iters: 200, epoch: 8 | loss: 0.6446415
	speed: 0.0159s/iter; left time: 6.8564s
Epoch: 8 cost time: 3.4344053268432617
Epoch: 8, Steps: 210 | Train Loss: 0.6344420 Vali Loss: 0.8295127 Test Loss: 0.7857193
EarlyStopping counter: 4 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.6292299
	speed: 0.0145s/iter; left time: 4.6555s
	iters: 200, epoch: 9 | loss: 0.6680506
	speed: 0.0168s/iter; left time: 3.7055s
Epoch: 9 cost time: 3.6190078258514404
Epoch: 9, Steps: 210 | Train Loss: 0.6346895 Vali Loss: 0.8236350 Test Loss: 0.7851816
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.0_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.7920653820037842, mae:0.7080459594726562
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.0_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.7324499
	speed: 0.0203s/iter; left time: 40.5398s
	iters: 200, epoch: 1 | loss: 0.7312385
	speed: 0.0167s/iter; left time: 31.7175s
Epoch: 1 cost time: 3.571781873703003
Epoch: 1, Steps: 210 | Train Loss: 0.7876362 Vali Loss: 0.9113783 Test Loss: 0.7644557
Validation loss decreased (inf --> 0.911378).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.7900766
	speed: 0.0171s/iter; left time: 30.6013s
	iters: 200, epoch: 2 | loss: 0.6954758
	speed: 0.0223s/iter; left time: 37.6448s
Epoch: 2 cost time: 4.672322750091553
Epoch: 2, Steps: 210 | Train Loss: 0.7210738 Vali Loss: 0.9080498 Test Loss: 0.7811588
Validation loss decreased (0.911378 --> 0.908050).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.7071597
	speed: 0.0201s/iter; left time: 31.8373s
	iters: 200, epoch: 3 | loss: 0.6589903
	speed: 0.0185s/iter; left time: 27.4186s
Epoch: 3 cost time: 3.9297118186950684
Epoch: 3, Steps: 210 | Train Loss: 0.6797347 Vali Loss: 0.9299268 Test Loss: 0.7989870
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.6519526
	speed: 0.0154s/iter; left time: 21.0853s
	iters: 200, epoch: 4 | loss: 0.6379711
	speed: 0.0138s/iter; left time: 17.5825s
Epoch: 4 cost time: 3.015822410583496
Epoch: 4, Steps: 210 | Train Loss: 0.6599770 Vali Loss: 0.9423020 Test Loss: 0.7820349
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.6819817
	speed: 0.0165s/iter; left time: 19.1292s
	iters: 200, epoch: 5 | loss: 0.6304884
	speed: 0.0160s/iter; left time: 16.9602s
Epoch: 5 cost time: 3.4670324325561523
Epoch: 5, Steps: 210 | Train Loss: 0.6494573 Vali Loss: 0.9295629 Test Loss: 0.7859127
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.6612983
	speed: 0.0174s/iter; left time: 16.5296s
	iters: 200, epoch: 6 | loss: 0.6532418
	speed: 0.0160s/iter; left time: 13.5847s
Epoch: 6 cost time: 3.3975589275360107
Epoch: 6, Steps: 210 | Train Loss: 0.6451356 Vali Loss: 0.9485788 Test Loss: 0.7889246
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.6690395
	speed: 0.0212s/iter; left time: 15.6920s
	iters: 200, epoch: 7 | loss: 0.6412689
	speed: 0.0175s/iter; left time: 11.1906s
Epoch: 7 cost time: 3.7100319862365723
Epoch: 7, Steps: 210 | Train Loss: 0.6425738 Vali Loss: 0.9318813 Test Loss: 0.7837747
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.0_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.7811587452888489, mae:0.7021428346633911
Seed: 42
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.95_alpha0.0_96_336Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.95_alpha0.0.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           336                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.0_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.8696967
	speed: 0.0290s/iter; left time: 56.7831s
	iters: 200, epoch: 1 | loss: 0.8875862
	speed: 0.0206s/iter; left time: 38.3199s
Epoch: 1 cost time: 4.296038389205933
Epoch: 1, Steps: 206 | Train Loss: 0.8914040 Vali Loss: 1.0473994 Test Loss: 0.8259971
Validation loss decreased (inf --> 1.047399).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.7912391
	speed: 0.0145s/iter; left time: 25.3920s
	iters: 200, epoch: 2 | loss: 0.7901474
	speed: 0.0131s/iter; left time: 21.6713s
Epoch: 2 cost time: 2.733675718307495
Epoch: 2, Steps: 206 | Train Loss: 0.8049209 Vali Loss: 1.0241314 Test Loss: 0.8068432
Validation loss decreased (1.047399 --> 1.024131).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.7530267
	speed: 0.0127s/iter; left time: 19.7273s
	iters: 200, epoch: 3 | loss: 0.7354916
	speed: 0.0105s/iter; left time: 15.2817s
Epoch: 3 cost time: 2.3198111057281494
Epoch: 3, Steps: 206 | Train Loss: 0.7382263 Vali Loss: 1.0308695 Test Loss: 0.8515599
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.7000443
	speed: 0.0157s/iter; left time: 21.0706s
	iters: 200, epoch: 4 | loss: 0.7230325
	speed: 0.0139s/iter; left time: 17.2693s
Epoch: 4 cost time: 2.9416253566741943
Epoch: 4, Steps: 206 | Train Loss: 0.7076900 Vali Loss: 1.0504795 Test Loss: 0.8480388
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.6704378
	speed: 0.0181s/iter; left time: 20.5645s
	iters: 200, epoch: 5 | loss: 0.6637667
	speed: 0.0180s/iter; left time: 18.7009s
Epoch: 5 cost time: 3.7968220710754395
Epoch: 5, Steps: 206 | Train Loss: 0.6942502 Vali Loss: 1.0681669 Test Loss: 0.8395258
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.6818763
	speed: 0.0220s/iter; left time: 20.4950s
	iters: 200, epoch: 6 | loss: 0.7180593
	speed: 0.0193s/iter; left time: 16.0589s
Epoch: 6 cost time: 4.054680109024048
Epoch: 6, Steps: 206 | Train Loss: 0.6884315 Vali Loss: 1.0731503 Test Loss: 0.8407509
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.6800562
	speed: 0.0177s/iter; left time: 12.8619s
	iters: 200, epoch: 7 | loss: 0.6906268
	speed: 0.0160s/iter; left time: 10.0232s
Epoch: 7 cost time: 3.4181480407714844
Epoch: 7, Steps: 206 | Train Loss: 0.6860489 Vali Loss: 1.0781523 Test Loss: 0.8427551
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.0_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.8068431615829468, mae:0.7139073014259338
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.0_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.8936782
	speed: 0.0165s/iter; left time: 32.2673s
	iters: 200, epoch: 1 | loss: 0.8422049
	speed: 0.0147s/iter; left time: 27.4357s
Epoch: 1 cost time: 3.0721840858459473
Epoch: 1, Steps: 206 | Train Loss: 0.8979297 Vali Loss: 1.1053244 Test Loss: 0.8187644
Validation loss decreased (inf --> 1.105324).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.8093823
	speed: 0.0172s/iter; left time: 30.2170s
	iters: 200, epoch: 2 | loss: 0.7700776
	speed: 0.0155s/iter; left time: 25.6839s
Epoch: 2 cost time: 3.3133134841918945
Epoch: 2, Steps: 206 | Train Loss: 0.8140025 Vali Loss: 1.0756567 Test Loss: 0.8442502
Validation loss decreased (1.105324 --> 1.075657).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.7402921
	speed: 0.0180s/iter; left time: 27.9483s
	iters: 200, epoch: 3 | loss: 0.7471506
	speed: 0.0174s/iter; left time: 25.2221s
Epoch: 3 cost time: 3.6935365200042725
Epoch: 3, Steps: 206 | Train Loss: 0.7497068 Vali Loss: 1.0383999 Test Loss: 0.8093959
Validation loss decreased (1.075657 --> 1.038400).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.7134306
	speed: 0.0174s/iter; left time: 23.3575s
	iters: 200, epoch: 4 | loss: 0.6905560
	speed: 0.0214s/iter; left time: 26.5812s
Epoch: 4 cost time: 4.4968485832214355
Epoch: 4, Steps: 206 | Train Loss: 0.7068925 Vali Loss: 1.0509990 Test Loss: 0.8408172
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.6986973
	speed: 0.0221s/iter; left time: 25.1440s
	iters: 200, epoch: 5 | loss: 0.6680723
	speed: 0.0164s/iter; left time: 17.0585s
Epoch: 5 cost time: 3.4713029861450195
Epoch: 5, Steps: 206 | Train Loss: 0.6879664 Vali Loss: 1.0494014 Test Loss: 0.8440695
EarlyStopping counter: 2 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.6763397
	speed: 0.0243s/iter; left time: 22.6262s
	iters: 200, epoch: 6 | loss: 0.6761194
	speed: 0.0187s/iter; left time: 15.5761s
Epoch: 6 cost time: 3.915069580078125
Epoch: 6, Steps: 206 | Train Loss: 0.6813545 Vali Loss: 1.0621907 Test Loss: 0.8421592
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.6700068
	speed: 0.0198s/iter; left time: 14.3314s
	iters: 200, epoch: 7 | loss: 0.7170856
	speed: 0.0172s/iter; left time: 10.7700s
Epoch: 7 cost time: 3.6455206871032715
Epoch: 7, Steps: 206 | Train Loss: 0.6773564 Vali Loss: 1.0644230 Test Loss: 0.8467115
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.6517882
	speed: 0.0198s/iter; left time: 10.2770s
	iters: 200, epoch: 8 | loss: 0.6603033
	speed: 0.0173s/iter; left time: 7.2376s
Epoch: 8 cost time: 3.611722946166992
Epoch: 8, Steps: 206 | Train Loss: 0.6750245 Vali Loss: 1.0632905 Test Loss: 0.8447006
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.0_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.8093959093093872, mae:0.7176028490066528
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.0_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.8168485
	speed: 0.0147s/iter; left time: 28.8369s
	iters: 200, epoch: 1 | loss: 0.8501951
	speed: 0.0126s/iter; left time: 23.4448s
Epoch: 1 cost time: 2.6798222064971924
Epoch: 1, Steps: 206 | Train Loss: 0.8857603 Vali Loss: 1.0119746 Test Loss: 0.8364881
Validation loss decreased (inf --> 1.011975).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.7731788
	speed: 0.0193s/iter; left time: 33.9587s
	iters: 200, epoch: 2 | loss: 0.7514083
	speed: 0.0159s/iter; left time: 26.3412s
Epoch: 2 cost time: 3.407956600189209
Epoch: 2, Steps: 206 | Train Loss: 0.8025529 Vali Loss: 1.0401706 Test Loss: 0.8021223
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.7153287
	speed: 0.0165s/iter; left time: 25.5292s
	iters: 200, epoch: 3 | loss: 0.7541730
	speed: 0.0155s/iter; left time: 22.3909s
Epoch: 3 cost time: 3.3191206455230713
Epoch: 3, Steps: 206 | Train Loss: 0.7455677 Vali Loss: 1.0315058 Test Loss: 0.8097670
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.6916695
	speed: 0.0214s/iter; left time: 28.7701s
	iters: 200, epoch: 4 | loss: 0.7165972
	speed: 0.0175s/iter; left time: 21.8104s
Epoch: 4 cost time: 3.6986751556396484
Epoch: 4, Steps: 206 | Train Loss: 0.7184739 Vali Loss: 1.0880207 Test Loss: 0.8250697
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.7065172
	speed: 0.0225s/iter; left time: 25.5841s
	iters: 200, epoch: 5 | loss: 0.7046565
	speed: 0.0185s/iter; left time: 19.2160s
Epoch: 5 cost time: 3.851562976837158
Epoch: 5, Steps: 206 | Train Loss: 0.7037445 Vali Loss: 1.0998675 Test Loss: 0.8255639
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.6827435
	speed: 0.0188s/iter; left time: 17.4918s
	iters: 200, epoch: 6 | loss: 0.7042387
	speed: 0.0169s/iter; left time: 14.0638s
Epoch: 6 cost time: 3.5720152854919434
Epoch: 6, Steps: 206 | Train Loss: 0.6971055 Vali Loss: 1.1059128 Test Loss: 0.8338002
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.0_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.8364880681037903, mae:0.7253454327583313
Seed: 42
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.95_alpha0.0_96_720Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.95_alpha0.0.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           720                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.0_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0087709
	speed: 0.0324s/iter; left time: 59.6184s
Epoch: 1 cost time: 5.178020715713501
Epoch: 1, Steps: 194 | Train Loss: 1.0940143 Vali Loss: 1.8740305 Test Loss: 0.9736391
Validation loss decreased (inf --> 1.874030).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.9488461
	speed: 0.0192s/iter; left time: 31.6697s
Epoch: 2 cost time: 2.9461376667022705
Epoch: 2, Steps: 194 | Train Loss: 0.9641458 Vali Loss: 1.9662277 Test Loss: 1.0256397
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.8019590
	speed: 0.0140s/iter; left time: 20.4007s
Epoch: 3 cost time: 2.7931723594665527
Epoch: 3, Steps: 194 | Train Loss: 0.8455556 Vali Loss: 1.9165450 Test Loss: 1.0210888
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.8523268
	speed: 0.0187s/iter; left time: 23.5815s
Epoch: 4 cost time: 3.437368869781494
Epoch: 4, Steps: 194 | Train Loss: 0.8051743 Vali Loss: 1.9201007 Test Loss: 1.0178211
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.7402750
	speed: 0.0222s/iter; left time: 23.6180s
Epoch: 5 cost time: 3.5273194313049316
Epoch: 5, Steps: 194 | Train Loss: 0.7888766 Vali Loss: 1.9311216 Test Loss: 1.0235759
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.7498679
	speed: 0.0172s/iter; left time: 14.9826s
Epoch: 6 cost time: 2.9604954719543457
Epoch: 6, Steps: 194 | Train Loss: 0.7808185 Vali Loss: 1.9761007 Test Loss: 1.0279377
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.0_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.9736391305923462, mae:0.7870190739631653
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.0_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0244614
	speed: 0.0150s/iter; left time: 27.6031s
Epoch: 1 cost time: 2.5051798820495605
Epoch: 1, Steps: 194 | Train Loss: 1.0940844 Vali Loss: 1.9477029 Test Loss: 0.9317607
Validation loss decreased (inf --> 1.947703).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.8409883
	speed: 0.0173s/iter; left time: 28.4635s
Epoch: 2 cost time: 3.0752367973327637
Epoch: 2, Steps: 194 | Train Loss: 0.9454403 Vali Loss: 2.1384234 Test Loss: 1.0548731
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.7678649
	speed: 0.0147s/iter; left time: 21.2887s
Epoch: 3 cost time: 2.7564282417297363
Epoch: 3, Steps: 194 | Train Loss: 0.8321016 Vali Loss: 2.0109279 Test Loss: 1.1172093
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.7756779
	speed: 0.0140s/iter; left time: 17.6588s
Epoch: 4 cost time: 2.9149563312530518
Epoch: 4, Steps: 194 | Train Loss: 0.7856723 Vali Loss: 1.9090457 Test Loss: 1.1327525
Validation loss decreased (1.947703 --> 1.909046).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.8017558
	speed: 0.0247s/iter; left time: 26.2856s
Epoch: 5 cost time: 4.063425779342651
Epoch: 5, Steps: 194 | Train Loss: 0.7647094 Vali Loss: 1.9414420 Test Loss: 1.1532264
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.7578380
	speed: 0.0134s/iter; left time: 11.6778s
Epoch: 6 cost time: 2.9436402320861816
Epoch: 6, Steps: 194 | Train Loss: 0.7551483 Vali Loss: 1.9590244 Test Loss: 1.1544527
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.7590374
	speed: 0.0136s/iter; left time: 9.2409s
Epoch: 7 cost time: 2.879951238632202
Epoch: 7, Steps: 194 | Train Loss: 0.7523017 Vali Loss: 1.9294209 Test Loss: 1.1616038
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.7320547
	speed: 0.0156s/iter; left time: 7.5475s
Epoch: 8 cost time: 2.895738363265991
Epoch: 8, Steps: 194 | Train Loss: 0.7498851 Vali Loss: 1.9217733 Test Loss: 1.1616836
EarlyStopping counter: 4 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.7229283
	speed: 0.0217s/iter; left time: 6.2776s
Epoch: 9 cost time: 3.915637969970703
Epoch: 9, Steps: 194 | Train Loss: 0.7487147 Vali Loss: 1.9459529 Test Loss: 1.1638652
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.0_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.132752776145935, mae:0.8382830619812012
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.0_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0239886
	speed: 0.0181s/iter; left time: 33.3575s
Epoch: 1 cost time: 3.400550365447998
Epoch: 1, Steps: 194 | Train Loss: 1.0926520 Vali Loss: 1.7903277 Test Loss: 0.9598339
Validation loss decreased (inf --> 1.790328).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.9294158
	speed: 0.0261s/iter; left time: 42.9711s
Epoch: 2 cost time: 3.8283190727233887
Epoch: 2, Steps: 194 | Train Loss: 0.9118094 Vali Loss: 2.0449221 Test Loss: 1.1324921
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.9593952
	speed: 0.0183s/iter; left time: 26.5641s
Epoch: 3 cost time: 2.88362455368042
Epoch: 3, Steps: 194 | Train Loss: 0.8214042 Vali Loss: 2.1040595 Test Loss: 1.1065493
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.7829900
	speed: 0.0202s/iter; left time: 25.4937s
Epoch: 4 cost time: 3.3961098194122314
Epoch: 4, Steps: 194 | Train Loss: 0.7799032 Vali Loss: 2.0571597 Test Loss: 1.1519811
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.7599586
	speed: 0.0166s/iter; left time: 17.6969s
Epoch: 5 cost time: 2.8169009685516357
Epoch: 5, Steps: 194 | Train Loss: 0.7625709 Vali Loss: 2.1425209 Test Loss: 1.1491528
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.7802572
	speed: 0.0181s/iter; left time: 15.7777s
Epoch: 6 cost time: 2.8900973796844482
Epoch: 6, Steps: 194 | Train Loss: 0.7530548 Vali Loss: 2.1171446 Test Loss: 1.1653539
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.0_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.9598338007926941, mae:0.7820351719856262
Seed: 123
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.95_alpha0.0_96_96Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.95_alpha0.0.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           96                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.0_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.6852736
	speed: 0.0286s/iter; left time: 58.0703s
	iters: 200, epoch: 1 | loss: 0.6867605
	speed: 0.0203s/iter; left time: 39.2218s
Epoch: 1 cost time: 4.255005359649658
Epoch: 1, Steps: 213 | Train Loss: 0.7048083 Vali Loss: 0.7132965 Test Loss: 0.6691772
Validation loss decreased (inf --> 0.713296).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.6719849
	speed: 0.0222s/iter; left time: 40.2854s
	iters: 200, epoch: 2 | loss: 0.6737692
	speed: 0.0163s/iter; left time: 28.0351s
Epoch: 2 cost time: 3.4736111164093018
Epoch: 2, Steps: 213 | Train Loss: 0.6594215 Vali Loss: 0.7614814 Test Loss: 0.6894308
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.6134131
	speed: 0.0173s/iter; left time: 27.7536s
	iters: 200, epoch: 3 | loss: 0.6612854
	speed: 0.0153s/iter; left time: 23.0337s
Epoch: 3 cost time: 3.4280338287353516
Epoch: 3, Steps: 213 | Train Loss: 0.6305035 Vali Loss: 0.7837500 Test Loss: 0.6905951
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.6150036
	speed: 0.0178s/iter; left time: 24.7720s
	iters: 200, epoch: 4 | loss: 0.6000650
	speed: 0.0155s/iter; left time: 19.9865s
Epoch: 4 cost time: 3.3830811977386475
Epoch: 4, Steps: 213 | Train Loss: 0.6130946 Vali Loss: 0.7818248 Test Loss: 0.6945069
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.6062474
	speed: 0.0163s/iter; left time: 19.1771s
	iters: 200, epoch: 5 | loss: 0.6269016
	speed: 0.0141s/iter; left time: 15.1693s
Epoch: 5 cost time: 3.1246635913848877
Epoch: 5, Steps: 213 | Train Loss: 0.6039474 Vali Loss: 0.7886106 Test Loss: 0.6902114
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.6093385
	speed: 0.0199s/iter; left time: 19.2149s
	iters: 200, epoch: 6 | loss: 0.6148138
	speed: 0.0171s/iter; left time: 14.8495s
Epoch: 6 cost time: 3.6507036685943604
Epoch: 6, Steps: 213 | Train Loss: 0.6007739 Vali Loss: 0.7832651 Test Loss: 0.6965480
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.0_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.6691772937774658, mae:0.6548446416854858
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.0_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.7186244
	speed: 0.0201s/iter; left time: 40.8607s
	iters: 200, epoch: 1 | loss: 0.6850817
	speed: 0.0152s/iter; left time: 29.2628s
Epoch: 1 cost time: 3.199690580368042
Epoch: 1, Steps: 213 | Train Loss: 0.7070661 Vali Loss: 0.7198297 Test Loss: 0.6645361
Validation loss decreased (inf --> 0.719830).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.7010440
	speed: 0.0205s/iter; left time: 37.3143s
	iters: 200, epoch: 2 | loss: 0.6177553
	speed: 0.0150s/iter; left time: 25.8353s
Epoch: 2 cost time: 3.239579439163208
Epoch: 2, Steps: 213 | Train Loss: 0.6628209 Vali Loss: 0.7468452 Test Loss: 0.6767728
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.6187679
	speed: 0.0219s/iter; left time: 35.1650s
	iters: 200, epoch: 3 | loss: 0.6494592
	speed: 0.0167s/iter; left time: 25.1444s
Epoch: 3 cost time: 3.555745840072632
Epoch: 3, Steps: 213 | Train Loss: 0.6323439 Vali Loss: 0.7506680 Test Loss: 0.6829033
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.6179313
	speed: 0.0212s/iter; left time: 29.4929s
	iters: 200, epoch: 4 | loss: 0.5943928
	speed: 0.0179s/iter; left time: 23.0644s
Epoch: 4 cost time: 3.7855491638183594
Epoch: 4, Steps: 213 | Train Loss: 0.6162472 Vali Loss: 0.7706971 Test Loss: 0.7057266
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.5920788
	speed: 0.0205s/iter; left time: 24.1402s
	iters: 200, epoch: 5 | loss: 0.6220016
	speed: 0.0182s/iter; left time: 19.6609s
Epoch: 5 cost time: 3.875333309173584
Epoch: 5, Steps: 213 | Train Loss: 0.6071617 Vali Loss: 0.7489315 Test Loss: 0.7057191
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.6070325
	speed: 0.0173s/iter; left time: 16.7303s
	iters: 200, epoch: 6 | loss: 0.6176239
	speed: 0.0136s/iter; left time: 11.7726s
Epoch: 6 cost time: 2.9418585300445557
Epoch: 6, Steps: 213 | Train Loss: 0.6034883 Vali Loss: 0.7661771 Test Loss: 0.7058873
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.0_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.6645360589027405, mae:0.6529206037521362
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.0_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.7475777
	speed: 0.0148s/iter; left time: 30.0547s
	iters: 200, epoch: 1 | loss: 0.6685113
	speed: 0.0124s/iter; left time: 23.9317s
Epoch: 1 cost time: 2.711071014404297
Epoch: 1, Steps: 213 | Train Loss: 0.7055918 Vali Loss: 0.7386176 Test Loss: 0.6724048
Validation loss decreased (inf --> 0.738618).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.6393509
	speed: 0.0153s/iter; left time: 27.8025s
	iters: 200, epoch: 2 | loss: 0.6060119
	speed: 0.0122s/iter; left time: 20.9165s
Epoch: 2 cost time: 2.7541255950927734
Epoch: 2, Steps: 213 | Train Loss: 0.6545372 Vali Loss: 0.7601895 Test Loss: 0.6825053
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.6240821
	speed: 0.0157s/iter; left time: 25.1627s
	iters: 200, epoch: 3 | loss: 0.6113762
	speed: 0.0159s/iter; left time: 23.9854s
Epoch: 3 cost time: 3.4572246074676514
Epoch: 3, Steps: 213 | Train Loss: 0.6256998 Vali Loss: 0.7366896 Test Loss: 0.6941712
Validation loss decreased (0.738618 --> 0.736690).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.6116506
	speed: 0.0177s/iter; left time: 24.6918s
	iters: 200, epoch: 4 | loss: 0.6152502
	speed: 0.0161s/iter; left time: 20.7716s
Epoch: 4 cost time: 3.5345444679260254
Epoch: 4, Steps: 213 | Train Loss: 0.6092751 Vali Loss: 0.7359653 Test Loss: 0.6891137
Validation loss decreased (0.736690 --> 0.735965).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.5822184
	speed: 0.0169s/iter; left time: 19.8767s
	iters: 200, epoch: 5 | loss: 0.5793309
	speed: 0.0164s/iter; left time: 17.7134s
Epoch: 5 cost time: 3.8288869857788086
Epoch: 5, Steps: 213 | Train Loss: 0.6009502 Vali Loss: 0.7293061 Test Loss: 0.6972100
Validation loss decreased (0.735965 --> 0.729306).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.5795248
	speed: 0.0157s/iter; left time: 15.1804s
	iters: 200, epoch: 6 | loss: 0.5665222
	speed: 0.0143s/iter; left time: 12.4244s
Epoch: 6 cost time: 3.0824081897735596
Epoch: 6, Steps: 213 | Train Loss: 0.5975172 Vali Loss: 0.7379677 Test Loss: 0.6988081
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.6148020
	speed: 0.0176s/iter; left time: 13.2300s
	iters: 200, epoch: 7 | loss: 0.5942374
	speed: 0.0166s/iter; left time: 10.8453s
Epoch: 7 cost time: 3.6252498626708984
Epoch: 7, Steps: 213 | Train Loss: 0.5960723 Vali Loss: 0.7324881 Test Loss: 0.6967754
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.6150246
	speed: 0.0201s/iter; left time: 10.8628s
	iters: 200, epoch: 8 | loss: 0.6157341
	speed: 0.0204s/iter; left time: 8.9563s
Epoch: 8 cost time: 4.473985433578491
Epoch: 8, Steps: 213 | Train Loss: 0.5948644 Vali Loss: 0.7344494 Test Loss: 0.6961507
EarlyStopping counter: 3 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.5816633
	speed: 0.0120s/iter; left time: 3.9216s
	iters: 200, epoch: 9 | loss: 0.5598625
	speed: 0.0112s/iter; left time: 2.5473s
Epoch: 9 cost time: 2.4933018684387207
Epoch: 9, Steps: 213 | Train Loss: 0.5937517 Vali Loss: 0.7340917 Test Loss: 0.6956887
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 0.5695262
	speed: 0.0186s/iter; left time: 2.1253s
	iters: 200, epoch: 10 | loss: 0.5960266
	speed: 0.0165s/iter; left time: 0.2307s
Epoch: 10 cost time: 3.652549982070923
Epoch: 10, Steps: 213 | Train Loss: 0.5940435 Vali Loss: 0.7337863 Test Loss: 0.6957609
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.0_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.6972101926803589, mae:0.6667343974113464
Seed: 123
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.95_alpha0.0_96_192Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.95_alpha0.0.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           192                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.0_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.8265840
	speed: 0.0288s/iter; left time: 57.6979s
	iters: 200, epoch: 1 | loss: 0.8365476
	speed: 0.0201s/iter; left time: 38.2189s
Epoch: 1 cost time: 4.368018388748169
Epoch: 1, Steps: 210 | Train Loss: 0.7872938 Vali Loss: 0.9149835 Test Loss: 0.7756072
Validation loss decreased (inf --> 0.914984).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.6732368
	speed: 0.0198s/iter; left time: 35.4830s
	iters: 200, epoch: 2 | loss: 0.6753833
	speed: 0.0183s/iter; left time: 30.9146s
Epoch: 2 cost time: 4.0699684619903564
Epoch: 2, Steps: 210 | Train Loss: 0.7222284 Vali Loss: 0.9419130 Test Loss: 0.7640077
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.6711771
	speed: 0.0155s/iter; left time: 24.5266s
	iters: 200, epoch: 3 | loss: 0.6869473
	speed: 0.0149s/iter; left time: 22.0826s
Epoch: 3 cost time: 3.250551223754883
Epoch: 3, Steps: 210 | Train Loss: 0.6812499 Vali Loss: 0.9041755 Test Loss: 0.7677056
Validation loss decreased (0.914984 --> 0.904175).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.6386660
	speed: 0.0203s/iter; left time: 27.7932s
	iters: 200, epoch: 4 | loss: 0.6239280
	speed: 0.0185s/iter; left time: 23.5151s
Epoch: 4 cost time: 3.9210197925567627
Epoch: 4, Steps: 210 | Train Loss: 0.6571274 Vali Loss: 0.9511562 Test Loss: 0.8012434
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.6533307
	speed: 0.0203s/iter; left time: 23.5321s
	iters: 200, epoch: 5 | loss: 0.6016058
	speed: 0.0170s/iter; left time: 17.9888s
Epoch: 5 cost time: 3.6779935359954834
Epoch: 5, Steps: 210 | Train Loss: 0.6456380 Vali Loss: 0.9799012 Test Loss: 0.8002156
EarlyStopping counter: 2 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.6292051
	speed: 0.0232s/iter; left time: 22.0405s
	iters: 200, epoch: 6 | loss: 0.6108138
	speed: 0.0191s/iter; left time: 16.2522s
Epoch: 6 cost time: 4.011389255523682
Epoch: 6, Steps: 210 | Train Loss: 0.6407956 Vali Loss: 0.9586190 Test Loss: 0.8034416
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.6443514
	speed: 0.0180s/iter; left time: 13.3667s
	iters: 200, epoch: 7 | loss: 0.6394351
	speed: 0.0154s/iter; left time: 9.8423s
Epoch: 7 cost time: 3.2629079818725586
Epoch: 7, Steps: 210 | Train Loss: 0.6377365 Vali Loss: 0.9706236 Test Loss: 0.7974498
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.6476536
	speed: 0.0141s/iter; left time: 7.4833s
	iters: 200, epoch: 8 | loss: 0.6320739
	speed: 0.0116s/iter; left time: 5.0030s
Epoch: 8 cost time: 2.483588933944702
Epoch: 8, Steps: 210 | Train Loss: 0.6371292 Vali Loss: 0.9632073 Test Loss: 0.7988368
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.0_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.767705500125885, mae:0.6975241303443909
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.0_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.7933344
	speed: 0.0180s/iter; left time: 35.9265s
	iters: 200, epoch: 1 | loss: 0.7413614
	speed: 0.0167s/iter; left time: 31.6989s
Epoch: 1 cost time: 3.5695114135742188
Epoch: 1, Steps: 210 | Train Loss: 0.7891727 Vali Loss: 0.8634471 Test Loss: 0.7636763
Validation loss decreased (inf --> 0.863447).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.7120838
	speed: 0.0181s/iter; left time: 32.4242s
	iters: 200, epoch: 2 | loss: 0.6837823
	speed: 0.0152s/iter; left time: 25.7775s
Epoch: 2 cost time: 3.299262285232544
Epoch: 2, Steps: 210 | Train Loss: 0.7304128 Vali Loss: 0.9241803 Test Loss: 0.7638543
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.6680945
	speed: 0.0173s/iter; left time: 27.4058s
	iters: 200, epoch: 3 | loss: 0.6951938
	speed: 0.0160s/iter; left time: 23.6674s
Epoch: 3 cost time: 3.4222159385681152
Epoch: 3, Steps: 210 | Train Loss: 0.6893822 Vali Loss: 0.9485194 Test Loss: 0.7623816
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.7046069
	speed: 0.0202s/iter; left time: 27.6556s
	iters: 200, epoch: 4 | loss: 0.6723523
	speed: 0.0153s/iter; left time: 19.4395s
Epoch: 4 cost time: 3.257272243499756
Epoch: 4, Steps: 210 | Train Loss: 0.6653974 Vali Loss: 0.9291054 Test Loss: 0.7730373
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.6351354
	speed: 0.0184s/iter; left time: 21.3258s
	iters: 200, epoch: 5 | loss: 0.6125017
	speed: 0.0163s/iter; left time: 17.2438s
Epoch: 5 cost time: 3.610320568084717
Epoch: 5, Steps: 210 | Train Loss: 0.6551327 Vali Loss: 0.9575328 Test Loss: 0.7789132
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.6710688
	speed: 0.0261s/iter; left time: 24.8249s
	iters: 200, epoch: 6 | loss: 0.6198450
	speed: 0.0217s/iter; left time: 18.4748s
Epoch: 6 cost time: 4.607711315155029
Epoch: 6, Steps: 210 | Train Loss: 0.6499836 Vali Loss: 0.9525586 Test Loss: 0.7781646
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.0_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.7636761665344238, mae:0.6937826871871948
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.0_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.7644043
	speed: 0.0184s/iter; left time: 36.8361s
	iters: 200, epoch: 1 | loss: 0.7712545
	speed: 0.0163s/iter; left time: 30.8916s
Epoch: 1 cost time: 3.5123722553253174
Epoch: 1, Steps: 210 | Train Loss: 0.7997026 Vali Loss: 1.0033877 Test Loss: 0.7697332
Validation loss decreased (inf --> 1.003388).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.7370974
	speed: 0.0128s/iter; left time: 22.8910s
	iters: 200, epoch: 2 | loss: 0.7258350
	speed: 0.0118s/iter; left time: 19.9997s
Epoch: 2 cost time: 2.741025924682617
Epoch: 2, Steps: 210 | Train Loss: 0.7354213 Vali Loss: 1.0220407 Test Loss: 0.7790375
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.7117857
	speed: 0.0211s/iter; left time: 33.3003s
	iters: 200, epoch: 3 | loss: 0.6807539
	speed: 0.0187s/iter; left time: 27.6557s
Epoch: 3 cost time: 4.000462055206299
Epoch: 3, Steps: 210 | Train Loss: 0.6931984 Vali Loss: 1.0281751 Test Loss: 0.7902967
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.6559271
	speed: 0.0181s/iter; left time: 24.7657s
	iters: 200, epoch: 4 | loss: 0.6699842
	speed: 0.0195s/iter; left time: 24.7529s
Epoch: 4 cost time: 4.215667247772217
Epoch: 4, Steps: 210 | Train Loss: 0.6708065 Vali Loss: 1.0235572 Test Loss: 0.7978893
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.6558211
	speed: 0.0135s/iter; left time: 15.6963s
	iters: 200, epoch: 5 | loss: 0.6580634
	speed: 0.0127s/iter; left time: 13.4224s
Epoch: 5 cost time: 2.7635915279388428
Epoch: 5, Steps: 210 | Train Loss: 0.6593198 Vali Loss: 1.0773327 Test Loss: 0.8062658
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.6381759
	speed: 0.0180s/iter; left time: 17.1033s
	iters: 200, epoch: 6 | loss: 0.6423532
	speed: 0.0168s/iter; left time: 14.3103s
Epoch: 6 cost time: 3.632373809814453
Epoch: 6, Steps: 210 | Train Loss: 0.6551359 Vali Loss: 1.0503947 Test Loss: 0.8080410
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.0_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.7697333097457886, mae:0.6979135274887085
Seed: 123
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.95_alpha0.0_96_336Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.95_alpha0.0.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           336                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.0_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.8862944
	speed: 0.0247s/iter; left time: 48.4284s
	iters: 200, epoch: 1 | loss: 0.8716734
	speed: 0.0181s/iter; left time: 33.6312s
Epoch: 1 cost time: 3.7422587871551514
Epoch: 1, Steps: 206 | Train Loss: 0.8918476 Vali Loss: 1.0777549 Test Loss: 0.8462471
Validation loss decreased (inf --> 1.077755).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.8090494
	speed: 0.0170s/iter; left time: 29.8770s
	iters: 200, epoch: 2 | loss: 0.8323735
	speed: 0.0133s/iter; left time: 21.9890s
Epoch: 2 cost time: 2.7789716720581055
Epoch: 2, Steps: 206 | Train Loss: 0.8139598 Vali Loss: 1.0254540 Test Loss: 0.8371058
Validation loss decreased (1.077755 --> 1.025454).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.7504045
	speed: 0.0186s/iter; left time: 28.8842s
	iters: 200, epoch: 3 | loss: 0.7331724
	speed: 0.0158s/iter; left time: 22.8662s
Epoch: 3 cost time: 3.323395013809204
Epoch: 3, Steps: 206 | Train Loss: 0.7529985 Vali Loss: 1.0846267 Test Loss: 0.8708897
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.7629104
	speed: 0.0183s/iter; left time: 24.6334s
	iters: 200, epoch: 4 | loss: 0.7295093
	speed: 0.0167s/iter; left time: 20.7180s
Epoch: 4 cost time: 3.557703971862793
Epoch: 4, Steps: 206 | Train Loss: 0.7173113 Vali Loss: 1.0595407 Test Loss: 0.8741269
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.6905569
	speed: 0.0181s/iter; left time: 20.6222s
	iters: 200, epoch: 5 | loss: 0.6905010
	speed: 0.0192s/iter; left time: 19.9162s
Epoch: 5 cost time: 4.008552074432373
Epoch: 5, Steps: 206 | Train Loss: 0.7031188 Vali Loss: 1.0896311 Test Loss: 0.8715587
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.6894061
	speed: 0.0141s/iter; left time: 13.1199s
	iters: 200, epoch: 6 | loss: 0.6889301
	speed: 0.0121s/iter; left time: 10.0658s
Epoch: 6 cost time: 2.535322904586792
Epoch: 6, Steps: 206 | Train Loss: 0.6951868 Vali Loss: 1.0808611 Test Loss: 0.8671540
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.7088387
	speed: 0.0144s/iter; left time: 10.4696s
	iters: 200, epoch: 7 | loss: 0.7081336
	speed: 0.0134s/iter; left time: 8.3438s
Epoch: 7 cost time: 2.8120734691619873
Epoch: 7, Steps: 206 | Train Loss: 0.6915207 Vali Loss: 1.0956793 Test Loss: 0.8645253
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.0_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.8371057510375977, mae:0.7276763916015625
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.0_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.8888009
	speed: 0.0237s/iter; left time: 46.3990s
	iters: 200, epoch: 1 | loss: 0.8238407
	speed: 0.0185s/iter; left time: 34.3993s
Epoch: 1 cost time: 3.86218523979187
Epoch: 1, Steps: 206 | Train Loss: 0.8875236 Vali Loss: 1.0311850 Test Loss: 0.8594316
Validation loss decreased (inf --> 1.031185).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.7796166
	speed: 0.0219s/iter; left time: 38.4269s
	iters: 200, epoch: 2 | loss: 0.7535206
	speed: 0.0169s/iter; left time: 27.9027s
Epoch: 2 cost time: 3.493964910507202
Epoch: 2, Steps: 206 | Train Loss: 0.8046382 Vali Loss: 1.0971657 Test Loss: 0.8695244
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.7167268
	speed: 0.0166s/iter; left time: 25.7292s
	iters: 200, epoch: 3 | loss: 0.7458273
	speed: 0.0138s/iter; left time: 19.9849s
Epoch: 3 cost time: 2.9710240364074707
Epoch: 3, Steps: 206 | Train Loss: 0.7281497 Vali Loss: 1.2250768 Test Loss: 0.8460208
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.6852559
	speed: 0.0144s/iter; left time: 19.3883s
	iters: 200, epoch: 4 | loss: 0.7077227
	speed: 0.0118s/iter; left time: 14.6407s
Epoch: 4 cost time: 2.5245742797851562
Epoch: 4, Steps: 206 | Train Loss: 0.6898429 Vali Loss: 1.2691042 Test Loss: 0.8732880
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.6941865
	speed: 0.0173s/iter; left time: 19.6951s
	iters: 200, epoch: 5 | loss: 0.6622344
	speed: 0.0174s/iter; left time: 18.0835s
Epoch: 5 cost time: 3.6687586307525635
Epoch: 5, Steps: 206 | Train Loss: 0.6760878 Vali Loss: 1.3264753 Test Loss: 0.8847767
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.6583315
	speed: 0.0159s/iter; left time: 14.7583s
	iters: 200, epoch: 6 | loss: 0.6502075
	speed: 0.0137s/iter; left time: 11.3597s
Epoch: 6 cost time: 2.964521646499634
Epoch: 6, Steps: 206 | Train Loss: 0.6694007 Vali Loss: 1.3244709 Test Loss: 0.8807730
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.0_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.859431803226471, mae:0.7359369993209839
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.0_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.8756407
	speed: 0.0203s/iter; left time: 39.8686s
	iters: 200, epoch: 1 | loss: 0.8782085
	speed: 0.0172s/iter; left time: 32.0739s
Epoch: 1 cost time: 3.6415655612945557
Epoch: 1, Steps: 206 | Train Loss: 0.8909367 Vali Loss: 1.0431253 Test Loss: 0.8347339
Validation loss decreased (inf --> 1.043125).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.8086915
	speed: 0.0157s/iter; left time: 27.6157s
	iters: 200, epoch: 2 | loss: 0.7723534
	speed: 0.0173s/iter; left time: 28.6342s
Epoch: 2 cost time: 3.642483949661255
Epoch: 2, Steps: 206 | Train Loss: 0.8035592 Vali Loss: 1.1171682 Test Loss: 0.8700052
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.7337270
	speed: 0.0229s/iter; left time: 35.4436s
	iters: 200, epoch: 3 | loss: 0.7118303
	speed: 0.0189s/iter; left time: 27.4341s
Epoch: 3 cost time: 3.9199471473693848
Epoch: 3, Steps: 206 | Train Loss: 0.7396506 Vali Loss: 1.1691542 Test Loss: 0.8449131
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.6967785
	speed: 0.0250s/iter; left time: 33.5755s
	iters: 200, epoch: 4 | loss: 0.6849024
	speed: 0.0194s/iter; left time: 24.1486s
Epoch: 4 cost time: 4.04231595993042
Epoch: 4, Steps: 206 | Train Loss: 0.7003291 Vali Loss: 1.0927793 Test Loss: 0.8485374
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.6715288
	speed: 0.0275s/iter; left time: 31.2547s
	iters: 200, epoch: 5 | loss: 0.6512322
	speed: 0.0202s/iter; left time: 20.9679s
Epoch: 5 cost time: 4.19551682472229
Epoch: 5, Steps: 206 | Train Loss: 0.6845878 Vali Loss: 1.1328468 Test Loss: 0.8571067
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.6715780
	speed: 0.0180s/iter; left time: 16.7841s
	iters: 200, epoch: 6 | loss: 0.6645342
	speed: 0.0136s/iter; left time: 11.3192s
Epoch: 6 cost time: 2.8814868927001953
Epoch: 6, Steps: 206 | Train Loss: 0.6773308 Vali Loss: 1.1277159 Test Loss: 0.8517944
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.0_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.834733784198761, mae:0.7261082530021667
Seed: 123
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.95_alpha0.0_96_720Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.95_alpha0.0.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           720                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.0_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0723423
	speed: 0.0324s/iter; left time: 59.5893s
Epoch: 1 cost time: 4.77124547958374
Epoch: 1, Steps: 194 | Train Loss: 1.0936677 Vali Loss: 1.8554664 Test Loss: 0.9559655
Validation loss decreased (inf --> 1.855466).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.9405249
	speed: 0.0187s/iter; left time: 30.7888s
Epoch: 2 cost time: 3.1807291507720947
Epoch: 2, Steps: 194 | Train Loss: 0.9386511 Vali Loss: 1.8611442 Test Loss: 1.0482453
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.8199051
	speed: 0.0166s/iter; left time: 24.1522s
Epoch: 3 cost time: 2.6282076835632324
Epoch: 3, Steps: 194 | Train Loss: 0.8313259 Vali Loss: 2.0545261 Test Loss: 1.0827333
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.7530419
	speed: 0.0174s/iter; left time: 21.9576s
Epoch: 4 cost time: 3.143195152282715
Epoch: 4, Steps: 194 | Train Loss: 0.7878807 Vali Loss: 2.0298998 Test Loss: 1.0583497
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.7418153
	speed: 0.0168s/iter; left time: 17.8990s
Epoch: 5 cost time: 3.283496379852295
Epoch: 5, Steps: 194 | Train Loss: 0.7689011 Vali Loss: 2.0487685 Test Loss: 1.1007938
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.7963236
	speed: 0.0160s/iter; left time: 13.9241s
Epoch: 6 cost time: 2.983273506164551
Epoch: 6, Steps: 194 | Train Loss: 0.7591060 Vali Loss: 2.1102948 Test Loss: 1.1111133
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.0_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.9559656381607056, mae:0.7801377177238464
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.0_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.1028279
	speed: 0.0138s/iter; left time: 25.3566s
Epoch: 1 cost time: 2.502345561981201
Epoch: 1, Steps: 194 | Train Loss: 1.0943238 Vali Loss: 1.9388161 Test Loss: 0.9406468
Validation loss decreased (inf --> 1.938816).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0243659
	speed: 0.0190s/iter; left time: 31.2206s
Epoch: 2 cost time: 3.408276081085205
Epoch: 2, Steps: 194 | Train Loss: 0.9656679 Vali Loss: 1.9406869 Test Loss: 1.0503663
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.8250542
	speed: 0.0155s/iter; left time: 22.4494s
Epoch: 3 cost time: 3.4387354850769043
Epoch: 3, Steps: 194 | Train Loss: 0.8511370 Vali Loss: 1.9236768 Test Loss: 1.1282693
Validation loss decreased (1.938816 --> 1.923677).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.8157282
	speed: 0.0195s/iter; left time: 24.5698s
Epoch: 4 cost time: 4.39195442199707
Epoch: 4, Steps: 194 | Train Loss: 0.7990214 Vali Loss: 1.9440869 Test Loss: 1.1701497
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.7752655
	speed: 0.0173s/iter; left time: 18.4474s
Epoch: 5 cost time: 3.093545436859131
Epoch: 5, Steps: 194 | Train Loss: 0.7790201 Vali Loss: 1.9561784 Test Loss: 1.1804585
EarlyStopping counter: 2 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.8203049
	speed: 0.0134s/iter; left time: 11.6772s
Epoch: 6 cost time: 2.667772054672241
Epoch: 6, Steps: 194 | Train Loss: 0.7711841 Vali Loss: 1.9460206 Test Loss: 1.1708485
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.7633116
	speed: 0.0166s/iter; left time: 11.2281s
Epoch: 7 cost time: 2.824511766433716
Epoch: 7, Steps: 194 | Train Loss: 0.7668313 Vali Loss: 1.9447645 Test Loss: 1.1698873
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.8131021
	speed: 0.0171s/iter; left time: 8.2707s
Epoch: 8 cost time: 3.2192041873931885
Epoch: 8, Steps: 194 | Train Loss: 0.7662176 Vali Loss: 1.9393069 Test Loss: 1.1754247
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.0_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.1282691955566406, mae:0.8410470485687256
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.0_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0080061
	speed: 0.0196s/iter; left time: 36.0128s
Epoch: 1 cost time: 3.1839802265167236
Epoch: 1, Steps: 194 | Train Loss: 1.0935945 Vali Loss: 1.8807681 Test Loss: 0.9770699
Validation loss decreased (inf --> 1.880768).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.9890030
	speed: 0.0149s/iter; left time: 24.5146s
Epoch: 2 cost time: 2.4789516925811768
Epoch: 2, Steps: 194 | Train Loss: 0.9444025 Vali Loss: 2.0992892 Test Loss: 1.1009028
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.8437327
	speed: 0.0219s/iter; left time: 31.7951s
Epoch: 3 cost time: 3.4659411907196045
Epoch: 3, Steps: 194 | Train Loss: 0.8302921 Vali Loss: 2.0846424 Test Loss: 1.1218102
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.8142295
	speed: 0.0177s/iter; left time: 22.3468s
Epoch: 4 cost time: 3.5287740230560303
Epoch: 4, Steps: 194 | Train Loss: 0.7797949 Vali Loss: 2.1285162 Test Loss: 1.1273979
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.7160773
	speed: 0.0205s/iter; left time: 21.8338s
Epoch: 5 cost time: 3.3364624977111816
Epoch: 5, Steps: 194 | Train Loss: 0.7616266 Vali Loss: 2.0906327 Test Loss: 1.1347823
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.7879500
	speed: 0.0188s/iter; left time: 16.3555s
Epoch: 6 cost time: 3.2417612075805664
Epoch: 6, Steps: 194 | Train Loss: 0.7539177 Vali Loss: 2.0459933 Test Loss: 1.1366209
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.0_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.9770697355270386, mae:0.7890446782112122
Seed: 456
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.95_alpha0.0_96_96Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.95_alpha0.0.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           96                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.0_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.6973141
	speed: 0.0390s/iter; left time: 79.1964s
	iters: 200, epoch: 1 | loss: 0.6718043
	speed: 0.0318s/iter; left time: 61.3220s
Epoch: 1 cost time: 6.662146329879761
Epoch: 1, Steps: 213 | Train Loss: 0.7071189 Vali Loss: 0.7595749 Test Loss: 0.6768141
Validation loss decreased (inf --> 0.759575).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.6712229
	speed: 0.0202s/iter; left time: 36.7599s
	iters: 200, epoch: 2 | loss: 0.6992566
	speed: 0.0153s/iter; left time: 26.3299s
Epoch: 2 cost time: 3.3195643424987793
Epoch: 2, Steps: 213 | Train Loss: 0.6655643 Vali Loss: 0.7108263 Test Loss: 0.6641244
Validation loss decreased (0.759575 --> 0.710826).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.6082949
	speed: 0.0157s/iter; left time: 25.1938s
	iters: 200, epoch: 3 | loss: 0.6117224
	speed: 0.0158s/iter; left time: 23.8186s
Epoch: 3 cost time: 3.5130615234375
Epoch: 3, Steps: 213 | Train Loss: 0.6400955 Vali Loss: 0.7314878 Test Loss: 0.6683005
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.6216347
	speed: 0.0183s/iter; left time: 25.4485s
	iters: 200, epoch: 4 | loss: 0.6332031
	speed: 0.0155s/iter; left time: 20.0361s
Epoch: 4 cost time: 3.3560569286346436
Epoch: 4, Steps: 213 | Train Loss: 0.6229177 Vali Loss: 0.7507716 Test Loss: 0.6676731
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.5962906
	speed: 0.0154s/iter; left time: 18.1113s
	iters: 200, epoch: 5 | loss: 0.6216466
	speed: 0.0133s/iter; left time: 14.3931s
Epoch: 5 cost time: 2.976400852203369
Epoch: 5, Steps: 213 | Train Loss: 0.6145532 Vali Loss: 0.7686077 Test Loss: 0.6730186
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.6257504
	speed: 0.0187s/iter; left time: 18.0361s
	iters: 200, epoch: 6 | loss: 0.5681028
	speed: 0.0151s/iter; left time: 13.0444s
Epoch: 6 cost time: 3.257497787475586
Epoch: 6, Steps: 213 | Train Loss: 0.6106125 Vali Loss: 0.7608551 Test Loss: 0.6671205
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.5754380
	speed: 0.0171s/iter; left time: 12.9135s
	iters: 200, epoch: 7 | loss: 0.6400289
	speed: 0.0192s/iter; left time: 12.5448s
Epoch: 7 cost time: 4.344786643981934
Epoch: 7, Steps: 213 | Train Loss: 0.6088438 Vali Loss: 0.7622161 Test Loss: 0.6679327
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.0_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.6641243100166321, mae:0.652357816696167
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.0_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.6763074
	speed: 0.0246s/iter; left time: 50.0176s
	iters: 200, epoch: 1 | loss: 0.6358247
	speed: 0.0187s/iter; left time: 36.0144s
Epoch: 1 cost time: 4.008378744125366
Epoch: 1, Steps: 213 | Train Loss: 0.7125326 Vali Loss: 0.7415600 Test Loss: 0.6704195
Validation loss decreased (inf --> 0.741560).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.6586484
	speed: 0.0160s/iter; left time: 29.0459s
	iters: 200, epoch: 2 | loss: 0.6501297
	speed: 0.0142s/iter; left time: 24.4050s
Epoch: 2 cost time: 3.0703015327453613
Epoch: 2, Steps: 213 | Train Loss: 0.6627487 Vali Loss: 0.7247352 Test Loss: 0.6807984
Validation loss decreased (0.741560 --> 0.724735).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.6258182
	speed: 0.0164s/iter; left time: 26.2842s
	iters: 200, epoch: 3 | loss: 0.6466330
	speed: 0.0137s/iter; left time: 20.6288s
Epoch: 3 cost time: 2.9707398414611816
Epoch: 3, Steps: 213 | Train Loss: 0.6304140 Vali Loss: 0.7020500 Test Loss: 0.6769639
Validation loss decreased (0.724735 --> 0.702050).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.6148862
	speed: 0.0196s/iter; left time: 27.2479s
	iters: 200, epoch: 4 | loss: 0.6111111
	speed: 0.0180s/iter; left time: 23.3094s
Epoch: 4 cost time: 3.923356294631958
Epoch: 4, Steps: 213 | Train Loss: 0.6115679 Vali Loss: 0.6944003 Test Loss: 0.6971363
Validation loss decreased (0.702050 --> 0.694400).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.5754768
	speed: 0.0176s/iter; left time: 20.7181s
	iters: 200, epoch: 5 | loss: 0.5828844
	speed: 0.0190s/iter; left time: 20.5316s
Epoch: 5 cost time: 4.227808237075806
Epoch: 5, Steps: 213 | Train Loss: 0.6019391 Vali Loss: 0.6939083 Test Loss: 0.6980127
Validation loss decreased (0.694400 --> 0.693908).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.5992141
	speed: 0.0195s/iter; left time: 18.8056s
	iters: 200, epoch: 6 | loss: 0.5875373
	speed: 0.0144s/iter; left time: 12.4538s
Epoch: 6 cost time: 3.08266019821167
Epoch: 6, Steps: 213 | Train Loss: 0.5976306 Vali Loss: 0.6940042 Test Loss: 0.6986426
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.5987225
	speed: 0.0133s/iter; left time: 10.0039s
	iters: 200, epoch: 7 | loss: 0.6054141
	speed: 0.0109s/iter; left time: 7.0942s
Epoch: 7 cost time: 2.3612990379333496
Epoch: 7, Steps: 213 | Train Loss: 0.5949368 Vali Loss: 0.6940525 Test Loss: 0.7028244
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.6131908
	speed: 0.0166s/iter; left time: 8.9565s
	iters: 200, epoch: 8 | loss: 0.5840006
	speed: 0.0145s/iter; left time: 6.3748s
Epoch: 8 cost time: 3.1093363761901855
Epoch: 8, Steps: 213 | Train Loss: 0.5938321 Vali Loss: 0.6950125 Test Loss: 0.7039347
EarlyStopping counter: 3 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.5994058
	speed: 0.0213s/iter; left time: 6.9517s
	iters: 200, epoch: 9 | loss: 0.5986979
	speed: 0.0189s/iter; left time: 4.2929s
Epoch: 9 cost time: 4.056185245513916
Epoch: 9, Steps: 213 | Train Loss: 0.5939846 Vali Loss: 0.6955027 Test Loss: 0.7035495
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 0.5854317
	speed: 0.0242s/iter; left time: 2.7558s
	iters: 200, epoch: 10 | loss: 0.5987235
	speed: 0.0179s/iter; left time: 0.2504s
Epoch: 10 cost time: 3.9602482318878174
Epoch: 10, Steps: 213 | Train Loss: 0.5936343 Vali Loss: 0.6939278 Test Loss: 0.7038471
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.0_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.6980127692222595, mae:0.6670576930046082
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.0_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.6931602
	speed: 0.0167s/iter; left time: 33.8434s
	iters: 200, epoch: 1 | loss: 0.6754183
	speed: 0.0158s/iter; left time: 30.5100s
Epoch: 1 cost time: 3.471975088119507
Epoch: 1, Steps: 213 | Train Loss: 0.7037942 Vali Loss: 0.7274572 Test Loss: 0.6714004
Validation loss decreased (inf --> 0.727457).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.7020914
	speed: 0.0262s/iter; left time: 47.5769s
	iters: 200, epoch: 2 | loss: 0.6219958
	speed: 0.0252s/iter; left time: 43.3254s
Epoch: 2 cost time: 5.250253915786743
Epoch: 2, Steps: 213 | Train Loss: 0.6632702 Vali Loss: 0.7194515 Test Loss: 0.6862450
Validation loss decreased (0.727457 --> 0.719452).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.6141905
	speed: 0.0316s/iter; left time: 50.7603s
	iters: 200, epoch: 3 | loss: 0.6388925
	speed: 0.0226s/iter; left time: 34.0190s
Epoch: 3 cost time: 4.816560983657837
Epoch: 3, Steps: 213 | Train Loss: 0.6327059 Vali Loss: 0.7343150 Test Loss: 0.7024470
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.6022307
	speed: 0.0159s/iter; left time: 22.1123s
	iters: 200, epoch: 4 | loss: 0.6143595
	speed: 0.0130s/iter; left time: 16.7825s
Epoch: 4 cost time: 2.86317777633667
Epoch: 4, Steps: 213 | Train Loss: 0.6141208 Vali Loss: 0.7479021 Test Loss: 0.6980931
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.6144329
	speed: 0.0184s/iter; left time: 21.7052s
	iters: 200, epoch: 5 | loss: 0.6147720
	speed: 0.0167s/iter; left time: 18.0274s
Epoch: 5 cost time: 3.636303663253784
Epoch: 5, Steps: 213 | Train Loss: 0.6068459 Vali Loss: 0.7549166 Test Loss: 0.7034132
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.6027219
	speed: 0.0164s/iter; left time: 15.8410s
	iters: 200, epoch: 6 | loss: 0.6132215
	speed: 0.0161s/iter; left time: 13.9242s
Epoch: 6 cost time: 3.54211688041687
Epoch: 6, Steps: 213 | Train Loss: 0.6020933 Vali Loss: 0.7654585 Test Loss: 0.7068454
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.5829658
	speed: 0.0186s/iter; left time: 13.9928s
	iters: 200, epoch: 7 | loss: 0.5870081
	speed: 0.0209s/iter; left time: 13.6484s
Epoch: 7 cost time: 4.538599014282227
Epoch: 7, Steps: 213 | Train Loss: 0.6002699 Vali Loss: 0.7596354 Test Loss: 0.7081517
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.0_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.6862450242042542, mae:0.6620813608169556
Seed: 456
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.95_alpha0.0_96_192Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.95_alpha0.0.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           192                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.0_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.7934586
	speed: 0.0312s/iter; left time: 62.4790s
	iters: 200, epoch: 1 | loss: 0.7972123
	speed: 0.0233s/iter; left time: 44.2388s
Epoch: 1 cost time: 4.908061504364014
Epoch: 1, Steps: 210 | Train Loss: 0.7886815 Vali Loss: 0.9208369 Test Loss: 0.7680991
Validation loss decreased (inf --> 0.920837).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.7019933
	speed: 0.0152s/iter; left time: 27.2634s
	iters: 200, epoch: 2 | loss: 0.7226456
	speed: 0.0136s/iter; left time: 23.0110s
Epoch: 2 cost time: 2.955408811569214
Epoch: 2, Steps: 210 | Train Loss: 0.7208466 Vali Loss: 0.8894714 Test Loss: 0.7931648
Validation loss decreased (0.920837 --> 0.889471).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.6537513
	speed: 0.0163s/iter; left time: 25.8314s
	iters: 200, epoch: 3 | loss: 0.6738522
	speed: 0.0154s/iter; left time: 22.7746s
Epoch: 3 cost time: 3.2699220180511475
Epoch: 3, Steps: 210 | Train Loss: 0.6830100 Vali Loss: 0.9223526 Test Loss: 0.7734255
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.6377990
	speed: 0.0279s/iter; left time: 38.2562s
	iters: 200, epoch: 4 | loss: 0.6629441
	speed: 0.0214s/iter; left time: 27.2282s
Epoch: 4 cost time: 4.516946077346802
Epoch: 4, Steps: 210 | Train Loss: 0.6616204 Vali Loss: 0.9328342 Test Loss: 0.7883905
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.7010465
	speed: 0.0201s/iter; left time: 23.3241s
	iters: 200, epoch: 5 | loss: 0.6368777
	speed: 0.0171s/iter; left time: 18.1235s
Epoch: 5 cost time: 3.6409125328063965
Epoch: 5, Steps: 210 | Train Loss: 0.6504415 Vali Loss: 0.9403125 Test Loss: 0.7919563
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.6588406
	speed: 0.0144s/iter; left time: 13.7220s
	iters: 200, epoch: 6 | loss: 0.6732364
	speed: 0.0121s/iter; left time: 10.3154s
Epoch: 6 cost time: 2.6731772422790527
Epoch: 6, Steps: 210 | Train Loss: 0.6452423 Vali Loss: 0.9626630 Test Loss: 0.7975875
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.6273592
	speed: 0.0153s/iter; left time: 11.3466s
	iters: 200, epoch: 7 | loss: 0.6382375
	speed: 0.0151s/iter; left time: 9.6781s
Epoch: 7 cost time: 3.303840398788452
Epoch: 7, Steps: 210 | Train Loss: 0.6427092 Vali Loss: 0.9418573 Test Loss: 0.7939882
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.0_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.7931647300720215, mae:0.7071937918663025
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.0_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.7466329
	speed: 0.0214s/iter; left time: 42.7713s
	iters: 200, epoch: 1 | loss: 0.7724603
	speed: 0.0185s/iter; left time: 35.2628s
Epoch: 1 cost time: 3.903630495071411
Epoch: 1, Steps: 210 | Train Loss: 0.7901098 Vali Loss: 0.9304148 Test Loss: 0.7631202
Validation loss decreased (inf --> 0.930415).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.7502526
	speed: 0.0157s/iter; left time: 28.0836s
	iters: 200, epoch: 2 | loss: 0.7426894
	speed: 0.0133s/iter; left time: 22.5699s
Epoch: 2 cost time: 2.8468098640441895
Epoch: 2, Steps: 210 | Train Loss: 0.7271421 Vali Loss: 0.9699259 Test Loss: 0.7716256
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.7059087
	speed: 0.0137s/iter; left time: 21.6492s
	iters: 200, epoch: 3 | loss: 0.6588331
	speed: 0.0139s/iter; left time: 20.5594s
Epoch: 3 cost time: 2.975548267364502
Epoch: 3, Steps: 210 | Train Loss: 0.6889992 Vali Loss: 0.9805009 Test Loss: 0.7882494
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.6273113
	speed: 0.0158s/iter; left time: 21.6869s
	iters: 200, epoch: 4 | loss: 0.6386287
	speed: 0.0135s/iter; left time: 17.1354s
Epoch: 4 cost time: 2.936573028564453
Epoch: 4, Steps: 210 | Train Loss: 0.6646984 Vali Loss: 0.9968098 Test Loss: 0.7885147
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.6707726
	speed: 0.0143s/iter; left time: 16.5733s
	iters: 200, epoch: 5 | loss: 0.6562744
	speed: 0.0129s/iter; left time: 13.6696s
Epoch: 5 cost time: 2.755549669265747
Epoch: 5, Steps: 210 | Train Loss: 0.6546350 Vali Loss: 1.0020458 Test Loss: 0.7976732
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.6510515
	speed: 0.0162s/iter; left time: 15.4088s
	iters: 200, epoch: 6 | loss: 0.6717443
	speed: 0.0134s/iter; left time: 11.4339s
Epoch: 6 cost time: 2.9575934410095215
Epoch: 6, Steps: 210 | Train Loss: 0.6481173 Vali Loss: 1.0019710 Test Loss: 0.7970391
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.0_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.7631199955940247, mae:0.6936651468276978
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.0_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.7886707
	speed: 0.0204s/iter; left time: 40.7791s
	iters: 200, epoch: 1 | loss: 0.7694745
	speed: 0.0173s/iter; left time: 32.9756s
Epoch: 1 cost time: 3.682713508605957
Epoch: 1, Steps: 210 | Train Loss: 0.7948081 Vali Loss: 0.8785185 Test Loss: 0.7694093
Validation loss decreased (inf --> 0.878518).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.6978712
	speed: 0.0189s/iter; left time: 33.8269s
	iters: 200, epoch: 2 | loss: 0.6992499
	speed: 0.0164s/iter; left time: 27.7665s
Epoch: 2 cost time: 3.5201480388641357
Epoch: 2, Steps: 210 | Train Loss: 0.7312181 Vali Loss: 0.9597428 Test Loss: 0.7849193
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.7057238
	speed: 0.0162s/iter; left time: 25.6870s
	iters: 200, epoch: 3 | loss: 0.6943185
	speed: 0.0159s/iter; left time: 23.6148s
Epoch: 3 cost time: 3.447772741317749
Epoch: 3, Steps: 210 | Train Loss: 0.6910835 Vali Loss: 0.9404756 Test Loss: 0.7823030
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.6481752
	speed: 0.0121s/iter; left time: 16.6473s
	iters: 200, epoch: 4 | loss: 0.6531957
	speed: 0.0150s/iter; left time: 19.0861s
Epoch: 4 cost time: 3.3532769680023193
Epoch: 4, Steps: 210 | Train Loss: 0.6671581 Vali Loss: 0.9536514 Test Loss: 0.7764676
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.6508261
	speed: 0.0184s/iter; left time: 21.3597s
	iters: 200, epoch: 5 | loss: 0.6533095
	speed: 0.0192s/iter; left time: 20.3966s
Epoch: 5 cost time: 4.1064229011535645
Epoch: 5, Steps: 210 | Train Loss: 0.6549316 Vali Loss: 0.9706926 Test Loss: 0.7841833
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.6560554
	speed: 0.0188s/iter; left time: 17.8627s
	iters: 200, epoch: 6 | loss: 0.6672954
	speed: 0.0209s/iter; left time: 17.7494s
Epoch: 6 cost time: 4.44731068611145
Epoch: 6, Steps: 210 | Train Loss: 0.6491749 Vali Loss: 0.9837922 Test Loss: 0.7895895
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.0_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.7694093585014343, mae:0.6960288882255554
Seed: 456
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.95_alpha0.0_96_336Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.95_alpha0.0.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           336                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.0_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.8646892
	speed: 0.0294s/iter; left time: 57.6899s
	iters: 200, epoch: 1 | loss: 0.8620241
	speed: 0.0221s/iter; left time: 41.1334s
Epoch: 1 cost time: 4.6258533000946045
Epoch: 1, Steps: 206 | Train Loss: 0.8931229 Vali Loss: 1.0354940 Test Loss: 0.8410760
Validation loss decreased (inf --> 1.035494).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.8394668
	speed: 0.0211s/iter; left time: 37.0775s
	iters: 200, epoch: 2 | loss: 0.7916632
	speed: 0.0181s/iter; left time: 29.9843s
Epoch: 2 cost time: 3.8069677352905273
Epoch: 2, Steps: 206 | Train Loss: 0.8068435 Vali Loss: 1.0532576 Test Loss: 0.8434666
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.7493746
	speed: 0.0189s/iter; left time: 29.2477s
	iters: 200, epoch: 3 | loss: 0.7650332
	speed: 0.0155s/iter; left time: 22.4686s
Epoch: 3 cost time: 3.2353875637054443
Epoch: 3, Steps: 206 | Train Loss: 0.7449579 Vali Loss: 1.0114522 Test Loss: 0.8465253
Validation loss decreased (1.035494 --> 1.011452).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.7064261
	speed: 0.0151s/iter; left time: 20.2738s
	iters: 200, epoch: 4 | loss: 0.6867842
	speed: 0.0147s/iter; left time: 18.2478s
Epoch: 4 cost time: 3.0939712524414062
Epoch: 4, Steps: 206 | Train Loss: 0.7127327 Vali Loss: 0.9977554 Test Loss: 0.8502288
Validation loss decreased (1.011452 --> 0.997755).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.6894770
	speed: 0.0209s/iter; left time: 23.7991s
	iters: 200, epoch: 5 | loss: 0.6887474
	speed: 0.0165s/iter; left time: 17.0601s
Epoch: 5 cost time: 3.4528825283050537
Epoch: 5, Steps: 206 | Train Loss: 0.6992106 Vali Loss: 1.0020784 Test Loss: 0.8350836
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.6823843
	speed: 0.0234s/iter; left time: 21.7516s
	iters: 200, epoch: 6 | loss: 0.6557184
	speed: 0.0183s/iter; left time: 15.1922s
Epoch: 6 cost time: 3.8205807209014893
Epoch: 6, Steps: 206 | Train Loss: 0.6919696 Vali Loss: 1.0046282 Test Loss: 0.8419791
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.6932104
	speed: 0.0183s/iter; left time: 13.2407s
	iters: 200, epoch: 7 | loss: 0.6739466
	speed: 0.0165s/iter; left time: 10.3291s
Epoch: 7 cost time: 3.4493398666381836
Epoch: 7, Steps: 206 | Train Loss: 0.6888345 Vali Loss: 1.0145067 Test Loss: 0.8404691
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.6849410
	speed: 0.0148s/iter; left time: 7.6709s
	iters: 200, epoch: 8 | loss: 0.6641285
	speed: 0.0122s/iter; left time: 5.1212s
Epoch: 8 cost time: 2.5684244632720947
Epoch: 8, Steps: 206 | Train Loss: 0.6881143 Vali Loss: 1.0125517 Test Loss: 0.8416609
EarlyStopping counter: 4 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.7139974
	speed: 0.0159s/iter; left time: 4.9623s
	iters: 200, epoch: 9 | loss: 0.6912212
	speed: 0.0154s/iter; left time: 3.2902s
Epoch: 9 cost time: 3.2814056873321533
Epoch: 9, Steps: 206 | Train Loss: 0.6878719 Vali Loss: 1.0140839 Test Loss: 0.8422216
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.0_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.8502289056777954, mae:0.7300773859024048
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.0_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.8996075
	speed: 0.0169s/iter; left time: 33.0735s
	iters: 200, epoch: 1 | loss: 0.8514234
	speed: 0.0159s/iter; left time: 29.6741s
Epoch: 1 cost time: 3.408705949783325
Epoch: 1, Steps: 206 | Train Loss: 0.9013437 Vali Loss: 1.0423753 Test Loss: 0.8339208
Validation loss decreased (inf --> 1.042375).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.7903534
	speed: 0.0176s/iter; left time: 30.8134s
	iters: 200, epoch: 2 | loss: 0.7000392
	speed: 0.0161s/iter; left time: 26.6392s
Epoch: 2 cost time: 3.403815269470215
Epoch: 2, Steps: 206 | Train Loss: 0.8062354 Vali Loss: 1.0512217 Test Loss: 0.8778946
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.7528346
	speed: 0.0180s/iter; left time: 27.8533s
	iters: 200, epoch: 3 | loss: 0.7262011
	speed: 0.0141s/iter; left time: 20.4000s
Epoch: 3 cost time: 2.998337984085083
Epoch: 3, Steps: 206 | Train Loss: 0.7454818 Vali Loss: 1.0858217 Test Loss: 0.8595833
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.7319764
	speed: 0.0140s/iter; left time: 18.8145s
	iters: 200, epoch: 4 | loss: 0.6827149
	speed: 0.0136s/iter; left time: 16.9665s
Epoch: 4 cost time: 2.8907604217529297
Epoch: 4, Steps: 206 | Train Loss: 0.7124852 Vali Loss: 1.0740011 Test Loss: 0.8757434
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.7026407
	speed: 0.0189s/iter; left time: 21.5422s
	iters: 200, epoch: 5 | loss: 0.6533118
	speed: 0.0207s/iter; left time: 21.4523s
Epoch: 5 cost time: 4.329983472824097
Epoch: 5, Steps: 206 | Train Loss: 0.6969598 Vali Loss: 1.0831389 Test Loss: 0.8736715
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.7086243
	speed: 0.0197s/iter; left time: 18.3576s
	iters: 200, epoch: 6 | loss: 0.6858363
	speed: 0.0205s/iter; left time: 17.0214s
Epoch: 6 cost time: 4.320621967315674
Epoch: 6, Steps: 206 | Train Loss: 0.6884662 Vali Loss: 1.0690835 Test Loss: 0.8663964
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.0_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.8339207768440247, mae:0.7253178954124451
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.0_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.8229757
	speed: 0.0185s/iter; left time: 36.1846s
	iters: 200, epoch: 1 | loss: 0.8442394
	speed: 0.0143s/iter; left time: 26.5616s
Epoch: 1 cost time: 3.0295236110687256
Epoch: 1, Steps: 206 | Train Loss: 0.8870628 Vali Loss: 1.1074597 Test Loss: 0.8656111
Validation loss decreased (inf --> 1.107460).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.8181644
	speed: 0.0207s/iter; left time: 36.2735s
	iters: 200, epoch: 2 | loss: 0.7466264
	speed: 0.0154s/iter; left time: 25.4629s
Epoch: 2 cost time: 3.2305352687835693
Epoch: 2, Steps: 206 | Train Loss: 0.7841347 Vali Loss: 1.1561729 Test Loss: 0.8490121
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.7022112
	speed: 0.0185s/iter; left time: 28.6998s
	iters: 200, epoch: 3 | loss: 0.6868690
	speed: 0.0153s/iter; left time: 22.0975s
Epoch: 3 cost time: 3.241657257080078
Epoch: 3, Steps: 206 | Train Loss: 0.7133848 Vali Loss: 1.2072077 Test Loss: 0.8827710
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.6825092
	speed: 0.0151s/iter; left time: 20.2414s
	iters: 200, epoch: 4 | loss: 0.6712953
	speed: 0.0145s/iter; left time: 18.0237s
Epoch: 4 cost time: 3.044909715652466
Epoch: 4, Steps: 206 | Train Loss: 0.6803055 Vali Loss: 1.2412165 Test Loss: 0.8775046
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.6677229
	speed: 0.0149s/iter; left time: 16.9500s
	iters: 200, epoch: 5 | loss: 0.7018325
	speed: 0.0139s/iter; left time: 14.4582s
Epoch: 5 cost time: 3.007373809814453
Epoch: 5, Steps: 206 | Train Loss: 0.6673133 Vali Loss: 1.2644304 Test Loss: 0.8744188
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.6703578
	speed: 0.0170s/iter; left time: 15.8016s
	iters: 200, epoch: 6 | loss: 0.6658601
	speed: 0.0149s/iter; left time: 12.3683s
Epoch: 6 cost time: 3.162196636199951
Epoch: 6, Steps: 206 | Train Loss: 0.6596954 Vali Loss: 1.2660346 Test Loss: 0.8750602
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.0_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.8656111359596252, mae:0.73775315284729
Seed: 456
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.95_alpha0.0_96_720Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.95_alpha0.0.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           720                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.0_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.1041023
	speed: 0.0399s/iter; left time: 73.4009s
Epoch: 1 cost time: 5.254690647125244
Epoch: 1, Steps: 194 | Train Loss: 1.0854225 Vali Loss: 1.9432044 Test Loss: 0.9654288
Validation loss decreased (inf --> 1.943204).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.9428872
	speed: 0.0167s/iter; left time: 27.4280s
Epoch: 2 cost time: 3.090071678161621
Epoch: 2, Steps: 194 | Train Loss: 0.9265094 Vali Loss: 2.0106688 Test Loss: 1.0338891
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.8122318
	speed: 0.0176s/iter; left time: 25.5330s
Epoch: 3 cost time: 3.5336170196533203
Epoch: 3, Steps: 194 | Train Loss: 0.8249581 Vali Loss: 2.1478381 Test Loss: 1.0944214
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.8910901
	speed: 0.0184s/iter; left time: 23.1516s
Epoch: 4 cost time: 3.281061887741089
Epoch: 4, Steps: 194 | Train Loss: 0.7847318 Vali Loss: 2.0971842 Test Loss: 1.0955240
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.7682747
	speed: 0.0170s/iter; left time: 18.1002s
Epoch: 5 cost time: 3.3297247886657715
Epoch: 5, Steps: 194 | Train Loss: 0.7661731 Vali Loss: 2.1317732 Test Loss: 1.0948505
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.8155974
	speed: 0.0170s/iter; left time: 14.8273s
Epoch: 6 cost time: 2.849043369293213
Epoch: 6, Steps: 194 | Train Loss: 0.7589917 Vali Loss: 2.0989962 Test Loss: 1.0945606
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.0_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.9654286503791809, mae:0.7831239104270935
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.0_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0888073
	speed: 0.0160s/iter; left time: 29.4471s
Epoch: 1 cost time: 3.20595383644104
Epoch: 1, Steps: 194 | Train Loss: 1.1000167 Vali Loss: 1.9505458 Test Loss: 0.9407863
Validation loss decreased (inf --> 1.950546).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.9326854
	speed: 0.0172s/iter; left time: 28.2867s
Epoch: 2 cost time: 3.2594802379608154
Epoch: 2, Steps: 194 | Train Loss: 0.9638138 Vali Loss: 1.9451536 Test Loss: 1.0103223
Validation loss decreased (1.950546 --> 1.945154).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.9621611
	speed: 0.0213s/iter; left time: 31.0110s
Epoch: 3 cost time: 3.905927896499634
Epoch: 3, Steps: 194 | Train Loss: 0.8699329 Vali Loss: 1.7328707 Test Loss: 1.0533148
Validation loss decreased (1.945154 --> 1.732871).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.8193929
	speed: 0.0155s/iter; left time: 19.5001s
Epoch: 4 cost time: 2.523153066635132
Epoch: 4, Steps: 194 | Train Loss: 0.8234544 Vali Loss: 1.6768752 Test Loss: 1.0762837
Validation loss decreased (1.732871 --> 1.676875).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.8348591
	speed: 0.0177s/iter; left time: 18.8552s
Epoch: 5 cost time: 2.839747667312622
Epoch: 5, Steps: 194 | Train Loss: 0.8035445 Vali Loss: 1.7606190 Test Loss: 1.1158835
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.8209103
	speed: 0.0204s/iter; left time: 17.7784s
Epoch: 6 cost time: 3.377393960952759
Epoch: 6, Steps: 194 | Train Loss: 0.7937660 Vali Loss: 1.7029803 Test Loss: 1.1178765
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.8634292
	speed: 0.0244s/iter; left time: 16.4888s
Epoch: 7 cost time: 3.9707982540130615
Epoch: 7, Steps: 194 | Train Loss: 0.7883111 Vali Loss: 1.7116827 Test Loss: 1.1163722
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.7572531
	speed: 0.0175s/iter; left time: 8.4580s
Epoch: 8 cost time: 3.139894485473633
Epoch: 8, Steps: 194 | Train Loss: 0.7866746 Vali Loss: 1.7015680 Test Loss: 1.1227822
EarlyStopping counter: 4 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.8246606
	speed: 0.0217s/iter; left time: 6.2593s
Epoch: 9 cost time: 3.597555637359619
Epoch: 9, Steps: 194 | Train Loss: 0.7853085 Vali Loss: 1.7025712 Test Loss: 1.1230173
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.0_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0762836933135986, mae:0.822513997554779
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.0_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.1682438
	speed: 0.0167s/iter; left time: 30.8273s
Epoch: 1 cost time: 2.725435256958008
Epoch: 1, Steps: 194 | Train Loss: 1.0866425 Vali Loss: 2.0491235 Test Loss: 0.9691016
Validation loss decreased (inf --> 2.049124).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.9146428
	speed: 0.0176s/iter; left time: 29.0341s
Epoch: 2 cost time: 3.200784921646118
Epoch: 2, Steps: 194 | Train Loss: 0.9181523 Vali Loss: 1.9439021 Test Loss: 1.0841329
Validation loss decreased (2.049124 --> 1.943902).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.8661784
	speed: 0.0168s/iter; left time: 24.3388s
Epoch: 3 cost time: 2.850137233734131
Epoch: 3, Steps: 194 | Train Loss: 0.8323677 Vali Loss: 2.0449400 Test Loss: 1.1333600
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.8546700
	speed: 0.0135s/iter; left time: 17.0167s
Epoch: 4 cost time: 2.4541876316070557
Epoch: 4, Steps: 194 | Train Loss: 0.7854038 Vali Loss: 1.8367960 Test Loss: 1.1376634
Validation loss decreased (1.943902 --> 1.836796).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.8083313
	speed: 0.0218s/iter; left time: 23.1663s
Epoch: 5 cost time: 3.467513084411621
Epoch: 5, Steps: 194 | Train Loss: 0.7679938 Vali Loss: 2.0282772 Test Loss: 1.1476707
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.7308904
	speed: 0.0146s/iter; left time: 12.6890s
Epoch: 6 cost time: 2.4888665676116943
Epoch: 6, Steps: 194 | Train Loss: 0.7571626 Vali Loss: 1.9965140 Test Loss: 1.1774225
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.7465200
	speed: 0.0176s/iter; left time: 11.9352s
Epoch: 7 cost time: 3.119307279586792
Epoch: 7, Steps: 194 | Train Loss: 0.7530293 Vali Loss: 1.9773405 Test Loss: 1.1896199
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.7531691
	speed: 0.0177s/iter; left time: 8.5340s
Epoch: 8 cost time: 3.3979837894439697
Epoch: 8, Steps: 194 | Train Loss: 0.7527800 Vali Loss: 1.9669272 Test Loss: 1.1826673
EarlyStopping counter: 4 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.7351722
	speed: 0.0247s/iter; left time: 7.1248s
Epoch: 9 cost time: 4.297683000564575
Epoch: 9, Steps: 194 | Train Loss: 0.7500415 Vali Loss: 1.9679061 Test Loss: 1.1822330
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.0_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.1376633644104004, mae:0.8469422459602356
Seed: 42
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.5_alpha0.8_96_96Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.5_alpha0.8.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           96                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.8_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0333154
	speed: 0.0353s/iter; left time: 71.7881s
	iters: 200, epoch: 1 | loss: 1.0281795
	speed: 0.0246s/iter; left time: 47.5933s
Epoch: 1 cost time: 5.093539714813232
Epoch: 1, Steps: 213 | Train Loss: 1.0335116 Vali Loss: 1.0500346 Test Loss: 1.0419217
Validation loss decreased (inf --> 1.050035).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0279263
	speed: 0.0230s/iter; left time: 41.8164s
	iters: 200, epoch: 2 | loss: 1.0578341
	speed: 0.0183s/iter; left time: 31.3960s
Epoch: 2 cost time: 3.977193593978882
Epoch: 2, Steps: 213 | Train Loss: 1.0168870 Vali Loss: 1.0480258 Test Loss: 1.0435917
Validation loss decreased (1.050035 --> 1.048026).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.9870088
	speed: 0.0144s/iter; left time: 23.1811s
	iters: 200, epoch: 3 | loss: 0.9682893
	speed: 0.0129s/iter; left time: 19.4152s
Epoch: 3 cost time: 2.855006456375122
Epoch: 3, Steps: 213 | Train Loss: 1.0110659 Vali Loss: 1.0470376 Test Loss: 1.0457046
Validation loss decreased (1.048026 --> 1.047038).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.9814652
	speed: 0.0157s/iter; left time: 21.8007s
	iters: 200, epoch: 4 | loss: 1.0182132
	speed: 0.0139s/iter; left time: 17.9087s
Epoch: 4 cost time: 3.1519830226898193
Epoch: 4, Steps: 213 | Train Loss: 1.0070010 Vali Loss: 1.0504248 Test Loss: 1.0476662
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0087122
	speed: 0.0162s/iter; left time: 19.0450s
	iters: 200, epoch: 5 | loss: 0.9886830
	speed: 0.0134s/iter; left time: 14.4775s
Epoch: 5 cost time: 2.953158378601074
Epoch: 5, Steps: 213 | Train Loss: 1.0042560 Vali Loss: 1.0502226 Test Loss: 1.0486779
EarlyStopping counter: 2 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0275171
	speed: 0.0172s/iter; left time: 16.5986s
	iters: 200, epoch: 6 | loss: 1.0170870
	speed: 0.0145s/iter; left time: 12.5626s
Epoch: 6 cost time: 3.139855146408081
Epoch: 6, Steps: 213 | Train Loss: 1.0029237 Vali Loss: 1.0474950 Test Loss: 1.0492132
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.9884664
	speed: 0.0118s/iter; left time: 8.9119s
	iters: 200, epoch: 7 | loss: 0.9975368
	speed: 0.0107s/iter; left time: 6.9842s
Epoch: 7 cost time: 2.446568727493286
Epoch: 7, Steps: 213 | Train Loss: 1.0019848 Vali Loss: 1.0503001 Test Loss: 1.0494840
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.9766397
	speed: 0.0188s/iter; left time: 10.1748s
	iters: 200, epoch: 8 | loss: 1.0192552
	speed: 0.0179s/iter; left time: 7.8866s
Epoch: 8 cost time: 3.9050137996673584
Epoch: 8, Steps: 213 | Train Loss: 1.0015449 Vali Loss: 1.0508442 Test Loss: 1.0496383
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.8_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0457044839859009, mae:0.8203138113021851
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.8_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0187588
	speed: 0.0154s/iter; left time: 31.2810s
	iters: 200, epoch: 1 | loss: 1.0330597
	speed: 0.0157s/iter; left time: 30.2308s
Epoch: 1 cost time: 3.455514907836914
Epoch: 1, Steps: 213 | Train Loss: 1.0318113 Vali Loss: 1.0528623 Test Loss: 1.0417405
Validation loss decreased (inf --> 1.052862).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0246959
	speed: 0.0165s/iter; left time: 30.0002s
	iters: 200, epoch: 2 | loss: 1.0077933
	speed: 0.0147s/iter; left time: 25.1829s
Epoch: 2 cost time: 3.210167407989502
Epoch: 2, Steps: 213 | Train Loss: 1.0193821 Vali Loss: 1.0488394 Test Loss: 1.0432546
Validation loss decreased (1.052862 --> 1.048839).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0064406
	speed: 0.0154s/iter; left time: 24.7960s
	iters: 200, epoch: 3 | loss: 1.0293044
	speed: 0.0159s/iter; left time: 23.9511s
Epoch: 3 cost time: 3.5106260776519775
Epoch: 3, Steps: 213 | Train Loss: 1.0142166 Vali Loss: 1.0456089 Test Loss: 1.0452454
Validation loss decreased (1.048839 --> 1.045609).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0230615
	speed: 0.0185s/iter; left time: 25.7771s
	iters: 200, epoch: 4 | loss: 1.0185734
	speed: 0.0171s/iter; left time: 22.0796s
Epoch: 4 cost time: 3.7226474285125732
Epoch: 4, Steps: 213 | Train Loss: 1.0110154 Vali Loss: 1.0454081 Test Loss: 1.0461912
Validation loss decreased (1.045609 --> 1.045408).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0106544
	speed: 0.0232s/iter; left time: 27.3753s
	iters: 200, epoch: 5 | loss: 1.0375586
	speed: 0.0188s/iter; left time: 20.3148s
Epoch: 5 cost time: 4.044759273529053
Epoch: 5, Steps: 213 | Train Loss: 1.0087160 Vali Loss: 1.0479121 Test Loss: 1.0475084
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0414841
	speed: 0.0246s/iter; left time: 23.7722s
	iters: 200, epoch: 6 | loss: 0.9539024
	speed: 0.0177s/iter; left time: 15.3330s
Epoch: 6 cost time: 3.7779042720794678
Epoch: 6, Steps: 213 | Train Loss: 1.0073726 Vali Loss: 1.0469716 Test Loss: 1.0483061
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.9843806
	speed: 0.0296s/iter; left time: 22.2725s
	iters: 200, epoch: 7 | loss: 1.0113823
	speed: 0.0215s/iter; left time: 14.0213s
Epoch: 7 cost time: 4.575965881347656
Epoch: 7, Steps: 213 | Train Loss: 1.0067954 Vali Loss: 1.0484684 Test Loss: 1.0485466
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.9903044
	speed: 0.0217s/iter; left time: 11.7086s
	iters: 200, epoch: 8 | loss: 0.9906205
	speed: 0.0176s/iter; left time: 7.7250s
Epoch: 8 cost time: 3.746612787246704
Epoch: 8, Steps: 213 | Train Loss: 1.0063818 Vali Loss: 1.0454218 Test Loss: 1.0487123
EarlyStopping counter: 4 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 1.0033550
	speed: 0.0173s/iter; left time: 5.6521s
	iters: 200, epoch: 9 | loss: 0.9973256
	speed: 0.0151s/iter; left time: 3.4303s
Epoch: 9 cost time: 3.273650646209717
Epoch: 9, Steps: 213 | Train Loss: 1.0059862 Vali Loss: 1.0473957 Test Loss: 1.0487884
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.8_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0461912155151367, mae:0.8204423785209656
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.8_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0450382
	speed: 0.0152s/iter; left time: 30.9567s
	iters: 200, epoch: 1 | loss: 1.0075593
	speed: 0.0137s/iter; left time: 26.3990s
Epoch: 1 cost time: 3.011538028717041
Epoch: 1, Steps: 213 | Train Loss: 1.0347796 Vali Loss: 1.0510519 Test Loss: 1.0426623
Validation loss decreased (inf --> 1.051052).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0161735
	speed: 0.0180s/iter; left time: 32.7028s
	iters: 200, epoch: 2 | loss: 1.0441904
	speed: 0.0166s/iter; left time: 28.4438s
Epoch: 2 cost time: 3.6035969257354736
Epoch: 2, Steps: 213 | Train Loss: 1.0175961 Vali Loss: 1.0506070 Test Loss: 1.0433401
Validation loss decreased (1.051052 --> 1.050607).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0033034
	speed: 0.0216s/iter; left time: 34.6439s
	iters: 200, epoch: 3 | loss: 1.0125692
	speed: 0.0179s/iter; left time: 26.9288s
Epoch: 3 cost time: 3.898456335067749
Epoch: 3, Steps: 213 | Train Loss: 1.0122050 Vali Loss: 1.0491083 Test Loss: 1.0454549
Validation loss decreased (1.050607 --> 1.049108).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0176079
	speed: 0.0244s/iter; left time: 34.0173s
	iters: 200, epoch: 4 | loss: 1.0086524
	speed: 0.0185s/iter; left time: 23.8551s
Epoch: 4 cost time: 4.011000871658325
Epoch: 4, Steps: 213 | Train Loss: 1.0087274 Vali Loss: 1.0470290 Test Loss: 1.0464751
Validation loss decreased (1.049108 --> 1.047029).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0102427
	speed: 0.0179s/iter; left time: 21.1616s
	iters: 200, epoch: 5 | loss: 0.9710389
	speed: 0.0156s/iter; left time: 16.7796s
Epoch: 5 cost time: 3.343170642852783
Epoch: 5, Steps: 213 | Train Loss: 1.0062683 Vali Loss: 1.0457258 Test Loss: 1.0471760
Validation loss decreased (1.047029 --> 1.045726).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.9843261
	speed: 0.0146s/iter; left time: 14.1435s
	iters: 200, epoch: 6 | loss: 1.0172739
	speed: 0.0141s/iter; left time: 12.2277s
Epoch: 6 cost time: 3.229017496109009
Epoch: 6, Steps: 213 | Train Loss: 1.0048416 Vali Loss: 1.0489640 Test Loss: 1.0478227
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0291817
	speed: 0.0139s/iter; left time: 10.4293s
	iters: 200, epoch: 7 | loss: 0.9751799
	speed: 0.0169s/iter; left time: 11.0203s
Epoch: 7 cost time: 3.682513952255249
Epoch: 7, Steps: 213 | Train Loss: 1.0041119 Vali Loss: 1.0494118 Test Loss: 1.0481242
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.9605478
	speed: 0.0189s/iter; left time: 10.2291s
	iters: 200, epoch: 8 | loss: 1.0468051
	speed: 0.0158s/iter; left time: 6.9359s
Epoch: 8 cost time: 3.4444239139556885
Epoch: 8, Steps: 213 | Train Loss: 1.0035879 Vali Loss: 1.0493547 Test Loss: 1.0483035
EarlyStopping counter: 3 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 1.0177537
	speed: 0.0129s/iter; left time: 4.2083s
	iters: 200, epoch: 9 | loss: 1.0308585
	speed: 0.0123s/iter; left time: 2.8004s
Epoch: 9 cost time: 2.7024638652801514
Epoch: 9, Steps: 213 | Train Loss: 1.0033299 Vali Loss: 1.0485688 Test Loss: 1.0483812
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 0.9916689
	speed: 0.0156s/iter; left time: 1.7799s
	iters: 200, epoch: 10 | loss: 1.0391657
	speed: 0.0154s/iter; left time: 0.2154s
Epoch: 10 cost time: 3.403496265411377
Epoch: 10, Steps: 213 | Train Loss: 1.0034531 Vali Loss: 1.0486708 Test Loss: 1.0484216
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.8_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0471758842468262, mae:0.8211548924446106
Seed: 42
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.5_alpha0.8_96_192Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.5_alpha0.8.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           192                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.8_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0275358
	speed: 0.0361s/iter; left time: 72.2241s
	iters: 200, epoch: 1 | loss: 1.0535517
	speed: 0.0241s/iter; left time: 45.8801s
Epoch: 1 cost time: 5.002096176147461
Epoch: 1, Steps: 210 | Train Loss: 1.0376704 Vali Loss: 1.0609874 Test Loss: 1.0489872
Validation loss decreased (inf --> 1.060987).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0250325
	speed: 0.0174s/iter; left time: 31.2345s
	iters: 200, epoch: 2 | loss: 1.0517011
	speed: 0.0163s/iter; left time: 27.5345s
Epoch: 2 cost time: 3.4807348251342773
Epoch: 2, Steps: 210 | Train Loss: 1.0248527 Vali Loss: 1.0575362 Test Loss: 1.0500407
Validation loss decreased (1.060987 --> 1.057536).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0269926
	speed: 0.0184s/iter; left time: 29.1528s
	iters: 200, epoch: 3 | loss: 1.0100667
	speed: 0.0172s/iter; left time: 25.4075s
Epoch: 3 cost time: 3.7223756313323975
Epoch: 3, Steps: 210 | Train Loss: 1.0205031 Vali Loss: 1.0542184 Test Loss: 1.0515249
Validation loss decreased (1.057536 --> 1.054218).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0047714
	speed: 0.0203s/iter; left time: 27.8646s
	iters: 200, epoch: 4 | loss: 0.9958360
	speed: 0.0238s/iter; left time: 30.2453s
Epoch: 4 cost time: 5.047101020812988
Epoch: 4, Steps: 210 | Train Loss: 1.0175483 Vali Loss: 1.0538415 Test Loss: 1.0537617
Validation loss decreased (1.054218 --> 1.053841).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.9976025
	speed: 0.0162s/iter; left time: 18.8104s
	iters: 200, epoch: 5 | loss: 1.0239182
	speed: 0.0137s/iter; left time: 14.5811s
Epoch: 5 cost time: 2.9976608753204346
Epoch: 5, Steps: 210 | Train Loss: 1.0156201 Vali Loss: 1.0551169 Test Loss: 1.0550216
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0326886
	speed: 0.0202s/iter; left time: 19.1877s
	iters: 200, epoch: 6 | loss: 0.9959788
	speed: 0.0181s/iter; left time: 15.3712s
Epoch: 6 cost time: 3.846855640411377
Epoch: 6, Steps: 210 | Train Loss: 1.0144156 Vali Loss: 1.0575063 Test Loss: 1.0557510
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0045748
	speed: 0.0202s/iter; left time: 14.9522s
	iters: 200, epoch: 7 | loss: 1.0193008
	speed: 0.0195s/iter; left time: 12.5021s
Epoch: 7 cost time: 4.127209901809692
Epoch: 7, Steps: 210 | Train Loss: 1.0137219 Vali Loss: 1.0549989 Test Loss: 1.0561035
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.9837434
	speed: 0.0158s/iter; left time: 8.3961s
	iters: 200, epoch: 8 | loss: 0.9973165
	speed: 0.0144s/iter; left time: 6.2042s
Epoch: 8 cost time: 3.044546604156494
Epoch: 8, Steps: 210 | Train Loss: 1.0133520 Vali Loss: 1.0558540 Test Loss: 1.0562402
EarlyStopping counter: 4 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 1.0113976
	speed: 0.0112s/iter; left time: 3.5934s
	iters: 200, epoch: 9 | loss: 1.0241040
	speed: 0.0113s/iter; left time: 2.4924s
Epoch: 9 cost time: 2.49516224861145
Epoch: 9, Steps: 210 | Train Loss: 1.0133693 Vali Loss: 1.0550030 Test Loss: 1.0563112
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.8_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0537617206573486, mae:0.823494017124176
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.8_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0281442
	speed: 0.0286s/iter; left time: 57.1316s
	iters: 200, epoch: 1 | loss: 1.0310701
	speed: 0.0218s/iter; left time: 41.4613s
Epoch: 1 cost time: 4.672415018081665
Epoch: 1, Steps: 210 | Train Loss: 1.0372631 Vali Loss: 1.0608952 Test Loss: 1.0488925
Validation loss decreased (inf --> 1.060895).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.9990863
	speed: 0.0193s/iter; left time: 34.5750s
	iters: 200, epoch: 2 | loss: 1.0385231
	speed: 0.0177s/iter; left time: 29.9654s
Epoch: 2 cost time: 3.8110477924346924
Epoch: 2, Steps: 210 | Train Loss: 1.0247816 Vali Loss: 1.0555438 Test Loss: 1.0502058
Validation loss decreased (1.060895 --> 1.055544).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0218228
	speed: 0.0193s/iter; left time: 30.5408s
	iters: 200, epoch: 3 | loss: 1.0399019
	speed: 0.0200s/iter; left time: 29.5662s
Epoch: 3 cost time: 4.17189621925354
Epoch: 3, Steps: 210 | Train Loss: 1.0204597 Vali Loss: 1.0547931 Test Loss: 1.0523466
Validation loss decreased (1.055544 --> 1.054793).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0131280
	speed: 0.0212s/iter; left time: 29.1282s
	iters: 200, epoch: 4 | loss: 1.0040669
	speed: 0.0184s/iter; left time: 23.4337s
Epoch: 4 cost time: 3.88236927986145
Epoch: 4, Steps: 210 | Train Loss: 1.0178930 Vali Loss: 1.0551138 Test Loss: 1.0540307
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0327778
	speed: 0.0269s/iter; left time: 31.2402s
	iters: 200, epoch: 5 | loss: 1.0237449
	speed: 0.0208s/iter; left time: 22.0892s
Epoch: 5 cost time: 4.437047004699707
Epoch: 5, Steps: 210 | Train Loss: 1.0161837 Vali Loss: 1.0529760 Test Loss: 1.0548978
Validation loss decreased (1.054793 --> 1.052976).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0061032
	speed: 0.0198s/iter; left time: 18.8750s
	iters: 200, epoch: 6 | loss: 1.0141114
	speed: 0.0173s/iter; left time: 14.7568s
Epoch: 6 cost time: 3.7350735664367676
Epoch: 6, Steps: 210 | Train Loss: 1.0151861 Vali Loss: 1.0521634 Test Loss: 1.0555050
Validation loss decreased (1.052976 --> 1.052163).  Saving model ...
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0355477
	speed: 0.0113s/iter; left time: 8.3794s
	iters: 200, epoch: 7 | loss: 1.0033201
	speed: 0.0156s/iter; left time: 10.0262s
Epoch: 7 cost time: 3.443359851837158
Epoch: 7, Steps: 210 | Train Loss: 1.0145202 Vali Loss: 1.0509429 Test Loss: 1.0559001
Validation loss decreased (1.052163 --> 1.050943).  Saving model ...
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.9891431
	speed: 0.0229s/iter; left time: 12.1714s
	iters: 200, epoch: 8 | loss: 1.0015638
	speed: 0.0189s/iter; left time: 8.1599s
Epoch: 8 cost time: 4.048128366470337
Epoch: 8, Steps: 210 | Train Loss: 1.0143898 Vali Loss: 1.0546732 Test Loss: 1.0560428
EarlyStopping counter: 1 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 1.0170239
	speed: 0.0181s/iter; left time: 5.7947s
	iters: 200, epoch: 9 | loss: 1.0204184
	speed: 0.0160s/iter; left time: 3.5405s
Epoch: 9 cost time: 3.3863024711608887
Epoch: 9, Steps: 210 | Train Loss: 1.0142515 Vali Loss: 1.0539101 Test Loss: 1.0561107
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 1.0407437
	speed: 0.0189s/iter; left time: 2.1022s
	iters: 200, epoch: 10 | loss: 1.0138316
	speed: 0.0169s/iter; left time: 0.1857s
Epoch: 10 cost time: 3.582136631011963
Epoch: 10, Steps: 210 | Train Loss: 1.0140466 Vali Loss: 1.0520566 Test Loss: 1.0561460
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.8_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0559000968933105, mae:0.8243324160575867
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.8_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0508976
	speed: 0.0166s/iter; left time: 33.2655s
	iters: 200, epoch: 1 | loss: 1.0182229
	speed: 0.0139s/iter; left time: 26.3477s
Epoch: 1 cost time: 2.987044095993042
Epoch: 1, Steps: 210 | Train Loss: 1.0380614 Vali Loss: 1.0608550 Test Loss: 1.0491083
Validation loss decreased (inf --> 1.060855).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0503774
	speed: 0.0157s/iter; left time: 28.0560s
	iters: 200, epoch: 2 | loss: 1.0450647
	speed: 0.0151s/iter; left time: 25.4954s
Epoch: 2 cost time: 3.2890818119049072
Epoch: 2, Steps: 210 | Train Loss: 1.0261151 Vali Loss: 1.0602139 Test Loss: 1.0484611
Validation loss decreased (1.060855 --> 1.060214).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0285747
	speed: 0.0166s/iter; left time: 26.2877s
	iters: 200, epoch: 3 | loss: 1.0044527
	speed: 0.0165s/iter; left time: 24.4014s
Epoch: 3 cost time: 3.5133755207061768
Epoch: 3, Steps: 210 | Train Loss: 1.0225767 Vali Loss: 1.0587270 Test Loss: 1.0494837
Validation loss decreased (1.060214 --> 1.058727).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0240139
	speed: 0.0229s/iter; left time: 31.3469s
	iters: 200, epoch: 4 | loss: 1.0520480
	speed: 0.0228s/iter; left time: 29.0220s
Epoch: 4 cost time: 4.918911695480347
Epoch: 4, Steps: 210 | Train Loss: 1.0201956 Vali Loss: 1.0549496 Test Loss: 1.0505868
Validation loss decreased (1.058727 --> 1.054950).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.9883938
	speed: 0.0147s/iter; left time: 17.0668s
	iters: 200, epoch: 5 | loss: 1.0166814
	speed: 0.0134s/iter; left time: 14.2485s
Epoch: 5 cost time: 3.003998041152954
Epoch: 5, Steps: 210 | Train Loss: 1.0186861 Vali Loss: 1.0583137 Test Loss: 1.0508338
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0188661
	speed: 0.0170s/iter; left time: 16.2133s
	iters: 200, epoch: 6 | loss: 1.0527999
	speed: 0.0157s/iter; left time: 13.3513s
Epoch: 6 cost time: 3.3642027378082275
Epoch: 6, Steps: 210 | Train Loss: 1.0176541 Vali Loss: 1.0556324 Test Loss: 1.0515538
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0022759
	speed: 0.0174s/iter; left time: 12.8631s
	iters: 200, epoch: 7 | loss: 1.0206927
	speed: 0.0170s/iter; left time: 10.9003s
Epoch: 7 cost time: 3.6445279121398926
Epoch: 7, Steps: 210 | Train Loss: 1.0170418 Vali Loss: 1.0584525 Test Loss: 1.0517657
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 1.0191606
	speed: 0.0174s/iter; left time: 9.2422s
	iters: 200, epoch: 8 | loss: 0.9920551
	speed: 0.0162s/iter; left time: 6.9783s
Epoch: 8 cost time: 3.5046799182891846
Epoch: 8, Steps: 210 | Train Loss: 1.0168273 Vali Loss: 1.0558554 Test Loss: 1.0518744
EarlyStopping counter: 4 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 1.0363135
	speed: 0.0143s/iter; left time: 4.5817s
	iters: 200, epoch: 9 | loss: 1.0138354
	speed: 0.0130s/iter; left time: 2.8813s
Epoch: 9 cost time: 2.826101064682007
Epoch: 9, Steps: 210 | Train Loss: 1.0165849 Vali Loss: 1.0570666 Test Loss: 1.0519454
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.8_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0505868196487427, mae:0.822222888469696
Seed: 42
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.5_alpha0.8_96_336Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.5_alpha0.8.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           336                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.8_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0396711
	speed: 0.0338s/iter; left time: 66.1991s
	iters: 200, epoch: 1 | loss: 1.0470883
	speed: 0.0254s/iter; left time: 47.2121s
Epoch: 1 cost time: 5.272260665893555
Epoch: 1, Steps: 206 | Train Loss: 1.0416405 Vali Loss: 1.0537819 Test Loss: 1.0389981
Validation loss decreased (inf --> 1.053782).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0344310
	speed: 0.0189s/iter; left time: 33.1920s
	iters: 200, epoch: 2 | loss: 1.0395230
	speed: 0.0142s/iter; left time: 23.5211s
Epoch: 2 cost time: 2.98736834526062
Epoch: 2, Steps: 206 | Train Loss: 1.0311529 Vali Loss: 1.0498686 Test Loss: 1.0387887
Validation loss decreased (1.053782 --> 1.049869).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0148679
	speed: 0.0192s/iter; left time: 29.7427s
	iters: 200, epoch: 3 | loss: 1.0523741
	speed: 0.0170s/iter; left time: 24.6243s
Epoch: 3 cost time: 3.558884382247925
Epoch: 3, Steps: 206 | Train Loss: 1.0277776 Vali Loss: 1.0483510 Test Loss: 1.0388362
Validation loss decreased (1.049869 --> 1.048351).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.9958131
	speed: 0.0204s/iter; left time: 27.3353s
	iters: 200, epoch: 4 | loss: 1.0021914
	speed: 0.0173s/iter; left time: 21.5650s
Epoch: 4 cost time: 3.613569974899292
Epoch: 4, Steps: 206 | Train Loss: 1.0258976 Vali Loss: 1.0483520 Test Loss: 1.0406358
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0157027
	speed: 0.0166s/iter; left time: 18.8763s
	iters: 200, epoch: 5 | loss: 1.0555946
	speed: 0.0169s/iter; left time: 17.5652s
Epoch: 5 cost time: 3.642662763595581
Epoch: 5, Steps: 206 | Train Loss: 1.0245430 Vali Loss: 1.0489742 Test Loss: 1.0418096
EarlyStopping counter: 2 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0275054
	speed: 0.0139s/iter; left time: 12.9114s
	iters: 200, epoch: 6 | loss: 1.0197872
	speed: 0.0117s/iter; left time: 9.7582s
Epoch: 6 cost time: 2.5253078937530518
Epoch: 6, Steps: 206 | Train Loss: 1.0237474 Vali Loss: 1.0491818 Test Loss: 1.0425184
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0005416
	speed: 0.0136s/iter; left time: 9.8525s
	iters: 200, epoch: 7 | loss: 1.0223371
	speed: 0.0118s/iter; left time: 7.3879s
Epoch: 7 cost time: 2.5189883708953857
Epoch: 7, Steps: 206 | Train Loss: 1.0233137 Vali Loss: 1.0487831 Test Loss: 1.0427269
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 1.0176394
	speed: 0.0137s/iter; left time: 7.1101s
	iters: 200, epoch: 8 | loss: 1.0275819
	speed: 0.0120s/iter; left time: 5.0444s
Epoch: 8 cost time: 2.528881788253784
Epoch: 8, Steps: 206 | Train Loss: 1.0228964 Vali Loss: 1.0489917 Test Loss: 1.0428327
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.8_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0388362407684326, mae:0.8185111880302429
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.8_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0471255
	speed: 0.0159s/iter; left time: 31.2423s
	iters: 200, epoch: 1 | loss: 1.0374711
	speed: 0.0131s/iter; left time: 24.3085s
Epoch: 1 cost time: 2.8114283084869385
Epoch: 1, Steps: 206 | Train Loss: 1.0376571 Vali Loss: 1.0533129 Test Loss: 1.0374905
Validation loss decreased (inf --> 1.053313).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0214744
	speed: 0.0165s/iter; left time: 28.9631s
	iters: 200, epoch: 2 | loss: 1.0291926
	speed: 0.0169s/iter; left time: 27.9989s
Epoch: 2 cost time: 3.5695230960845947
Epoch: 2, Steps: 206 | Train Loss: 1.0316207 Vali Loss: 1.0513563 Test Loss: 1.0378143
Validation loss decreased (1.053313 --> 1.051356).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0549775
	speed: 0.0180s/iter; left time: 27.8049s
	iters: 200, epoch: 3 | loss: 1.0301907
	speed: 0.0167s/iter; left time: 24.2031s
Epoch: 3 cost time: 3.5534205436706543
Epoch: 3, Steps: 206 | Train Loss: 1.0299298 Vali Loss: 1.0500292 Test Loss: 1.0391527
Validation loss decreased (1.051356 --> 1.050029).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0388029
	speed: 0.0186s/iter; left time: 24.9531s
	iters: 200, epoch: 4 | loss: 1.0333951
	speed: 0.0155s/iter; left time: 19.2442s
Epoch: 4 cost time: 3.2589621543884277
Epoch: 4, Steps: 206 | Train Loss: 1.0283595 Vali Loss: 1.0489156 Test Loss: 1.0392057
Validation loss decreased (1.050029 --> 1.048916).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0386949
	speed: 0.0176s/iter; left time: 20.0620s
	iters: 200, epoch: 5 | loss: 1.0394543
	speed: 0.0140s/iter; left time: 14.5558s
Epoch: 5 cost time: 2.9712002277374268
Epoch: 5, Steps: 206 | Train Loss: 1.0272469 Vali Loss: 1.0482788 Test Loss: 1.0396494
Validation loss decreased (1.048916 --> 1.048279).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0041535
	speed: 0.0137s/iter; left time: 12.7910s
	iters: 200, epoch: 6 | loss: 1.0352366
	speed: 0.0148s/iter; left time: 12.2964s
Epoch: 6 cost time: 3.207414150238037
Epoch: 6, Steps: 206 | Train Loss: 1.0264727 Vali Loss: 1.0476098 Test Loss: 1.0400249
Validation loss decreased (1.048279 --> 1.047610).  Saving model ...
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0435520
	speed: 0.0271s/iter; left time: 19.6493s
	iters: 200, epoch: 7 | loss: 1.0281032
	speed: 0.0189s/iter; left time: 11.8409s
Epoch: 7 cost time: 3.9415318965911865
Epoch: 7, Steps: 206 | Train Loss: 1.0259482 Vali Loss: 1.0480037 Test Loss: 1.0402628
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 1.0324394
	speed: 0.0291s/iter; left time: 15.1046s
	iters: 200, epoch: 8 | loss: 1.0512369
	speed: 0.0205s/iter; left time: 8.5761s
Epoch: 8 cost time: 4.231363534927368
Epoch: 8, Steps: 206 | Train Loss: 1.0259127 Vali Loss: 1.0479921 Test Loss: 1.0403755
EarlyStopping counter: 2 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 1.0306792
	speed: 0.0166s/iter; left time: 5.2025s
	iters: 200, epoch: 9 | loss: 0.9995373
	speed: 0.0128s/iter; left time: 2.7215s
Epoch: 9 cost time: 2.712087869644165
Epoch: 9, Steps: 206 | Train Loss: 1.0258707 Vali Loss: 1.0478187 Test Loss: 1.0403836
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 1.0511245
	speed: 0.0214s/iter; left time: 2.2852s
	iters: 200, epoch: 10 | loss: 1.0231138
	speed: 0.0168s/iter; left time: 0.1174s
Epoch: 10 cost time: 3.5387487411499023
Epoch: 10, Steps: 206 | Train Loss: 1.0256136 Vali Loss: 1.0480741 Test Loss: 1.0403943
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.8_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.040024995803833, mae:0.8190637230873108
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.8_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0277870
	speed: 0.0191s/iter; left time: 37.5170s
	iters: 200, epoch: 1 | loss: 1.0604633
	speed: 0.0168s/iter; left time: 31.2817s
Epoch: 1 cost time: 3.4792208671569824
Epoch: 1, Steps: 206 | Train Loss: 1.0461892 Vali Loss: 1.0537571 Test Loss: 1.0412172
Validation loss decreased (inf --> 1.053757).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0421278
	speed: 0.0226s/iter; left time: 39.7373s
	iters: 200, epoch: 2 | loss: 1.0020224
	speed: 0.0186s/iter; left time: 30.7497s
Epoch: 2 cost time: 3.9129865169525146
Epoch: 2, Steps: 206 | Train Loss: 1.0321689 Vali Loss: 1.0522292 Test Loss: 1.0424172
Validation loss decreased (1.053757 --> 1.052229).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0315230
	speed: 0.0144s/iter; left time: 22.2671s
	iters: 200, epoch: 3 | loss: 1.0199046
	speed: 0.0132s/iter; left time: 19.1348s
Epoch: 3 cost time: 2.832869291305542
Epoch: 3, Steps: 206 | Train Loss: 1.0286098 Vali Loss: 1.0485404 Test Loss: 1.0406826
Validation loss decreased (1.052229 --> 1.048540).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0236880
	speed: 0.0125s/iter; left time: 16.7482s
	iters: 200, epoch: 4 | loss: 1.0508323
	speed: 0.0140s/iter; left time: 17.4383s
Epoch: 4 cost time: 3.0575594902038574
Epoch: 4, Steps: 206 | Train Loss: 1.0267496 Vali Loss: 1.0484577 Test Loss: 1.0422705
Validation loss decreased (1.048540 --> 1.048458).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0221976
	speed: 0.0173s/iter; left time: 19.7046s
	iters: 200, epoch: 5 | loss: 1.0318191
	speed: 0.0165s/iter; left time: 17.0662s
Epoch: 5 cost time: 3.5124833583831787
Epoch: 5, Steps: 206 | Train Loss: 1.0258508 Vali Loss: 1.0485927 Test Loss: 1.0427412
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0239940
	speed: 0.0160s/iter; left time: 14.8608s
	iters: 200, epoch: 6 | loss: 1.0014057
	speed: 0.0170s/iter; left time: 14.1506s
Epoch: 6 cost time: 3.728590488433838
Epoch: 6, Steps: 206 | Train Loss: 1.0248351 Vali Loss: 1.0486448 Test Loss: 1.0428672
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0483382
	speed: 0.0185s/iter; left time: 13.4146s
	iters: 200, epoch: 7 | loss: 1.0187918
	speed: 0.0159s/iter; left time: 9.9409s
Epoch: 7 cost time: 3.404607057571411
Epoch: 7, Steps: 206 | Train Loss: 1.0244481 Vali Loss: 1.0482998 Test Loss: 1.0430791
Validation loss decreased (1.048458 --> 1.048300).  Saving model ...
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 1.0013434
	speed: 0.0149s/iter; left time: 7.7414s
	iters: 200, epoch: 8 | loss: 1.0448292
	speed: 0.0130s/iter; left time: 5.4538s
Epoch: 8 cost time: 2.7850186824798584
Epoch: 8, Steps: 206 | Train Loss: 1.0241239 Vali Loss: 1.0483226 Test Loss: 1.0431447
EarlyStopping counter: 1 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 1.0195122
	speed: 0.0167s/iter; left time: 5.2284s
	iters: 200, epoch: 9 | loss: 1.0202501
	speed: 0.0145s/iter; left time: 3.0780s
Epoch: 9 cost time: 3.0808634757995605
Epoch: 9, Steps: 206 | Train Loss: 1.0240528 Vali Loss: 1.0484384 Test Loss: 1.0431265
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 0.9906272
	speed: 0.0175s/iter; left time: 1.8732s
	iters: 200, epoch: 10 | loss: 1.0266536
	speed: 0.0161s/iter; left time: 0.1125s
Epoch: 10 cost time: 3.4831786155700684
Epoch: 10, Steps: 206 | Train Loss: 1.0241023 Vali Loss: 1.0481524 Test Loss: 1.0431455
Validation loss decreased (1.048300 --> 1.048152).  Saving model ...
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.8_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0431456565856934, mae:0.8202487230300903
Seed: 42
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.5_alpha0.8_96_720Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.5_alpha0.8.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           720                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.8_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0564595
	speed: 0.0266s/iter; left time: 49.0064s
Epoch: 1 cost time: 4.025824069976807
Epoch: 1, Steps: 194 | Train Loss: 1.0466598 Vali Loss: 1.0440140 Test Loss: 1.0407202
Validation loss decreased (inf --> 1.044014).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0334202
	speed: 0.0220s/iter; left time: 36.1522s
Epoch: 2 cost time: 3.6987407207489014
Epoch: 2, Steps: 194 | Train Loss: 1.0379351 Vali Loss: 1.0456090 Test Loss: 1.0385752
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0406052
	speed: 0.0176s/iter; left time: 25.6103s
Epoch: 3 cost time: 3.3897321224212646
Epoch: 3, Steps: 194 | Train Loss: 1.0361165 Vali Loss: 1.0438011 Test Loss: 1.0402478
Validation loss decreased (1.044014 --> 1.043801).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0440907
	speed: 0.0185s/iter; left time: 23.2542s
Epoch: 4 cost time: 3.3693814277648926
Epoch: 4, Steps: 194 | Train Loss: 1.0348268 Vali Loss: 1.0428863 Test Loss: 1.0413684
Validation loss decreased (1.043801 --> 1.042886).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0496194
	speed: 0.0164s/iter; left time: 17.4148s
Epoch: 5 cost time: 3.0859920978546143
Epoch: 5, Steps: 194 | Train Loss: 1.0341330 Vali Loss: 1.0440447 Test Loss: 1.0404459
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0221034
	speed: 0.0170s/iter; left time: 14.7741s
Epoch: 6 cost time: 3.1120693683624268
Epoch: 6, Steps: 194 | Train Loss: 1.0333989 Vali Loss: 1.0436736 Test Loss: 1.0411540
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0183086
	speed: 0.0201s/iter; left time: 13.6242s
Epoch: 7 cost time: 3.339947462081909
Epoch: 7, Steps: 194 | Train Loss: 1.0334614 Vali Loss: 1.0439429 Test Loss: 1.0410441
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 1.0391868
	speed: 0.0298s/iter; left time: 14.4146s
Epoch: 8 cost time: 4.503936290740967
Epoch: 8, Steps: 194 | Train Loss: 1.0330657 Vali Loss: 1.0438058 Test Loss: 1.0410333
EarlyStopping counter: 4 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 1.0384320
	speed: 0.0171s/iter; left time: 4.9488s
Epoch: 9 cost time: 2.877377510070801
Epoch: 9, Steps: 194 | Train Loss: 1.0330396 Vali Loss: 1.0441976 Test Loss: 1.0410042
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.8_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0413683652877808, mae:0.818901777267456
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.8_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0652349
	speed: 0.0158s/iter; left time: 29.1618s
Epoch: 1 cost time: 3.150029420852661
Epoch: 1, Steps: 194 | Train Loss: 1.0455343 Vali Loss: 1.0445423 Test Loss: 1.0389045
Validation loss decreased (inf --> 1.044542).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0283886
	speed: 0.0207s/iter; left time: 34.1378s
Epoch: 2 cost time: 3.559584617614746
Epoch: 2, Steps: 194 | Train Loss: 1.0379168 Vali Loss: 1.0453581 Test Loss: 1.0393682
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0193754
	speed: 0.0205s/iter; left time: 29.7239s
Epoch: 3 cost time: 3.6596715450286865
Epoch: 3, Steps: 194 | Train Loss: 1.0365611 Vali Loss: 1.0452113 Test Loss: 1.0390947
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0286053
	speed: 0.0139s/iter; left time: 17.5013s
Epoch: 4 cost time: 2.5118865966796875
Epoch: 4, Steps: 194 | Train Loss: 1.0355229 Vali Loss: 1.0449628 Test Loss: 1.0395069
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0469738
	speed: 0.0174s/iter; left time: 18.4883s
Epoch: 5 cost time: 3.5036473274230957
Epoch: 5, Steps: 194 | Train Loss: 1.0347296 Vali Loss: 1.0447881 Test Loss: 1.0395768
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0347879
	speed: 0.0191s/iter; left time: 16.6405s
Epoch: 6 cost time: 3.4428093433380127
Epoch: 6, Steps: 194 | Train Loss: 1.0341289 Vali Loss: 1.0447260 Test Loss: 1.0400813
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.8_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0389046669006348, mae:0.8178662061691284
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.8_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0386086
	speed: 0.0202s/iter; left time: 37.1729s
Epoch: 1 cost time: 3.562551975250244
Epoch: 1, Steps: 194 | Train Loss: 1.0485569 Vali Loss: 1.0449971 Test Loss: 1.0402412
Validation loss decreased (inf --> 1.044997).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0660338
	speed: 0.0215s/iter; left time: 35.3992s
Epoch: 2 cost time: 3.126560688018799
Epoch: 2, Steps: 194 | Train Loss: 1.0380809 Vali Loss: 1.0459086 Test Loss: 1.0377890
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0209885
	speed: 0.0167s/iter; left time: 24.3080s
Epoch: 3 cost time: 2.969674825668335
Epoch: 3, Steps: 194 | Train Loss: 1.0367087 Vali Loss: 1.0433688 Test Loss: 1.0399326
Validation loss decreased (1.044997 --> 1.043369).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0571674
	speed: 0.0178s/iter; left time: 22.3600s
Epoch: 4 cost time: 3.120399236679077
Epoch: 4, Steps: 194 | Train Loss: 1.0355119 Vali Loss: 1.0452328 Test Loss: 1.0395497
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0314960
	speed: 0.0174s/iter; left time: 18.4885s
Epoch: 5 cost time: 3.5571019649505615
Epoch: 5, Steps: 194 | Train Loss: 1.0346216 Vali Loss: 1.0450945 Test Loss: 1.0398415
EarlyStopping counter: 2 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0315238
	speed: 0.0192s/iter; left time: 16.7282s
Epoch: 6 cost time: 3.295497417449951
Epoch: 6, Steps: 194 | Train Loss: 1.0341977 Vali Loss: 1.0453861 Test Loss: 1.0397496
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0315865
	speed: 0.0175s/iter; left time: 11.8161s
Epoch: 7 cost time: 2.8262827396392822
Epoch: 7, Steps: 194 | Train Loss: 1.0337977 Vali Loss: 1.0451698 Test Loss: 1.0399166
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 1.0471833
	speed: 0.0152s/iter; left time: 7.3598s
Epoch: 8 cost time: 2.5450971126556396
Epoch: 8, Steps: 194 | Train Loss: 1.0339665 Vali Loss: 1.0453807 Test Loss: 1.0400071
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.8_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0399324893951416, mae:0.8183368444442749
Seed: 123
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.5_alpha0.8_96_96Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.5_alpha0.8.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           96                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.8_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0047243
	speed: 0.0283s/iter; left time: 57.4065s
	iters: 200, epoch: 1 | loss: 1.0106790
	speed: 0.0208s/iter; left time: 40.0793s
Epoch: 1 cost time: 4.34315037727356
Epoch: 1, Steps: 213 | Train Loss: 1.0319711 Vali Loss: 1.0508194 Test Loss: 1.0423833
Validation loss decreased (inf --> 1.050819).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0545826
	speed: 0.0161s/iter; left time: 29.1984s
	iters: 200, epoch: 2 | loss: 1.0257711
	speed: 0.0184s/iter; left time: 31.5769s
Epoch: 2 cost time: 4.1156415939331055
Epoch: 2, Steps: 213 | Train Loss: 1.0176348 Vali Loss: 1.0501474 Test Loss: 1.0436795
Validation loss decreased (1.050819 --> 1.050147).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0634511
	speed: 0.0237s/iter; left time: 38.1050s
	iters: 200, epoch: 3 | loss: 1.0153651
	speed: 0.0250s/iter; left time: 37.5804s
Epoch: 3 cost time: 5.2251057624816895
Epoch: 3, Steps: 213 | Train Loss: 1.0125455 Vali Loss: 1.0484942 Test Loss: 1.0445637
Validation loss decreased (1.050147 --> 1.048494).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0269896
	speed: 0.0226s/iter; left time: 31.4275s
	iters: 200, epoch: 4 | loss: 1.0051755
	speed: 0.0181s/iter; left time: 23.3536s
Epoch: 4 cost time: 3.9149370193481445
Epoch: 4, Steps: 213 | Train Loss: 1.0086795 Vali Loss: 1.0443009 Test Loss: 1.0467048
Validation loss decreased (1.048494 --> 1.044301).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0189756
	speed: 0.0152s/iter; left time: 17.8619s
	iters: 200, epoch: 5 | loss: 0.9929721
	speed: 0.0141s/iter; left time: 15.1615s
Epoch: 5 cost time: 3.0099642276763916
Epoch: 5, Steps: 213 | Train Loss: 1.0062506 Vali Loss: 1.0476433 Test Loss: 1.0480267
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.9632980
	speed: 0.0197s/iter; left time: 19.0434s
	iters: 200, epoch: 6 | loss: 0.9999561
	speed: 0.0191s/iter; left time: 16.5792s
Epoch: 6 cost time: 4.087354898452759
Epoch: 6, Steps: 213 | Train Loss: 1.0046809 Vali Loss: 1.0460087 Test Loss: 1.0487169
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.9842513
	speed: 0.0212s/iter; left time: 15.9471s
	iters: 200, epoch: 7 | loss: 1.0278658
	speed: 0.0175s/iter; left time: 11.4338s
Epoch: 7 cost time: 3.7727596759796143
Epoch: 7, Steps: 213 | Train Loss: 1.0039920 Vali Loss: 1.0468113 Test Loss: 1.0489944
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.9980547
	speed: 0.0167s/iter; left time: 9.0419s
	iters: 200, epoch: 8 | loss: 1.0000134
	speed: 0.0161s/iter; left time: 7.0946s
Epoch: 8 cost time: 3.4835166931152344
Epoch: 8, Steps: 213 | Train Loss: 1.0033768 Vali Loss: 1.0462285 Test Loss: 1.0491675
EarlyStopping counter: 4 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.9889624
	speed: 0.0125s/iter; left time: 4.0889s
	iters: 200, epoch: 9 | loss: 0.9897231
	speed: 0.0111s/iter; left time: 2.5259s
Epoch: 9 cost time: 2.4815995693206787
Epoch: 9, Steps: 213 | Train Loss: 1.0034040 Vali Loss: 1.0448031 Test Loss: 1.0492516
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.8_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.046704888343811, mae:0.8208234310150146
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.8_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0304006
	speed: 0.0172s/iter; left time: 34.8420s
	iters: 200, epoch: 1 | loss: 1.0373359
	speed: 0.0171s/iter; left time: 32.9291s
Epoch: 1 cost time: 3.771225690841675
Epoch: 1, Steps: 213 | Train Loss: 1.0317408 Vali Loss: 1.0507306 Test Loss: 1.0416454
Validation loss decreased (inf --> 1.050731).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0085735
	speed: 0.0225s/iter; left time: 40.8296s
	iters: 200, epoch: 2 | loss: 1.0246906
	speed: 0.0236s/iter; left time: 40.5612s
Epoch: 2 cost time: 4.962377071380615
Epoch: 2, Steps: 213 | Train Loss: 1.0186237 Vali Loss: 1.0501981 Test Loss: 1.0443919
Validation loss decreased (1.050731 --> 1.050198).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.9725144
	speed: 0.0182s/iter; left time: 29.2269s
	iters: 200, epoch: 3 | loss: 1.0357482
	speed: 0.0189s/iter; left time: 28.4233s
Epoch: 3 cost time: 4.187662839889526
Epoch: 3, Steps: 213 | Train Loss: 1.0131190 Vali Loss: 1.0483611 Test Loss: 1.0460737
Validation loss decreased (1.050198 --> 1.048361).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0303512
	speed: 0.0137s/iter; left time: 19.1170s
	iters: 200, epoch: 4 | loss: 0.9919981
	speed: 0.0132s/iter; left time: 17.0984s
Epoch: 4 cost time: 2.8584964275360107
Epoch: 4, Steps: 213 | Train Loss: 1.0092949 Vali Loss: 1.0467875 Test Loss: 1.0482415
Validation loss decreased (1.048361 --> 1.046788).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.9907677
	speed: 0.0200s/iter; left time: 23.6078s
	iters: 200, epoch: 5 | loss: 1.0355122
	speed: 0.0198s/iter; left time: 21.3123s
Epoch: 5 cost time: 4.201647043228149
Epoch: 5, Steps: 213 | Train Loss: 1.0066170 Vali Loss: 1.0472171 Test Loss: 1.0503240
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.9772131
	speed: 0.0207s/iter; left time: 19.9576s
	iters: 200, epoch: 6 | loss: 0.9723071
	speed: 0.0170s/iter; left time: 14.6870s
Epoch: 6 cost time: 3.6949520111083984
Epoch: 6, Steps: 213 | Train Loss: 1.0047893 Vali Loss: 1.0468233 Test Loss: 1.0511656
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0017881
	speed: 0.0217s/iter; left time: 16.3087s
	iters: 200, epoch: 7 | loss: 0.9864399
	speed: 0.0185s/iter; left time: 12.0908s
Epoch: 7 cost time: 4.015381574630737
Epoch: 7, Steps: 213 | Train Loss: 1.0039513 Vali Loss: 1.0500299 Test Loss: 1.0515304
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 1.0331166
	speed: 0.0130s/iter; left time: 7.0348s
	iters: 200, epoch: 8 | loss: 0.9691824
	speed: 0.0127s/iter; left time: 5.5893s
Epoch: 8 cost time: 2.7712292671203613
Epoch: 8, Steps: 213 | Train Loss: 1.0034608 Vali Loss: 1.0478812 Test Loss: 1.0517327
EarlyStopping counter: 4 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 1.0048271
	speed: 0.0204s/iter; left time: 6.6779s
	iters: 200, epoch: 9 | loss: 1.0150225
	speed: 0.0189s/iter; left time: 4.3001s
Epoch: 9 cost time: 4.063903570175171
Epoch: 9, Steps: 213 | Train Loss: 1.0035783 Vali Loss: 1.0484171 Test Loss: 1.0518174
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.8_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0482414960861206, mae:0.8213936686515808
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.8_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0692490
	speed: 0.0233s/iter; left time: 47.3064s
	iters: 200, epoch: 1 | loss: 1.0133748
	speed: 0.0192s/iter; left time: 37.0345s
Epoch: 1 cost time: 4.071345090866089
Epoch: 1, Steps: 213 | Train Loss: 1.0332564 Vali Loss: 1.0518022 Test Loss: 1.0421790
Validation loss decreased (inf --> 1.051802).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.9551530
	speed: 0.0274s/iter; left time: 49.7810s
	iters: 200, epoch: 2 | loss: 1.0290563
	speed: 0.0198s/iter; left time: 34.0359s
Epoch: 2 cost time: 4.232816457748413
Epoch: 2, Steps: 213 | Train Loss: 1.0175849 Vali Loss: 1.0478565 Test Loss: 1.0437316
Validation loss decreased (1.051802 --> 1.047856).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.9958568
	speed: 0.0171s/iter; left time: 27.4497s
	iters: 200, epoch: 3 | loss: 0.9774150
	speed: 0.0165s/iter; left time: 24.8823s
Epoch: 3 cost time: 3.6582088470458984
Epoch: 3, Steps: 213 | Train Loss: 1.0125794 Vali Loss: 1.0474694 Test Loss: 1.0447446
Validation loss decreased (1.047856 --> 1.047469).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0392365
	speed: 0.0235s/iter; left time: 32.6931s
	iters: 200, epoch: 4 | loss: 1.0077857
	speed: 0.0205s/iter; left time: 26.5065s
Epoch: 4 cost time: 4.3979973793029785
Epoch: 4, Steps: 213 | Train Loss: 1.0089576 Vali Loss: 1.0484567 Test Loss: 1.0463064
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.9844164
	speed: 0.0166s/iter; left time: 19.5781s
	iters: 200, epoch: 5 | loss: 0.9860244
	speed: 0.0144s/iter; left time: 15.5845s
Epoch: 5 cost time: 3.1493566036224365
Epoch: 5, Steps: 213 | Train Loss: 1.0063350 Vali Loss: 1.0472382 Test Loss: 1.0476586
Validation loss decreased (1.047469 --> 1.047238).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.9942508
	speed: 0.0156s/iter; left time: 15.0402s
	iters: 200, epoch: 6 | loss: 1.0212638
	speed: 0.0128s/iter; left time: 11.0428s
Epoch: 6 cost time: 2.8776941299438477
Epoch: 6, Steps: 213 | Train Loss: 1.0047190 Vali Loss: 1.0478054 Test Loss: 1.0482678
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0138459
	speed: 0.0147s/iter; left time: 11.0464s
	iters: 200, epoch: 7 | loss: 1.0548441
	speed: 0.0152s/iter; left time: 9.9147s
Epoch: 7 cost time: 3.3174331188201904
Epoch: 7, Steps: 213 | Train Loss: 1.0040412 Vali Loss: 1.0468074 Test Loss: 1.0485799
Validation loss decreased (1.047238 --> 1.046807).  Saving model ...
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.9619497
	speed: 0.0176s/iter; left time: 9.5176s
	iters: 200, epoch: 8 | loss: 1.0033534
	speed: 0.0165s/iter; left time: 7.2721s
Epoch: 8 cost time: 3.6411519050598145
Epoch: 8, Steps: 213 | Train Loss: 1.0035969 Vali Loss: 1.0489900 Test Loss: 1.0487335
EarlyStopping counter: 1 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.9909867
	speed: 0.0207s/iter; left time: 6.7561s
	iters: 200, epoch: 9 | loss: 1.0413743
	speed: 0.0174s/iter; left time: 3.9567s
Epoch: 9 cost time: 3.669198989868164
Epoch: 9, Steps: 213 | Train Loss: 1.0034570 Vali Loss: 1.0458575 Test Loss: 1.0488069
Validation loss decreased (1.046807 --> 1.045858).  Saving model ...
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 0.9918298
	speed: 0.0189s/iter; left time: 2.1577s
	iters: 200, epoch: 10 | loss: 0.9805663
	speed: 0.0150s/iter; left time: 0.2107s
Epoch: 10 cost time: 3.216543674468994
Epoch: 10, Steps: 213 | Train Loss: 1.0032277 Vali Loss: 1.0496916 Test Loss: 1.0488487
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.8_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.04880690574646, mae:0.8215410709381104
Seed: 123
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.5_alpha0.8_96_192Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.5_alpha0.8.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           192                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.8_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0298241
	speed: 0.0306s/iter; left time: 61.2772s
	iters: 200, epoch: 1 | loss: 1.0075101
	speed: 0.0231s/iter; left time: 43.8226s
Epoch: 1 cost time: 4.85468316078186
Epoch: 1, Steps: 210 | Train Loss: 1.0359310 Vali Loss: 1.0615488 Test Loss: 1.0487281
Validation loss decreased (inf --> 1.061549).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0192726
	speed: 0.0190s/iter; left time: 33.9463s
	iters: 200, epoch: 2 | loss: 1.0213971
	speed: 0.0153s/iter; left time: 25.8881s
Epoch: 2 cost time: 3.2748026847839355
Epoch: 2, Steps: 210 | Train Loss: 1.0255790 Vali Loss: 1.0577239 Test Loss: 1.0499135
Validation loss decreased (1.061549 --> 1.057724).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0388279
	speed: 0.0136s/iter; left time: 21.5780s
	iters: 200, epoch: 3 | loss: 1.0662622
	speed: 0.0172s/iter; left time: 25.5054s
Epoch: 3 cost time: 3.671612024307251
Epoch: 3, Steps: 210 | Train Loss: 1.0218563 Vali Loss: 1.0582814 Test Loss: 1.0509490
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0154188
	speed: 0.0219s/iter; left time: 30.0115s
	iters: 200, epoch: 4 | loss: 1.0676234
	speed: 0.0197s/iter; left time: 25.0854s
Epoch: 4 cost time: 4.1905200481414795
Epoch: 4, Steps: 210 | Train Loss: 1.0192545 Vali Loss: 1.0552030 Test Loss: 1.0529021
Validation loss decreased (1.057724 --> 1.055203).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0309976
	speed: 0.0187s/iter; left time: 21.6799s
	iters: 200, epoch: 5 | loss: 1.0542287
	speed: 0.0166s/iter; left time: 17.6286s
Epoch: 5 cost time: 3.586198091506958
Epoch: 5, Steps: 210 | Train Loss: 1.0174019 Vali Loss: 1.0549017 Test Loss: 1.0536219
Validation loss decreased (1.055203 --> 1.054902).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0522938
	speed: 0.0221s/iter; left time: 21.0355s
	iters: 200, epoch: 6 | loss: 1.0163822
	speed: 0.0166s/iter; left time: 14.1384s
Epoch: 6 cost time: 3.4914677143096924
Epoch: 6, Steps: 210 | Train Loss: 1.0164277 Vali Loss: 1.0520728 Test Loss: 1.0542461
Validation loss decreased (1.054902 --> 1.052073).  Saving model ...
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0172555
	speed: 0.0126s/iter; left time: 9.3116s
	iters: 200, epoch: 7 | loss: 1.0165272
	speed: 0.0112s/iter; left time: 7.1518s
Epoch: 7 cost time: 2.459362268447876
Epoch: 7, Steps: 210 | Train Loss: 1.0158688 Vali Loss: 1.0549635 Test Loss: 1.0545462
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 1.0077389
	speed: 0.0131s/iter; left time: 6.9664s
	iters: 200, epoch: 8 | loss: 0.9759282
	speed: 0.0138s/iter; left time: 5.9520s
Epoch: 8 cost time: 3.016279458999634
Epoch: 8, Steps: 210 | Train Loss: 1.0155116 Vali Loss: 1.0533147 Test Loss: 1.0547124
EarlyStopping counter: 2 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 1.0116467
	speed: 0.0207s/iter; left time: 6.6368s
	iters: 200, epoch: 9 | loss: 0.9852359
	speed: 0.0183s/iter; left time: 4.0523s
Epoch: 9 cost time: 3.9271886348724365
Epoch: 9, Steps: 210 | Train Loss: 1.0154193 Vali Loss: 1.0523219 Test Loss: 1.0547864
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 1.0205336
	speed: 0.0248s/iter; left time: 2.7544s
	iters: 200, epoch: 10 | loss: 1.0226994
	speed: 0.0208s/iter; left time: 0.2286s
Epoch: 10 cost time: 4.4985737800598145
Epoch: 10, Steps: 210 | Train Loss: 1.0152867 Vali Loss: 1.0528818 Test Loss: 1.0548196
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.8_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.054245948791504, mae:0.8237624168395996
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.8_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0328354
	speed: 0.0154s/iter; left time: 30.8985s
	iters: 200, epoch: 1 | loss: 1.0302099
	speed: 0.0140s/iter; left time: 26.5808s
Epoch: 1 cost time: 3.0600645542144775
Epoch: 1, Steps: 210 | Train Loss: 1.0388978 Vali Loss: 1.0617738 Test Loss: 1.0487924
Validation loss decreased (inf --> 1.061774).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.9917707
	speed: 0.0225s/iter; left time: 40.3671s
	iters: 200, epoch: 2 | loss: 1.0251307
	speed: 0.0196s/iter; left time: 33.1320s
Epoch: 2 cost time: 4.1602373123168945
Epoch: 2, Steps: 210 | Train Loss: 1.0259916 Vali Loss: 1.0585672 Test Loss: 1.0491673
Validation loss decreased (1.061774 --> 1.058567).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0098548
	speed: 0.0217s/iter; left time: 34.2732s
	iters: 200, epoch: 3 | loss: 1.0615056
	speed: 0.0194s/iter; left time: 28.7401s
Epoch: 3 cost time: 4.147698640823364
Epoch: 3, Steps: 210 | Train Loss: 1.0221998 Vali Loss: 1.0558678 Test Loss: 1.0497032
Validation loss decreased (1.058567 --> 1.055868).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0264100
	speed: 0.0246s/iter; left time: 33.6672s
	iters: 200, epoch: 4 | loss: 1.0085058
	speed: 0.0191s/iter; left time: 24.3180s
Epoch: 4 cost time: 4.02897572517395
Epoch: 4, Steps: 210 | Train Loss: 1.0194706 Vali Loss: 1.0541042 Test Loss: 1.0511068
Validation loss decreased (1.055868 --> 1.054104).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0060176
	speed: 0.0173s/iter; left time: 20.0929s
	iters: 200, epoch: 5 | loss: 1.0050893
	speed: 0.0133s/iter; left time: 14.1259s
Epoch: 5 cost time: 2.8408865928649902
Epoch: 5, Steps: 210 | Train Loss: 1.0177342 Vali Loss: 1.0551687 Test Loss: 1.0520649
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0239401
	speed: 0.0160s/iter; left time: 15.2436s
	iters: 200, epoch: 6 | loss: 0.9762372
	speed: 0.0160s/iter; left time: 13.5990s
Epoch: 6 cost time: 3.42087721824646
Epoch: 6, Steps: 210 | Train Loss: 1.0167054 Vali Loss: 1.0550820 Test Loss: 1.0523038
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0173172
	speed: 0.0150s/iter; left time: 11.0901s
	iters: 200, epoch: 7 | loss: 1.0045712
	speed: 0.0147s/iter; left time: 9.4054s
Epoch: 7 cost time: 3.2022526264190674
Epoch: 7, Steps: 210 | Train Loss: 1.0162326 Vali Loss: 1.0545146 Test Loss: 1.0526025
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 1.0022525
	speed: 0.0203s/iter; left time: 10.8043s
	iters: 200, epoch: 8 | loss: 1.0512254
	speed: 0.0184s/iter; left time: 7.9134s
Epoch: 8 cost time: 3.8872671127319336
Epoch: 8, Steps: 210 | Train Loss: 1.0158018 Vali Loss: 1.0573413 Test Loss: 1.0527376
EarlyStopping counter: 4 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 1.0371366
	speed: 0.0145s/iter; left time: 4.6440s
	iters: 200, epoch: 9 | loss: 0.9813902
	speed: 0.0140s/iter; left time: 3.0864s
Epoch: 9 cost time: 2.9980220794677734
Epoch: 9, Steps: 210 | Train Loss: 1.0155756 Vali Loss: 1.0545546 Test Loss: 1.0528076
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.8_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0511068105697632, mae:0.8225265741348267
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.8_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0200260
	speed: 0.0169s/iter; left time: 33.7404s
	iters: 200, epoch: 1 | loss: 1.0242076
	speed: 0.0153s/iter; left time: 29.1146s
Epoch: 1 cost time: 3.363863229751587
Epoch: 1, Steps: 210 | Train Loss: 1.0363362 Vali Loss: 1.0620742 Test Loss: 1.0484428
Validation loss decreased (inf --> 1.062074).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0282580
	speed: 0.0224s/iter; left time: 40.0796s
	iters: 200, epoch: 2 | loss: 1.0286421
	speed: 0.0188s/iter; left time: 31.8090s
Epoch: 2 cost time: 3.9995787143707275
Epoch: 2, Steps: 210 | Train Loss: 1.0250890 Vali Loss: 1.0568994 Test Loss: 1.0500274
Validation loss decreased (1.062074 --> 1.056899).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0308928
	speed: 0.0175s/iter; left time: 27.6939s
	iters: 200, epoch: 3 | loss: 1.0050621
	speed: 0.0142s/iter; left time: 20.9830s
Epoch: 3 cost time: 3.0751101970672607
Epoch: 3, Steps: 210 | Train Loss: 1.0208658 Vali Loss: 1.0555186 Test Loss: 1.0520195
Validation loss decreased (1.056899 --> 1.055519).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0094118
	speed: 0.0165s/iter; left time: 22.6077s
	iters: 200, epoch: 4 | loss: 1.0092956
	speed: 0.0166s/iter; left time: 21.1062s
Epoch: 4 cost time: 3.5194222927093506
Epoch: 4, Steps: 210 | Train Loss: 1.0181670 Vali Loss: 1.0524291 Test Loss: 1.0535831
Validation loss decreased (1.055519 --> 1.052429).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0328753
	speed: 0.0275s/iter; left time: 31.9137s
	iters: 200, epoch: 5 | loss: 1.0293581
	speed: 0.0211s/iter; left time: 22.3683s
Epoch: 5 cost time: 4.446268796920776
Epoch: 5, Steps: 210 | Train Loss: 1.0162722 Vali Loss: 1.0546377 Test Loss: 1.0548811
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.9915719
	speed: 0.0318s/iter; left time: 30.2088s
	iters: 200, epoch: 6 | loss: 1.0010797
	speed: 0.0213s/iter; left time: 18.1528s
Epoch: 6 cost time: 4.4909586906433105
Epoch: 6, Steps: 210 | Train Loss: 1.0152223 Vali Loss: 1.0534379 Test Loss: 1.0554856
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.9898286
	speed: 0.0150s/iter; left time: 11.0850s
	iters: 200, epoch: 7 | loss: 1.0497258
	speed: 0.0138s/iter; left time: 8.8370s
Epoch: 7 cost time: 2.975672483444214
Epoch: 7, Steps: 210 | Train Loss: 1.0146465 Vali Loss: 1.0536056 Test Loss: 1.0557520
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.9891407
	speed: 0.0189s/iter; left time: 10.0440s
	iters: 200, epoch: 8 | loss: 0.9839346
	speed: 0.0193s/iter; left time: 8.3221s
Epoch: 8 cost time: 4.204120635986328
Epoch: 8, Steps: 210 | Train Loss: 1.0143915 Vali Loss: 1.0528072 Test Loss: 1.0559177
EarlyStopping counter: 4 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.9760013
	speed: 0.0211s/iter; left time: 6.7739s
	iters: 200, epoch: 9 | loss: 1.0022736
	speed: 0.0179s/iter; left time: 3.9601s
Epoch: 9 cost time: 3.8369195461273193
Epoch: 9, Steps: 210 | Train Loss: 1.0143333 Vali Loss: 1.0526143 Test Loss: 1.0559852
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.8_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0535833835601807, mae:0.8234012126922607
Seed: 123
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.5_alpha0.8_96_336Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.5_alpha0.8.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           336                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.8_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0634844
	speed: 0.0270s/iter; left time: 52.9469s
	iters: 200, epoch: 1 | loss: 1.0607812
	speed: 0.0205s/iter; left time: 38.1068s
Epoch: 1 cost time: 4.278162479400635
Epoch: 1, Steps: 206 | Train Loss: 1.0403540 Vali Loss: 1.0533544 Test Loss: 1.0390533
Validation loss decreased (inf --> 1.053354).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0289295
	speed: 0.0194s/iter; left time: 33.9977s
	iters: 200, epoch: 2 | loss: 1.0512503
	speed: 0.0175s/iter; left time: 29.0078s
Epoch: 2 cost time: 3.6698241233825684
Epoch: 2, Steps: 206 | Train Loss: 1.0314734 Vali Loss: 1.0504990 Test Loss: 1.0384808
Validation loss decreased (1.053354 --> 1.050499).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.9869303
	speed: 0.0175s/iter; left time: 27.1014s
	iters: 200, epoch: 3 | loss: 1.0192891
	speed: 0.0154s/iter; left time: 22.2515s
Epoch: 3 cost time: 3.2021210193634033
Epoch: 3, Steps: 206 | Train Loss: 1.0292715 Vali Loss: 1.0489569 Test Loss: 1.0397986
Validation loss decreased (1.050499 --> 1.048957).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0193747
	speed: 0.0207s/iter; left time: 27.8517s
	iters: 200, epoch: 4 | loss: 1.0210419
	speed: 0.0160s/iter; left time: 19.8743s
Epoch: 4 cost time: 3.3906044960021973
Epoch: 4, Steps: 206 | Train Loss: 1.0271013 Vali Loss: 1.0470808 Test Loss: 1.0398489
Validation loss decreased (1.048957 --> 1.047081).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0394107
	speed: 0.0178s/iter; left time: 20.2493s
	iters: 200, epoch: 5 | loss: 1.0294411
	speed: 0.0168s/iter; left time: 17.4196s
Epoch: 5 cost time: 3.5900087356567383
Epoch: 5, Steps: 206 | Train Loss: 1.0259137 Vali Loss: 1.0474041 Test Loss: 1.0407293
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0249079
	speed: 0.0174s/iter; left time: 16.1737s
	iters: 200, epoch: 6 | loss: 1.0017753
	speed: 0.0158s/iter; left time: 13.1660s
Epoch: 6 cost time: 3.3681695461273193
Epoch: 6, Steps: 206 | Train Loss: 1.0249498 Vali Loss: 1.0473775 Test Loss: 1.0411801
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0639466
	speed: 0.0161s/iter; left time: 11.7032s
	iters: 200, epoch: 7 | loss: 1.0408378
	speed: 0.0158s/iter; left time: 9.8514s
Epoch: 7 cost time: 3.347309112548828
Epoch: 7, Steps: 206 | Train Loss: 1.0247637 Vali Loss: 1.0470972 Test Loss: 1.0415488
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 1.0571593
	speed: 0.0121s/iter; left time: 6.2790s
	iters: 200, epoch: 8 | loss: 1.0276765
	speed: 0.0116s/iter; left time: 4.8603s
Epoch: 8 cost time: 2.4774417877197266
Epoch: 8, Steps: 206 | Train Loss: 1.0244002 Vali Loss: 1.0473347 Test Loss: 1.0416092
EarlyStopping counter: 4 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 1.0047091
	speed: 0.0157s/iter; left time: 4.9047s
	iters: 200, epoch: 9 | loss: 1.0317383
	speed: 0.0147s/iter; left time: 3.1318s
Epoch: 9 cost time: 3.096259832382202
Epoch: 9, Steps: 206 | Train Loss: 1.0244668 Vali Loss: 1.0470988 Test Loss: 1.0416281
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.8_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.039848804473877, mae:0.8189947605133057
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.8_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0217413
	speed: 0.0163s/iter; left time: 31.9364s
	iters: 200, epoch: 1 | loss: 1.0410868
	speed: 0.0147s/iter; left time: 27.4115s
Epoch: 1 cost time: 3.0894546508789062
Epoch: 1, Steps: 206 | Train Loss: 1.0402093 Vali Loss: 1.0535157 Test Loss: 1.0388781
Validation loss decreased (inf --> 1.053516).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0585823
	speed: 0.0241s/iter; left time: 42.2353s
	iters: 200, epoch: 2 | loss: 1.0375379
	speed: 0.0199s/iter; left time: 32.9349s
Epoch: 2 cost time: 4.164438724517822
Epoch: 2, Steps: 206 | Train Loss: 1.0311897 Vali Loss: 1.0503327 Test Loss: 1.0387042
Validation loss decreased (1.053516 --> 1.050333).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0288527
	speed: 0.0134s/iter; left time: 20.7207s
	iters: 200, epoch: 3 | loss: 1.0287913
	speed: 0.0133s/iter; left time: 19.2625s
Epoch: 3 cost time: 2.796470880508423
Epoch: 3, Steps: 206 | Train Loss: 1.0289112 Vali Loss: 1.0484715 Test Loss: 1.0387712
Validation loss decreased (1.050333 --> 1.048471).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0541161
	speed: 0.0140s/iter; left time: 18.8653s
	iters: 200, epoch: 4 | loss: 1.0181971
	speed: 0.0133s/iter; left time: 16.5752s
Epoch: 4 cost time: 2.8117411136627197
Epoch: 4, Steps: 206 | Train Loss: 1.0272198 Vali Loss: 1.0483913 Test Loss: 1.0402597
Validation loss decreased (1.048471 --> 1.048391).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.9987746
	speed: 0.0192s/iter; left time: 21.8854s
	iters: 200, epoch: 5 | loss: 1.0072235
	speed: 0.0158s/iter; left time: 16.3585s
Epoch: 5 cost time: 3.2915303707122803
Epoch: 5, Steps: 206 | Train Loss: 1.0259915 Vali Loss: 1.0478635 Test Loss: 1.0399514
Validation loss decreased (1.048391 --> 1.047863).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0092371
	speed: 0.0213s/iter; left time: 19.8347s
	iters: 200, epoch: 6 | loss: 1.0067141
	speed: 0.0175s/iter; left time: 14.5694s
Epoch: 6 cost time: 3.7323625087738037
Epoch: 6, Steps: 206 | Train Loss: 1.0254774 Vali Loss: 1.0480340 Test Loss: 1.0406822
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0238780
	speed: 0.0159s/iter; left time: 11.5333s
	iters: 200, epoch: 7 | loss: 1.0362819
	speed: 0.0136s/iter; left time: 8.4692s
Epoch: 7 cost time: 2.864863872528076
Epoch: 7, Steps: 206 | Train Loss: 1.0250853 Vali Loss: 1.0482588 Test Loss: 1.0406510
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 1.0016216
	speed: 0.0159s/iter; left time: 8.2587s
	iters: 200, epoch: 8 | loss: 1.0124755
	speed: 0.0134s/iter; left time: 5.5957s
Epoch: 8 cost time: 2.842630624771118
Epoch: 8, Steps: 206 | Train Loss: 1.0249138 Vali Loss: 1.0478535 Test Loss: 1.0407743
Validation loss decreased (1.047863 --> 1.047853).  Saving model ...
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 1.0213816
	speed: 0.0164s/iter; left time: 5.1312s
	iters: 200, epoch: 9 | loss: 1.0253505
	speed: 0.0144s/iter; left time: 3.0773s
Epoch: 9 cost time: 3.016491651535034
Epoch: 9, Steps: 206 | Train Loss: 1.0248272 Vali Loss: 1.0479519 Test Loss: 1.0408063
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 1.0463793
	speed: 0.0142s/iter; left time: 1.5164s
	iters: 200, epoch: 10 | loss: 1.0508679
	speed: 0.0122s/iter; left time: 0.0853s
Epoch: 10 cost time: 2.6244101524353027
Epoch: 10, Steps: 206 | Train Loss: 1.0247689 Vali Loss: 1.0479094 Test Loss: 1.0408436
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.8_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0407743453979492, mae:0.8193740844726562
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.8_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0352519
	speed: 0.0172s/iter; left time: 33.7273s
	iters: 200, epoch: 1 | loss: 1.0575408
	speed: 0.0143s/iter; left time: 26.6449s
Epoch: 1 cost time: 3.0298011302948
Epoch: 1, Steps: 206 | Train Loss: 1.0395319 Vali Loss: 1.0539081 Test Loss: 1.0386443
Validation loss decreased (inf --> 1.053908).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0219493
	speed: 0.0179s/iter; left time: 31.4548s
	iters: 200, epoch: 2 | loss: 1.0196753
	speed: 0.0152s/iter; left time: 25.0895s
Epoch: 2 cost time: 3.2365152835845947
Epoch: 2, Steps: 206 | Train Loss: 1.0313020 Vali Loss: 1.0513058 Test Loss: 1.0393168
Validation loss decreased (1.053908 --> 1.051306).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0293839
	speed: 0.0228s/iter; left time: 35.2714s
	iters: 200, epoch: 3 | loss: 1.0408093
	speed: 0.0177s/iter; left time: 25.6997s
Epoch: 3 cost time: 3.732023239135742
Epoch: 3, Steps: 206 | Train Loss: 1.0288622 Vali Loss: 1.0489718 Test Loss: 1.0395856
Validation loss decreased (1.051306 --> 1.048972).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0307181
	speed: 0.0154s/iter; left time: 20.7040s
	iters: 200, epoch: 4 | loss: 1.0381113
	speed: 0.0131s/iter; left time: 16.2307s
Epoch: 4 cost time: 2.772355318069458
Epoch: 4, Steps: 206 | Train Loss: 1.0271519 Vali Loss: 1.0485877 Test Loss: 1.0406388
Validation loss decreased (1.048972 --> 1.048588).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0617058
	speed: 0.0164s/iter; left time: 18.6408s
	iters: 200, epoch: 5 | loss: 1.0098087
	speed: 0.0225s/iter; left time: 23.3423s
Epoch: 5 cost time: 4.696655988693237
Epoch: 5, Steps: 206 | Train Loss: 1.0258494 Vali Loss: 1.0486792 Test Loss: 1.0414844
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0294349
	speed: 0.0200s/iter; left time: 18.6343s
	iters: 200, epoch: 6 | loss: 1.0155821
	speed: 0.0180s/iter; left time: 14.9614s
Epoch: 6 cost time: 3.8641905784606934
Epoch: 6, Steps: 206 | Train Loss: 1.0251430 Vali Loss: 1.0483806 Test Loss: 1.0416529
Validation loss decreased (1.048588 --> 1.048381).  Saving model ...
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0388237
	speed: 0.0196s/iter; left time: 14.2123s
	iters: 200, epoch: 7 | loss: 1.0023371
	speed: 0.0181s/iter; left time: 11.3086s
Epoch: 7 cost time: 3.8163185119628906
Epoch: 7, Steps: 206 | Train Loss: 1.0249780 Vali Loss: 1.0487379 Test Loss: 1.0418971
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 1.0417693
	speed: 0.0158s/iter; left time: 8.1924s
	iters: 200, epoch: 8 | loss: 1.0470605
	speed: 0.0132s/iter; left time: 5.5194s
Epoch: 8 cost time: 2.8122217655181885
Epoch: 8, Steps: 206 | Train Loss: 1.0247920 Vali Loss: 1.0485423 Test Loss: 1.0420294
EarlyStopping counter: 2 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 1.0201278
	speed: 0.0118s/iter; left time: 3.7017s
	iters: 200, epoch: 9 | loss: 1.0158865
	speed: 0.0131s/iter; left time: 2.7861s
Epoch: 9 cost time: 2.820345878601074
Epoch: 9, Steps: 206 | Train Loss: 1.0246597 Vali Loss: 1.0481713 Test Loss: 1.0420864
Validation loss decreased (1.048381 --> 1.048171).  Saving model ...
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 1.0201395
	speed: 0.0195s/iter; left time: 2.0894s
	iters: 200, epoch: 10 | loss: 1.0078684
	speed: 0.0157s/iter; left time: 0.1097s
Epoch: 10 cost time: 3.378938913345337
Epoch: 10, Steps: 206 | Train Loss: 1.0246880 Vali Loss: 1.0484999 Test Loss: 1.0420984
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.8_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0420862436294556, mae:0.8198007941246033
Seed: 123
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.5_alpha0.8_96_720Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.5_alpha0.8.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           720                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.8_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0241317
	speed: 0.0319s/iter; left time: 58.7853s
Epoch: 1 cost time: 4.761203765869141
Epoch: 1, Steps: 194 | Train Loss: 1.0452920 Vali Loss: 1.0438406 Test Loss: 1.0410787
Validation loss decreased (inf --> 1.043841).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0141761
	speed: 0.0186s/iter; left time: 30.6604s
Epoch: 2 cost time: 3.1340646743774414
Epoch: 2, Steps: 194 | Train Loss: 1.0377349 Vali Loss: 1.0447848 Test Loss: 1.0380040
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0468432
	speed: 0.0230s/iter; left time: 33.4049s
Epoch: 3 cost time: 3.548283815383911
Epoch: 3, Steps: 194 | Train Loss: 1.0366109 Vali Loss: 1.0442343 Test Loss: 1.0390362
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0347012
	speed: 0.0163s/iter; left time: 20.5426s
Epoch: 4 cost time: 2.805020570755005
Epoch: 4, Steps: 194 | Train Loss: 1.0358128 Vali Loss: 1.0441053 Test Loss: 1.0395701
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0302871
	speed: 0.0134s/iter; left time: 14.3071s
Epoch: 5 cost time: 2.519608736038208
Epoch: 5, Steps: 194 | Train Loss: 1.0352860 Vali Loss: 1.0443155 Test Loss: 1.0400087
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0287937
	speed: 0.0156s/iter; left time: 13.5781s
Epoch: 6 cost time: 3.01871919631958
Epoch: 6, Steps: 194 | Train Loss: 1.0348061 Vali Loss: 1.0446701 Test Loss: 1.0397456
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.8_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.041078805923462, mae:0.8187870383262634
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.8_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0381478
	speed: 0.0172s/iter; left time: 31.7395s
Epoch: 1 cost time: 3.168996810913086
Epoch: 1, Steps: 194 | Train Loss: 1.0456366 Vali Loss: 1.0437105 Test Loss: 1.0400189
Validation loss decreased (inf --> 1.043710).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0581632
	speed: 0.0191s/iter; left time: 31.4925s
Epoch: 2 cost time: 3.1533546447753906
Epoch: 2, Steps: 194 | Train Loss: 1.0383908 Vali Loss: 1.0435405 Test Loss: 1.0396354
Validation loss decreased (1.043710 --> 1.043540).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0114589
	speed: 0.0183s/iter; left time: 26.5426s
Epoch: 3 cost time: 2.8289854526519775
Epoch: 3, Steps: 194 | Train Loss: 1.0369805 Vali Loss: 1.0458418 Test Loss: 1.0384778
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0467489
	speed: 0.0173s/iter; left time: 21.8315s
Epoch: 4 cost time: 3.11822509765625
Epoch: 4, Steps: 194 | Train Loss: 1.0362973 Vali Loss: 1.0451766 Test Loss: 1.0392067
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0502105
	speed: 0.0209s/iter; left time: 22.2590s
Epoch: 5 cost time: 3.4511327743530273
Epoch: 5, Steps: 194 | Train Loss: 1.0355026 Vali Loss: 1.0460068 Test Loss: 1.0391501
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0561196
	speed: 0.0189s/iter; left time: 16.4894s
Epoch: 6 cost time: 3.360177755355835
Epoch: 6, Steps: 194 | Train Loss: 1.0351855 Vali Loss: 1.0450380 Test Loss: 1.0399653
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0333556
	speed: 0.0255s/iter; left time: 17.2309s
Epoch: 7 cost time: 4.173259258270264
Epoch: 7, Steps: 194 | Train Loss: 1.0348558 Vali Loss: 1.0457307 Test Loss: 1.0399024
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.8_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0396355390548706, mae:0.8181939125061035
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.8_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0449679
	speed: 0.0163s/iter; left time: 29.9964s
Epoch: 1 cost time: 2.9944796562194824
Epoch: 1, Steps: 194 | Train Loss: 1.0474166 Vali Loss: 1.0447036 Test Loss: 1.0387840
Validation loss decreased (inf --> 1.044704).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0583986
	speed: 0.0199s/iter; left time: 32.7776s
Epoch: 2 cost time: 3.1669437885284424
Epoch: 2, Steps: 194 | Train Loss: 1.0383886 Vali Loss: 1.0452338 Test Loss: 1.0393357
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0559422
	speed: 0.0168s/iter; left time: 24.3883s
Epoch: 3 cost time: 3.000908851623535
Epoch: 3, Steps: 194 | Train Loss: 1.0366933 Vali Loss: 1.0450858 Test Loss: 1.0393165
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0549847
	speed: 0.0160s/iter; left time: 20.1989s
Epoch: 4 cost time: 3.356743812561035
Epoch: 4, Steps: 194 | Train Loss: 1.0356201 Vali Loss: 1.0436376 Test Loss: 1.0402776
Validation loss decreased (1.044704 --> 1.043638).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0346450
	speed: 0.0180s/iter; left time: 19.2134s
Epoch: 5 cost time: 3.238476037979126
Epoch: 5, Steps: 194 | Train Loss: 1.0346223 Vali Loss: 1.0435667 Test Loss: 1.0406759
Validation loss decreased (1.043638 --> 1.043567).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0498105
	speed: 0.0161s/iter; left time: 14.0462s
Epoch: 6 cost time: 2.7714004516601562
Epoch: 6, Steps: 194 | Train Loss: 1.0344025 Vali Loss: 1.0441060 Test Loss: 1.0403289
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0364267
	speed: 0.0200s/iter; left time: 13.5172s
Epoch: 7 cost time: 3.662248134613037
Epoch: 7, Steps: 194 | Train Loss: 1.0340018 Vali Loss: 1.0437264 Test Loss: 1.0403688
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 1.0235014
	speed: 0.0168s/iter; left time: 8.1348s
Epoch: 8 cost time: 2.994194746017456
Epoch: 8, Steps: 194 | Train Loss: 1.0339484 Vali Loss: 1.0441639 Test Loss: 1.0404882
EarlyStopping counter: 3 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 1.0303656
	speed: 0.0172s/iter; left time: 4.9715s
Epoch: 9 cost time: 3.284252643585205
Epoch: 9, Steps: 194 | Train Loss: 1.0338582 Vali Loss: 1.0439620 Test Loss: 1.0405030
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 1.0135218
	speed: 0.0269s/iter; left time: 2.5592s
Epoch: 10 cost time: 4.134280204772949
Epoch: 10, Steps: 194 | Train Loss: 1.0336674 Vali Loss: 1.0437410 Test Loss: 1.0405314
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.8_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0406758785247803, mae:0.8186240196228027
Seed: 456
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.5_alpha0.8_96_96Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.5_alpha0.8.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           96                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.8_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0589318
	speed: 0.0349s/iter; left time: 70.9507s
	iters: 200, epoch: 1 | loss: 1.0237646
	speed: 0.0251s/iter; left time: 48.5238s
Epoch: 1 cost time: 5.298000812530518
Epoch: 1, Steps: 213 | Train Loss: 1.0328325 Vali Loss: 1.0549598 Test Loss: 1.0426910
Validation loss decreased (inf --> 1.054960).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.9969349
	speed: 0.0190s/iter; left time: 34.5905s
	iters: 200, epoch: 2 | loss: 0.9675925
	speed: 0.0142s/iter; left time: 24.3938s
Epoch: 2 cost time: 3.135634183883667
Epoch: 2, Steps: 213 | Train Loss: 1.0185189 Vali Loss: 1.0523336 Test Loss: 1.0427170
Validation loss decreased (1.054960 --> 1.052334).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0021271
	speed: 0.0144s/iter; left time: 23.1587s
	iters: 200, epoch: 3 | loss: 1.0534184
	speed: 0.0152s/iter; left time: 22.8714s
Epoch: 3 cost time: 3.285578727722168
Epoch: 3, Steps: 213 | Train Loss: 1.0135378 Vali Loss: 1.0469776 Test Loss: 1.0447030
Validation loss decreased (1.052334 --> 1.046978).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0552824
	speed: 0.0251s/iter; left time: 35.0024s
	iters: 200, epoch: 4 | loss: 0.9858383
	speed: 0.0224s/iter; left time: 28.9979s
Epoch: 4 cost time: 4.7265002727508545
Epoch: 4, Steps: 213 | Train Loss: 1.0098336 Vali Loss: 1.0499293 Test Loss: 1.0464081
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.9859673
	speed: 0.0209s/iter; left time: 24.5927s
	iters: 200, epoch: 5 | loss: 0.9801229
	speed: 0.0169s/iter; left time: 18.2217s
Epoch: 5 cost time: 3.6652603149414062
Epoch: 5, Steps: 213 | Train Loss: 1.0073170 Vali Loss: 1.0484099 Test Loss: 1.0471978
EarlyStopping counter: 2 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0389757
	speed: 0.0205s/iter; left time: 19.8493s
	iters: 200, epoch: 6 | loss: 1.0179058
	speed: 0.0168s/iter; left time: 14.5597s
Epoch: 6 cost time: 3.5971603393554688
Epoch: 6, Steps: 213 | Train Loss: 1.0058483 Vali Loss: 1.0478402 Test Loss: 1.0478282
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0308633
	speed: 0.0150s/iter; left time: 11.3058s
	iters: 200, epoch: 7 | loss: 0.9917527
	speed: 0.0131s/iter; left time: 8.5748s
Epoch: 7 cost time: 2.9119794368743896
Epoch: 7, Steps: 213 | Train Loss: 1.0050372 Vali Loss: 1.0472417 Test Loss: 1.0481471
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.9973916
	speed: 0.0242s/iter; left time: 13.0678s
	iters: 200, epoch: 8 | loss: 1.0325320
	speed: 0.0217s/iter; left time: 9.5302s
Epoch: 8 cost time: 4.774504661560059
Epoch: 8, Steps: 213 | Train Loss: 1.0046998 Vali Loss: 1.0490911 Test Loss: 1.0483043
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.8_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0447028875350952, mae:0.8200914859771729
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.8_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0690349
	speed: 0.0183s/iter; left time: 37.1755s
	iters: 200, epoch: 1 | loss: 1.0573313
	speed: 0.0176s/iter; left time: 33.9519s
Epoch: 1 cost time: 3.8158955574035645
Epoch: 1, Steps: 213 | Train Loss: 1.0316367 Vali Loss: 1.0523345 Test Loss: 1.0420002
Validation loss decreased (inf --> 1.052335).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0352142
	speed: 0.0199s/iter; left time: 36.2252s
	iters: 200, epoch: 2 | loss: 0.9674284
	speed: 0.0169s/iter; left time: 29.1178s
Epoch: 2 cost time: 3.680101156234741
Epoch: 2, Steps: 213 | Train Loss: 1.0180499 Vali Loss: 1.0482105 Test Loss: 1.0425968
Validation loss decreased (1.052335 --> 1.048211).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.9710542
	speed: 0.0139s/iter; left time: 22.2939s
	iters: 200, epoch: 3 | loss: 1.0216339
	speed: 0.0118s/iter; left time: 17.8092s
Epoch: 3 cost time: 2.5727107524871826
Epoch: 3, Steps: 213 | Train Loss: 1.0129908 Vali Loss: 1.0460773 Test Loss: 1.0440820
Validation loss decreased (1.048211 --> 1.046077).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0123397
	speed: 0.0167s/iter; left time: 23.2035s
	iters: 200, epoch: 4 | loss: 1.0276603
	speed: 0.0152s/iter; left time: 19.6665s
Epoch: 4 cost time: 3.23650860786438
Epoch: 4, Steps: 213 | Train Loss: 1.0095647 Vali Loss: 1.0472503 Test Loss: 1.0452631
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.9748127
	speed: 0.0200s/iter; left time: 23.5634s
	iters: 200, epoch: 5 | loss: 1.0168363
	speed: 0.0188s/iter; left time: 20.2922s
Epoch: 5 cost time: 4.070480823516846
Epoch: 5, Steps: 213 | Train Loss: 1.0071620 Vali Loss: 1.0468282 Test Loss: 1.0462481
EarlyStopping counter: 2 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.9957420
	speed: 0.0180s/iter; left time: 17.3458s
	iters: 200, epoch: 6 | loss: 0.9942584
	speed: 0.0162s/iter; left time: 14.0019s
Epoch: 6 cost time: 3.459461212158203
Epoch: 6, Steps: 213 | Train Loss: 1.0058662 Vali Loss: 1.0489658 Test Loss: 1.0467428
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.9758023
	speed: 0.0127s/iter; left time: 9.5578s
	iters: 200, epoch: 7 | loss: 0.9712540
	speed: 0.0116s/iter; left time: 7.5890s
Epoch: 7 cost time: 2.6049630641937256
Epoch: 7, Steps: 213 | Train Loss: 1.0049976 Vali Loss: 1.0497025 Test Loss: 1.0469854
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.9683837
	speed: 0.0206s/iter; left time: 11.1095s
	iters: 200, epoch: 8 | loss: 0.9977016
	speed: 0.0177s/iter; left time: 7.7998s
Epoch: 8 cost time: 3.875174045562744
Epoch: 8, Steps: 213 | Train Loss: 1.0048861 Vali Loss: 1.0483439 Test Loss: 1.0471084
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.8_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0440820455551147, mae:0.8198859691619873
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.8_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0355710
	speed: 0.0189s/iter; left time: 38.3942s
	iters: 200, epoch: 1 | loss: 1.0134799
	speed: 0.0191s/iter; left time: 36.8786s
Epoch: 1 cost time: 4.161181926727295
Epoch: 1, Steps: 213 | Train Loss: 1.0315376 Vali Loss: 1.0503827 Test Loss: 1.0419081
Validation loss decreased (inf --> 1.050383).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0250509
	speed: 0.0211s/iter; left time: 38.4365s
	iters: 200, epoch: 2 | loss: 0.9728824
	speed: 0.0175s/iter; left time: 30.1361s
Epoch: 2 cost time: 3.6866402626037598
Epoch: 2, Steps: 213 | Train Loss: 1.0176887 Vali Loss: 1.0470551 Test Loss: 1.0432221
Validation loss decreased (1.050383 --> 1.047055).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0027076
	speed: 0.0169s/iter; left time: 27.1766s
	iters: 200, epoch: 3 | loss: 1.0227637
	speed: 0.0154s/iter; left time: 23.2249s
Epoch: 3 cost time: 3.2579047679901123
Epoch: 3, Steps: 213 | Train Loss: 1.0126518 Vali Loss: 1.0486541 Test Loss: 1.0454524
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.9848002
	speed: 0.0239s/iter; left time: 33.3207s
	iters: 200, epoch: 4 | loss: 0.9707403
	speed: 0.0191s/iter; left time: 24.7386s
Epoch: 4 cost time: 4.120705842971802
Epoch: 4, Steps: 213 | Train Loss: 1.0092424 Vali Loss: 1.0481156 Test Loss: 1.0470608
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0135667
	speed: 0.0244s/iter; left time: 28.7654s
	iters: 200, epoch: 5 | loss: 1.0388024
	speed: 0.0201s/iter; left time: 21.6733s
Epoch: 5 cost time: 4.253790378570557
Epoch: 5, Steps: 213 | Train Loss: 1.0069300 Vali Loss: 1.0476835 Test Loss: 1.0480559
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0163126
	speed: 0.0200s/iter; left time: 19.2973s
	iters: 200, epoch: 6 | loss: 1.0056800
	speed: 0.0168s/iter; left time: 14.5412s
Epoch: 6 cost time: 3.531627655029297
Epoch: 6, Steps: 213 | Train Loss: 1.0053894 Vali Loss: 1.0471524 Test Loss: 1.0485901
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.9882872
	speed: 0.0150s/iter; left time: 11.3303s
	iters: 200, epoch: 7 | loss: 1.0344237
	speed: 0.0122s/iter; left time: 7.9665s
Epoch: 7 cost time: 2.64227557182312
Epoch: 7, Steps: 213 | Train Loss: 1.0046145 Vali Loss: 1.0467627 Test Loss: 1.0489173
Validation loss decreased (1.047055 --> 1.046763).  Saving model ...
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.9903688
	speed: 0.0160s/iter; left time: 8.6587s
	iters: 200, epoch: 8 | loss: 1.0092211
	speed: 0.0155s/iter; left time: 6.8340s
Epoch: 8 cost time: 3.3514316082000732
Epoch: 8, Steps: 213 | Train Loss: 1.0043908 Vali Loss: 1.0466692 Test Loss: 1.0490553
Validation loss decreased (1.046763 --> 1.046669).  Saving model ...
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 1.0275595
	speed: 0.0192s/iter; left time: 6.2845s
	iters: 200, epoch: 9 | loss: 0.9791621
	speed: 0.0189s/iter; left time: 4.2958s
Epoch: 9 cost time: 4.102415561676025
Epoch: 9, Steps: 213 | Train Loss: 1.0040915 Vali Loss: 1.0487645 Test Loss: 1.0491374
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 0.9838361
	speed: 0.0219s/iter; left time: 2.4946s
	iters: 200, epoch: 10 | loss: 1.0018810
	speed: 0.0200s/iter; left time: 0.2803s
Epoch: 10 cost time: 4.327145576477051
Epoch: 10, Steps: 213 | Train Loss: 1.0039366 Vali Loss: 1.0484576 Test Loss: 1.0491731
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.8_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0490554571151733, mae:0.8215382695198059
Seed: 456
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.5_alpha0.8_96_192Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.5_alpha0.8.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           192                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.8_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0317860
	speed: 0.0358s/iter; left time: 71.6729s
	iters: 200, epoch: 1 | loss: 1.0144210
	speed: 0.0270s/iter; left time: 51.3151s
Epoch: 1 cost time: 5.6421239376068115
Epoch: 1, Steps: 210 | Train Loss: 1.0368187 Vali Loss: 1.0622433 Test Loss: 1.0488524
Validation loss decreased (inf --> 1.062243).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0165861
	speed: 0.0215s/iter; left time: 38.4769s
	iters: 200, epoch: 2 | loss: 1.0270523
	speed: 0.0219s/iter; left time: 37.0836s
Epoch: 2 cost time: 4.5464088916778564
Epoch: 2, Steps: 210 | Train Loss: 1.0260102 Vali Loss: 1.0568303 Test Loss: 1.0493412
Validation loss decreased (1.062243 --> 1.056830).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0209129
	speed: 0.0168s/iter; left time: 26.5319s
	iters: 200, epoch: 3 | loss: 1.0275719
	speed: 0.0145s/iter; left time: 21.5222s
Epoch: 3 cost time: 3.1435201168060303
Epoch: 3, Steps: 210 | Train Loss: 1.0225565 Vali Loss: 1.0565646 Test Loss: 1.0510349
Validation loss decreased (1.056830 --> 1.056565).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0194312
	speed: 0.0281s/iter; left time: 38.5152s
	iters: 200, epoch: 4 | loss: 1.0193332
	speed: 0.0224s/iter; left time: 28.4767s
Epoch: 4 cost time: 4.727046012878418
Epoch: 4, Steps: 210 | Train Loss: 1.0199279 Vali Loss: 1.0554574 Test Loss: 1.0524831
Validation loss decreased (1.056565 --> 1.055457).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0424438
	speed: 0.0170s/iter; left time: 19.7602s
	iters: 200, epoch: 5 | loss: 1.0090159
	speed: 0.0163s/iter; left time: 17.2992s
Epoch: 5 cost time: 3.535327196121216
Epoch: 5, Steps: 210 | Train Loss: 1.0181562 Vali Loss: 1.0542411 Test Loss: 1.0534738
Validation loss decreased (1.055457 --> 1.054241).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0070976
	speed: 0.0168s/iter; left time: 15.9582s
	iters: 200, epoch: 6 | loss: 0.9991497
	speed: 0.0178s/iter; left time: 15.1180s
Epoch: 6 cost time: 3.857754945755005
Epoch: 6, Steps: 210 | Train Loss: 1.0170735 Vali Loss: 1.0524905 Test Loss: 1.0540973
Validation loss decreased (1.054241 --> 1.052490).  Saving model ...
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0274881
	speed: 0.0181s/iter; left time: 13.4374s
	iters: 200, epoch: 7 | loss: 1.0195756
	speed: 0.0149s/iter; left time: 9.5741s
Epoch: 7 cost time: 3.2153632640838623
Epoch: 7, Steps: 210 | Train Loss: 1.0164583 Vali Loss: 1.0544372 Test Loss: 1.0544579
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 1.0134057
	speed: 0.0204s/iter; left time: 10.8289s
	iters: 200, epoch: 8 | loss: 0.9891071
	speed: 0.0182s/iter; left time: 7.8297s
Epoch: 8 cost time: 3.9177048206329346
Epoch: 8, Steps: 210 | Train Loss: 1.0162008 Vali Loss: 1.0551028 Test Loss: 1.0546173
EarlyStopping counter: 2 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 1.0551467
	speed: 0.0186s/iter; left time: 5.9656s
	iters: 200, epoch: 9 | loss: 1.0071958
	speed: 0.0162s/iter; left time: 3.5837s
Epoch: 9 cost time: 3.441016674041748
Epoch: 9, Steps: 210 | Train Loss: 1.0159272 Vali Loss: 1.0557081 Test Loss: 1.0546870
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 0.9863061
	speed: 0.0216s/iter; left time: 2.4030s
	iters: 200, epoch: 10 | loss: 1.0089858
	speed: 0.0187s/iter; left time: 0.2055s
Epoch: 10 cost time: 3.975179672241211
Epoch: 10, Steps: 210 | Train Loss: 1.0160272 Vali Loss: 1.0536997 Test Loss: 1.0547239
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.8_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.054097056388855, mae:0.8236421942710876
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.8_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0727872
	speed: 0.0136s/iter; left time: 27.2254s
	iters: 200, epoch: 1 | loss: 1.0422404
	speed: 0.0136s/iter; left time: 25.8497s
Epoch: 1 cost time: 2.8964219093322754
Epoch: 1, Steps: 210 | Train Loss: 1.0340146 Vali Loss: 1.0611169 Test Loss: 1.0475552
Validation loss decreased (inf --> 1.061117).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0345451
	speed: 0.0146s/iter; left time: 26.1705s
	iters: 200, epoch: 2 | loss: 1.0288430
	speed: 0.0117s/iter; left time: 19.8120s
Epoch: 2 cost time: 2.5541741847991943
Epoch: 2, Steps: 210 | Train Loss: 1.0260745 Vali Loss: 1.0596690 Test Loss: 1.0498400
Validation loss decreased (1.061117 --> 1.059669).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0417869
	speed: 0.0225s/iter; left time: 35.5198s
	iters: 200, epoch: 3 | loss: 0.9951054
	speed: 0.0189s/iter; left time: 28.0250s
Epoch: 3 cost time: 4.067552089691162
Epoch: 3, Steps: 210 | Train Loss: 1.0226054 Vali Loss: 1.0569237 Test Loss: 1.0520239
Validation loss decreased (1.059669 --> 1.056924).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0299822
	speed: 0.0218s/iter; left time: 29.9093s
	iters: 200, epoch: 4 | loss: 1.0219755
	speed: 0.0183s/iter; left time: 23.2463s
Epoch: 4 cost time: 3.863687515258789
Epoch: 4, Steps: 210 | Train Loss: 1.0198561 Vali Loss: 1.0539978 Test Loss: 1.0539166
Validation loss decreased (1.056924 --> 1.053998).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0385174
	speed: 0.0224s/iter; left time: 25.9524s
	iters: 200, epoch: 5 | loss: 0.9997916
	speed: 0.0166s/iter; left time: 17.5925s
Epoch: 5 cost time: 3.5430045127868652
Epoch: 5, Steps: 210 | Train Loss: 1.0179236 Vali Loss: 1.0552806 Test Loss: 1.0556536
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0281467
	speed: 0.0142s/iter; left time: 13.4764s
	iters: 200, epoch: 6 | loss: 1.0088127
	speed: 0.0136s/iter; left time: 11.5787s
Epoch: 6 cost time: 2.9587700366973877
Epoch: 6, Steps: 210 | Train Loss: 1.0168594 Vali Loss: 1.0528013 Test Loss: 1.0563118
Validation loss decreased (1.053998 --> 1.052801).  Saving model ...
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0061706
	speed: 0.0182s/iter; left time: 13.4514s
	iters: 200, epoch: 7 | loss: 1.0194061
	speed: 0.0155s/iter; left time: 9.9114s
Epoch: 7 cost time: 3.4593565464019775
Epoch: 7, Steps: 210 | Train Loss: 1.0163733 Vali Loss: 1.0546092 Test Loss: 1.0566609
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 1.0223780
	speed: 0.0190s/iter; left time: 10.0750s
	iters: 200, epoch: 8 | loss: 1.0147718
	speed: 0.0168s/iter; left time: 7.2623s
Epoch: 8 cost time: 3.5918374061584473
Epoch: 8, Steps: 210 | Train Loss: 1.0161801 Vali Loss: 1.0530850 Test Loss: 1.0567893
EarlyStopping counter: 2 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 1.0128204
	speed: 0.0193s/iter; left time: 6.2004s
	iters: 200, epoch: 9 | loss: 1.0196676
	speed: 0.0164s/iter; left time: 3.6246s
Epoch: 9 cost time: 3.580181121826172
Epoch: 9, Steps: 210 | Train Loss: 1.0158994 Vali Loss: 1.0534236 Test Loss: 1.0568534
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 1.0048981
	speed: 0.0151s/iter; left time: 1.6757s
	iters: 200, epoch: 10 | loss: 1.0113835
	speed: 0.0139s/iter; left time: 0.1524s
Epoch: 10 cost time: 2.9732892513275146
Epoch: 10, Steps: 210 | Train Loss: 1.0158587 Vali Loss: 1.0523740 Test Loss: 1.0568999
Validation loss decreased (1.052801 --> 1.052374).  Saving model ...
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.8_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0568997859954834, mae:0.8247621655464172
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.8_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0260493
	speed: 0.0176s/iter; left time: 35.1986s
	iters: 200, epoch: 1 | loss: 1.0258381
	speed: 0.0160s/iter; left time: 30.4952s
Epoch: 1 cost time: 3.4879720211029053
Epoch: 1, Steps: 210 | Train Loss: 1.0389647 Vali Loss: 1.0632724 Test Loss: 1.0489500
Validation loss decreased (inf --> 1.063272).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0322208
	speed: 0.0169s/iter; left time: 30.2347s
	iters: 200, epoch: 2 | loss: 1.0191586
	speed: 0.0173s/iter; left time: 29.2071s
Epoch: 2 cost time: 3.88038969039917
Epoch: 2, Steps: 210 | Train Loss: 1.0255587 Vali Loss: 1.0592067 Test Loss: 1.0496817
Validation loss decreased (1.063272 --> 1.059207).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0431418
	speed: 0.0131s/iter; left time: 20.6486s
	iters: 200, epoch: 3 | loss: 1.0203351
	speed: 0.0123s/iter; left time: 18.2741s
Epoch: 3 cost time: 2.7095301151275635
Epoch: 3, Steps: 210 | Train Loss: 1.0215580 Vali Loss: 1.0562817 Test Loss: 1.0499128
Validation loss decreased (1.059207 --> 1.056282).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0081600
	speed: 0.0120s/iter; left time: 16.5137s
	iters: 200, epoch: 4 | loss: 0.9960579
	speed: 0.0169s/iter; left time: 21.5142s
Epoch: 4 cost time: 3.7483556270599365
Epoch: 4, Steps: 210 | Train Loss: 1.0188195 Vali Loss: 1.0545045 Test Loss: 1.0515890
Validation loss decreased (1.056282 --> 1.054505).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0293669
	speed: 0.0150s/iter; left time: 17.3861s
	iters: 200, epoch: 5 | loss: 1.0386038
	speed: 0.0139s/iter; left time: 14.7626s
Epoch: 5 cost time: 3.040388584136963
Epoch: 5, Steps: 210 | Train Loss: 1.0169129 Vali Loss: 1.0576382 Test Loss: 1.0527281
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.9964782
	speed: 0.0173s/iter; left time: 16.4748s
	iters: 200, epoch: 6 | loss: 1.0329717
	speed: 0.0168s/iter; left time: 14.2844s
Epoch: 6 cost time: 3.598118305206299
Epoch: 6, Steps: 210 | Train Loss: 1.0158202 Vali Loss: 1.0558877 Test Loss: 1.0528789
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0305961
	speed: 0.0183s/iter; left time: 13.5858s
	iters: 200, epoch: 7 | loss: 1.0327245
	speed: 0.0169s/iter; left time: 10.8055s
Epoch: 7 cost time: 3.586346387863159
Epoch: 7, Steps: 210 | Train Loss: 1.0152686 Vali Loss: 1.0553185 Test Loss: 1.0531939
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 1.0279084
	speed: 0.0172s/iter; left time: 9.1255s
	iters: 200, epoch: 8 | loss: 1.0368159
	speed: 0.0139s/iter; left time: 6.0082s
Epoch: 8 cost time: 3.0306971073150635
Epoch: 8, Steps: 210 | Train Loss: 1.0148689 Vali Loss: 1.0544355 Test Loss: 1.0533632
Validation loss decreased (1.054505 --> 1.054435).  Saving model ...
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 1.0144970
	speed: 0.0201s/iter; left time: 6.4462s
	iters: 200, epoch: 9 | loss: 0.9666138
	speed: 0.0169s/iter; left time: 3.7270s
Epoch: 9 cost time: 3.5827462673187256
Epoch: 9, Steps: 210 | Train Loss: 1.0147536 Vali Loss: 1.0555223 Test Loss: 1.0534202
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 1.0205297
	speed: 0.0166s/iter; left time: 1.8386s
	iters: 200, epoch: 10 | loss: 1.0355840
	speed: 0.0158s/iter; left time: 0.1741s
Epoch: 10 cost time: 3.3582146167755127
Epoch: 10, Steps: 210 | Train Loss: 1.0147222 Vali Loss: 1.0575874 Test Loss: 1.0534508
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.8_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0533630847930908, mae:0.8234414458274841
Seed: 456
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.5_alpha0.8_96_336Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.5_alpha0.8.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           336                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.8_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0821068
	speed: 0.0289s/iter; left time: 56.5861s
	iters: 200, epoch: 1 | loss: 1.0438094
	speed: 0.0216s/iter; left time: 40.1446s
Epoch: 1 cost time: 4.451141119003296
Epoch: 1, Steps: 206 | Train Loss: 1.0407844 Vali Loss: 1.0529615 Test Loss: 1.0393146
Validation loss decreased (inf --> 1.052961).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0532452
	speed: 0.0186s/iter; left time: 32.6565s
	iters: 200, epoch: 2 | loss: 1.0123847
	speed: 0.0159s/iter; left time: 26.3773s
Epoch: 2 cost time: 3.376002788543701
Epoch: 2, Steps: 206 | Train Loss: 1.0318019 Vali Loss: 1.0514003 Test Loss: 1.0401703
Validation loss decreased (1.052961 --> 1.051400).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.9987205
	speed: 0.0191s/iter; left time: 29.6036s
	iters: 200, epoch: 3 | loss: 1.0493000
	speed: 0.0171s/iter; left time: 24.7599s
Epoch: 3 cost time: 3.5579512119293213
Epoch: 3, Steps: 206 | Train Loss: 1.0292537 Vali Loss: 1.0484427 Test Loss: 1.0393251
Validation loss decreased (1.051400 --> 1.048443).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0047257
	speed: 0.0127s/iter; left time: 17.0655s
	iters: 200, epoch: 4 | loss: 1.0171847
	speed: 0.0111s/iter; left time: 13.8131s
Epoch: 4 cost time: 2.423779010772705
Epoch: 4, Steps: 206 | Train Loss: 1.0273766 Vali Loss: 1.0493239 Test Loss: 1.0419476
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0498641
	speed: 0.0200s/iter; left time: 22.7691s
	iters: 200, epoch: 5 | loss: 1.0504774
	speed: 0.0187s/iter; left time: 19.3819s
Epoch: 5 cost time: 3.907618522644043
Epoch: 5, Steps: 206 | Train Loss: 1.0263624 Vali Loss: 1.0488409 Test Loss: 1.0415657
EarlyStopping counter: 2 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0113993
	speed: 0.0281s/iter; left time: 26.1687s
	iters: 200, epoch: 6 | loss: 0.9930898
	speed: 0.0213s/iter; left time: 17.6839s
Epoch: 6 cost time: 4.448804616928101
Epoch: 6, Steps: 206 | Train Loss: 1.0256265 Vali Loss: 1.0482945 Test Loss: 1.0421292
Validation loss decreased (1.048443 --> 1.048295).  Saving model ...
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0302960
	speed: 0.0185s/iter; left time: 13.4331s
	iters: 200, epoch: 7 | loss: 1.0397806
	speed: 0.0163s/iter; left time: 10.1573s
Epoch: 7 cost time: 3.404745101928711
Epoch: 7, Steps: 206 | Train Loss: 1.0252928 Vali Loss: 1.0486478 Test Loss: 1.0423819
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 1.0215187
	speed: 0.0164s/iter; left time: 8.5007s
	iters: 200, epoch: 8 | loss: 1.0403100
	speed: 0.0131s/iter; left time: 5.5092s
Epoch: 8 cost time: 2.81254243850708
Epoch: 8, Steps: 206 | Train Loss: 1.0249700 Vali Loss: 1.0486685 Test Loss: 1.0425158
EarlyStopping counter: 2 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 1.0337771
	speed: 0.0180s/iter; left time: 5.6432s
	iters: 200, epoch: 9 | loss: 1.0326195
	speed: 0.0165s/iter; left time: 3.5113s
Epoch: 9 cost time: 3.4539384841918945
Epoch: 9, Steps: 206 | Train Loss: 1.0248232 Vali Loss: 1.0485063 Test Loss: 1.0426012
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 1.0357124
	speed: 0.0193s/iter; left time: 2.0696s
	iters: 200, epoch: 10 | loss: 1.0080100
	speed: 0.0206s/iter; left time: 0.1442s
Epoch: 10 cost time: 4.413353443145752
Epoch: 10, Steps: 206 | Train Loss: 1.0247328 Vali Loss: 1.0487976 Test Loss: 1.0426383
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.8_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0421292781829834, mae:0.8198695778846741
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.8_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0234008
	speed: 0.0211s/iter; left time: 41.4684s
	iters: 200, epoch: 1 | loss: 1.0446922
	speed: 0.0171s/iter; left time: 31.8852s
Epoch: 1 cost time: 3.5934741497039795
Epoch: 1, Steps: 206 | Train Loss: 1.0418508 Vali Loss: 1.0533441 Test Loss: 1.0392755
Validation loss decreased (inf --> 1.053344).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0399690
	speed: 0.0129s/iter; left time: 22.6569s
	iters: 200, epoch: 2 | loss: 1.0142545
	speed: 0.0118s/iter; left time: 19.5241s
Epoch: 2 cost time: 2.518580436706543
Epoch: 2, Steps: 206 | Train Loss: 1.0316827 Vali Loss: 1.0513599 Test Loss: 1.0403538
Validation loss decreased (1.053344 --> 1.051360).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0001322
	speed: 0.0158s/iter; left time: 24.4693s
	iters: 200, epoch: 3 | loss: 1.0257745
	speed: 0.0152s/iter; left time: 22.0220s
Epoch: 3 cost time: 3.2469520568847656
Epoch: 3, Steps: 206 | Train Loss: 1.0290114 Vali Loss: 1.0491065 Test Loss: 1.0407107
Validation loss decreased (1.051360 --> 1.049106).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0324780
	speed: 0.0203s/iter; left time: 27.2825s
	iters: 200, epoch: 4 | loss: 1.0006014
	speed: 0.0168s/iter; left time: 20.8319s
Epoch: 4 cost time: 3.559180498123169
Epoch: 4, Steps: 206 | Train Loss: 1.0268972 Vali Loss: 1.0479137 Test Loss: 1.0409057
Validation loss decreased (1.049106 --> 1.047914).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0331736
	speed: 0.0227s/iter; left time: 25.8407s
	iters: 200, epoch: 5 | loss: 1.0501887
	speed: 0.0183s/iter; left time: 18.9268s
Epoch: 5 cost time: 3.7908432483673096
Epoch: 5, Steps: 206 | Train Loss: 1.0258484 Vali Loss: 1.0480874 Test Loss: 1.0416565
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0147779
	speed: 0.0168s/iter; left time: 15.6319s
	iters: 200, epoch: 6 | loss: 1.0234286
	speed: 0.0153s/iter; left time: 12.7051s
Epoch: 6 cost time: 3.3105885982513428
Epoch: 6, Steps: 206 | Train Loss: 1.0248947 Vali Loss: 1.0482581 Test Loss: 1.0419111
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0280393
	speed: 0.0162s/iter; left time: 11.7591s
	iters: 200, epoch: 7 | loss: 1.0260507
	speed: 0.0142s/iter; left time: 8.9040s
Epoch: 7 cost time: 3.0320751667022705
Epoch: 7, Steps: 206 | Train Loss: 1.0244249 Vali Loss: 1.0481805 Test Loss: 1.0421659
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 1.0329201
	speed: 0.0145s/iter; left time: 7.5350s
	iters: 200, epoch: 8 | loss: 1.0206275
	speed: 0.0133s/iter; left time: 5.5593s
Epoch: 8 cost time: 2.8176026344299316
Epoch: 8, Steps: 206 | Train Loss: 1.0243604 Vali Loss: 1.0480334 Test Loss: 1.0422651
EarlyStopping counter: 4 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 1.0041057
	speed: 0.0132s/iter; left time: 4.1207s
	iters: 200, epoch: 9 | loss: 1.0036995
	speed: 0.0121s/iter; left time: 2.5758s
Epoch: 9 cost time: 2.5315563678741455
Epoch: 9, Steps: 206 | Train Loss: 1.0243736 Vali Loss: 1.0483321 Test Loss: 1.0423131
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.8_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0409059524536133, mae:0.8193680644035339
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.8_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0232317
	speed: 0.0193s/iter; left time: 37.8568s
	iters: 200, epoch: 1 | loss: 1.0296210
	speed: 0.0158s/iter; left time: 29.3332s
Epoch: 1 cost time: 3.3084237575531006
Epoch: 1, Steps: 206 | Train Loss: 1.0415844 Vali Loss: 1.0527303 Test Loss: 1.0383840
Validation loss decreased (inf --> 1.052730).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0229002
	speed: 0.0169s/iter; left time: 29.6778s
	iters: 200, epoch: 2 | loss: 1.0163512
	speed: 0.0149s/iter; left time: 24.6630s
Epoch: 2 cost time: 3.1581544876098633
Epoch: 2, Steps: 206 | Train Loss: 1.0316389 Vali Loss: 1.0506991 Test Loss: 1.0389718
Validation loss decreased (1.052730 --> 1.050699).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0446055
	speed: 0.0165s/iter; left time: 25.6235s
	iters: 200, epoch: 3 | loss: 1.0062541
	speed: 0.0146s/iter; left time: 21.1225s
Epoch: 3 cost time: 3.0892152786254883
Epoch: 3, Steps: 206 | Train Loss: 1.0287548 Vali Loss: 1.0484658 Test Loss: 1.0404221
Validation loss decreased (1.050699 --> 1.048466).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0210615
	speed: 0.0154s/iter; left time: 20.7406s
	iters: 200, epoch: 4 | loss: 0.9974146
	speed: 0.0140s/iter; left time: 17.4190s
Epoch: 4 cost time: 2.9138503074645996
Epoch: 4, Steps: 206 | Train Loss: 1.0266794 Vali Loss: 1.0483567 Test Loss: 1.0412673
Validation loss decreased (1.048466 --> 1.048357).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0319735
	speed: 0.0202s/iter; left time: 22.9794s
	iters: 200, epoch: 5 | loss: 1.0222632
	speed: 0.0169s/iter; left time: 17.5372s
Epoch: 5 cost time: 3.5833401679992676
Epoch: 5, Steps: 206 | Train Loss: 1.0252844 Vali Loss: 1.0479105 Test Loss: 1.0418469
Validation loss decreased (1.048357 --> 1.047910).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0145725
	speed: 0.0192s/iter; left time: 17.8923s
	iters: 200, epoch: 6 | loss: 1.0564485
	speed: 0.0170s/iter; left time: 14.0978s
Epoch: 6 cost time: 3.534324884414673
Epoch: 6, Steps: 206 | Train Loss: 1.0245513 Vali Loss: 1.0477726 Test Loss: 1.0422480
Validation loss decreased (1.047910 --> 1.047773).  Saving model ...
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0341399
	speed: 0.0156s/iter; left time: 11.3091s
	iters: 200, epoch: 7 | loss: 1.0341127
	speed: 0.0152s/iter; left time: 9.4726s
Epoch: 7 cost time: 3.243169069290161
Epoch: 7, Steps: 206 | Train Loss: 1.0241848 Vali Loss: 1.0474236 Test Loss: 1.0425543
Validation loss decreased (1.047773 --> 1.047424).  Saving model ...
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 1.0313971
	speed: 0.0170s/iter; left time: 8.8004s
	iters: 200, epoch: 8 | loss: 0.9823378
	speed: 0.0146s/iter; left time: 6.1263s
Epoch: 8 cost time: 3.0627658367156982
Epoch: 8, Steps: 206 | Train Loss: 1.0238449 Vali Loss: 1.0478277 Test Loss: 1.0427061
EarlyStopping counter: 1 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 1.0405790
	speed: 0.0148s/iter; left time: 4.6347s
	iters: 200, epoch: 9 | loss: 1.0130341
	speed: 0.0137s/iter; left time: 2.9164s
Epoch: 9 cost time: 2.9376659393310547
Epoch: 9, Steps: 206 | Train Loss: 1.0238373 Vali Loss: 1.0476214 Test Loss: 1.0427456
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 1.0185751
	speed: 0.0197s/iter; left time: 2.1039s
	iters: 200, epoch: 10 | loss: 1.0221355
	speed: 0.0172s/iter; left time: 0.1204s
Epoch: 10 cost time: 3.612060308456421
Epoch: 10, Steps: 206 | Train Loss: 1.0235813 Vali Loss: 1.0477194 Test Loss: 1.0427779
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.8_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0425541400909424, mae:0.8199806809425354
Seed: 456
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.5_alpha0.8_96_720Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.5_alpha0.8.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           720                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.8_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0348228
	speed: 0.0316s/iter; left time: 58.2572s
Epoch: 1 cost time: 5.1078808307647705
Epoch: 1, Steps: 194 | Train Loss: 1.0453051 Vali Loss: 1.0452157 Test Loss: 1.0385190
Validation loss decreased (inf --> 1.045216).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0476428
	speed: 0.0209s/iter; left time: 34.4601s
Epoch: 2 cost time: 3.5412535667419434
Epoch: 2, Steps: 194 | Train Loss: 1.0381665 Vali Loss: 1.0450507 Test Loss: 1.0391333
Validation loss decreased (1.045216 --> 1.045051).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0338686
	speed: 0.0181s/iter; left time: 26.2485s
Epoch: 3 cost time: 3.851414680480957
Epoch: 3, Steps: 194 | Train Loss: 1.0370213 Vali Loss: 1.0446076 Test Loss: 1.0392830
Validation loss decreased (1.045051 --> 1.044608).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0313128
	speed: 0.0177s/iter; left time: 22.3230s
Epoch: 4 cost time: 2.996859073638916
Epoch: 4, Steps: 194 | Train Loss: 1.0362024 Vali Loss: 1.0445668 Test Loss: 1.0398932
Validation loss decreased (1.044608 --> 1.044567).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0480622
	speed: 0.0167s/iter; left time: 17.7753s
Epoch: 5 cost time: 2.786505937576294
Epoch: 5, Steps: 194 | Train Loss: 1.0354190 Vali Loss: 1.0443358 Test Loss: 1.0404621
Validation loss decreased (1.044567 --> 1.044336).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0319619
	speed: 0.0156s/iter; left time: 13.6307s
Epoch: 6 cost time: 3.0846080780029297
Epoch: 6, Steps: 194 | Train Loss: 1.0350809 Vali Loss: 1.0443197 Test Loss: 1.0404397
Validation loss decreased (1.044336 --> 1.044320).  Saving model ...
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0125594
	speed: 0.0197s/iter; left time: 13.3255s
Epoch: 7 cost time: 3.6384799480438232
Epoch: 7, Steps: 194 | Train Loss: 1.0349351 Vali Loss: 1.0445026 Test Loss: 1.0405768
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 1.0382699
	speed: 0.0196s/iter; left time: 9.4814s
Epoch: 8 cost time: 3.3549644947052
Epoch: 8, Steps: 194 | Train Loss: 1.0346701 Vali Loss: 1.0446950 Test Loss: 1.0405840
EarlyStopping counter: 2 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 1.0200541
	speed: 0.0147s/iter; left time: 4.2386s
Epoch: 9 cost time: 2.8612914085388184
Epoch: 9, Steps: 194 | Train Loss: 1.0345451 Vali Loss: 1.0444148 Test Loss: 1.0406314
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 1.0315849
	speed: 0.0147s/iter; left time: 1.3963s
Epoch: 10 cost time: 2.812121629714966
Epoch: 10, Steps: 194 | Train Loss: 1.0346570 Vali Loss: 1.0444614 Test Loss: 1.0406470
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.8_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0404398441314697, mae:0.81855708360672
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.8_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0648102
	speed: 0.0169s/iter; left time: 31.0876s
Epoch: 1 cost time: 3.0602331161499023
Epoch: 1, Steps: 194 | Train Loss: 1.0476287 Vali Loss: 1.0457333 Test Loss: 1.0400089
Validation loss decreased (inf --> 1.045733).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0344300
	speed: 0.0268s/iter; left time: 44.1869s
Epoch: 2 cost time: 3.9286673069000244
Epoch: 2, Steps: 194 | Train Loss: 1.0381981 Vali Loss: 1.0443647 Test Loss: 1.0393827
Validation loss decreased (1.045733 --> 1.044365).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0313523
	speed: 0.0162s/iter; left time: 23.5746s
Epoch: 3 cost time: 2.830437660217285
Epoch: 3, Steps: 194 | Train Loss: 1.0368296 Vali Loss: 1.0439110 Test Loss: 1.0400761
Validation loss decreased (1.044365 --> 1.043911).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0173131
	speed: 0.0183s/iter; left time: 23.0821s
Epoch: 4 cost time: 3.3956639766693115
Epoch: 4, Steps: 194 | Train Loss: 1.0357300 Vali Loss: 1.0436634 Test Loss: 1.0403326
Validation loss decreased (1.043911 --> 1.043663).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0385841
	speed: 0.0273s/iter; left time: 29.1187s
Epoch: 5 cost time: 4.334779977798462
Epoch: 5, Steps: 194 | Train Loss: 1.0349949 Vali Loss: 1.0441966 Test Loss: 1.0400655
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0296212
	speed: 0.0173s/iter; left time: 15.0450s
Epoch: 6 cost time: 2.9773881435394287
Epoch: 6, Steps: 194 | Train Loss: 1.0345942 Vali Loss: 1.0440886 Test Loss: 1.0401838
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0285114
	speed: 0.0174s/iter; left time: 11.7733s
Epoch: 7 cost time: 2.946065664291382
Epoch: 7, Steps: 194 | Train Loss: 1.0342350 Vali Loss: 1.0442884 Test Loss: 1.0402189
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 1.0324246
	speed: 0.0120s/iter; left time: 5.7726s
Epoch: 8 cost time: 2.2921743392944336
Epoch: 8, Steps: 194 | Train Loss: 1.0340173 Vali Loss: 1.0443110 Test Loss: 1.0402693
EarlyStopping counter: 4 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 1.0261641
	speed: 0.0195s/iter; left time: 5.6310s
Epoch: 9 cost time: 3.7539772987365723
Epoch: 9, Steps: 194 | Train Loss: 1.0338726 Vali Loss: 1.0442071 Test Loss: 1.0402817
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.8_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0403324365615845, mae:0.818476140499115
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.5_alpha0.8_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0394491
	speed: 0.0185s/iter; left time: 34.1494s
Epoch: 1 cost time: 3.257399797439575
Epoch: 1, Steps: 194 | Train Loss: 1.0460554 Vali Loss: 1.0454559 Test Loss: 1.0387435
Validation loss decreased (inf --> 1.045456).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0425200
	speed: 0.0168s/iter; left time: 27.6551s
Epoch: 2 cost time: 2.9728384017944336
Epoch: 2, Steps: 194 | Train Loss: 1.0375590 Vali Loss: 1.0451804 Test Loss: 1.0398188
Validation loss decreased (1.045456 --> 1.045180).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0308374
	speed: 0.0154s/iter; left time: 22.3217s
Epoch: 3 cost time: 2.5398786067962646
Epoch: 3, Steps: 194 | Train Loss: 1.0361088 Vali Loss: 1.0447497 Test Loss: 1.0389301
Validation loss decreased (1.045180 --> 1.044750).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0753963
	speed: 0.0156s/iter; left time: 19.6361s
Epoch: 4 cost time: 3.0275607109069824
Epoch: 4, Steps: 194 | Train Loss: 1.0350104 Vali Loss: 1.0443200 Test Loss: 1.0396652
Validation loss decreased (1.044750 --> 1.044320).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0295897
	speed: 0.0174s/iter; left time: 18.5080s
Epoch: 5 cost time: 3.4619193077087402
Epoch: 5, Steps: 194 | Train Loss: 1.0340897 Vali Loss: 1.0445864 Test Loss: 1.0400575
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0160868
	speed: 0.0161s/iter; left time: 14.0454s
Epoch: 6 cost time: 3.1967873573303223
Epoch: 6, Steps: 194 | Train Loss: 1.0337127 Vali Loss: 1.0440564 Test Loss: 1.0403410
Validation loss decreased (1.044320 --> 1.044056).  Saving model ...
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 1.0201091
	speed: 0.0155s/iter; left time: 10.4752s
Epoch: 7 cost time: 2.586841106414795
Epoch: 7, Steps: 194 | Train Loss: 1.0334743 Vali Loss: 1.0442445 Test Loss: 1.0406196
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 1.0335025
	speed: 0.0134s/iter; left time: 6.4529s
Epoch: 8 cost time: 2.2600271701812744
Epoch: 8, Steps: 194 | Train Loss: 1.0332637 Vali Loss: 1.0441465 Test Loss: 1.0406576
EarlyStopping counter: 2 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 1.0460041
	speed: 0.0159s/iter; left time: 4.5934s
Epoch: 9 cost time: 2.931891918182373
Epoch: 9, Steps: 194 | Train Loss: 1.0331161 Vali Loss: 1.0443448 Test Loss: 1.0407301
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 1.0316197
	speed: 0.0149s/iter; left time: 1.4166s
Epoch: 10 cost time: 2.763594150543213
Epoch: 10, Steps: 194 | Train Loss: 1.0330181 Vali Loss: 1.0443032 Test Loss: 1.0407487
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_gamma0.5_alpha0.8_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.0403412580490112, mae:0.8184574842453003
Seed: 42
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.95_alpha0.8_96_96Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.95_alpha0.8.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           96                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.8_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.5329937
	speed: 0.0275s/iter; left time: 55.9230s
	iters: 200, epoch: 1 | loss: 0.5222079
	speed: 0.0197s/iter; left time: 38.0524s
Epoch: 1 cost time: 4.177984714508057
Epoch: 1, Steps: 213 | Train Loss: 0.5267204 Vali Loss: 0.5137256 Test Loss: 0.6239823
Validation loss decreased (inf --> 0.513726).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.4019559
	speed: 0.0170s/iter; left time: 30.8274s
	iters: 200, epoch: 2 | loss: 0.4513591
	speed: 0.0139s/iter; left time: 23.8653s
Epoch: 2 cost time: 3.0930089950561523
Epoch: 2, Steps: 213 | Train Loss: 0.4903398 Vali Loss: 0.5155814 Test Loss: 0.6260436
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.5281455
	speed: 0.0205s/iter; left time: 32.9676s
	iters: 200, epoch: 3 | loss: 0.5479783
	speed: 0.0176s/iter; left time: 26.5599s
Epoch: 3 cost time: 3.8599250316619873
Epoch: 3, Steps: 213 | Train Loss: 0.4644985 Vali Loss: 0.5431351 Test Loss: 0.6312053
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.4411679
	speed: 0.0201s/iter; left time: 27.9759s
	iters: 200, epoch: 4 | loss: 0.4108902
	speed: 0.0212s/iter; left time: 27.3442s
Epoch: 4 cost time: 4.597609519958496
Epoch: 4, Steps: 213 | Train Loss: 0.4460171 Vali Loss: 0.5189484 Test Loss: 0.6294279
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.4209912
	speed: 0.0175s/iter; left time: 20.6566s
	iters: 200, epoch: 5 | loss: 0.4079673
	speed: 0.0135s/iter; left time: 14.6147s
Epoch: 5 cost time: 2.906247854232788
Epoch: 5, Steps: 213 | Train Loss: 0.4363235 Vali Loss: 0.5324127 Test Loss: 0.6418988
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.4483906
	speed: 0.0165s/iter; left time: 15.8966s
	iters: 200, epoch: 6 | loss: 0.4468478
	speed: 0.0168s/iter; left time: 14.5452s
Epoch: 6 cost time: 3.6707258224487305
Epoch: 6, Steps: 213 | Train Loss: 0.4296894 Vali Loss: 0.5387597 Test Loss: 0.6486684
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.8_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.6239822506904602, mae:0.6247373223304749
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.8_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.5140807
	speed: 0.0178s/iter; left time: 36.1261s
	iters: 200, epoch: 1 | loss: 0.4884436
	speed: 0.0170s/iter; left time: 32.8714s
Epoch: 1 cost time: 3.7478885650634766
Epoch: 1, Steps: 213 | Train Loss: 0.5238055 Vali Loss: 0.5001938 Test Loss: 0.5945461
Validation loss decreased (inf --> 0.500194).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.4858457
	speed: 0.0249s/iter; left time: 45.2927s
	iters: 200, epoch: 2 | loss: 0.5730602
	speed: 0.0193s/iter; left time: 33.2426s
Epoch: 2 cost time: 4.097201108932495
Epoch: 2, Steps: 213 | Train Loss: 0.4873755 Vali Loss: 0.5080019 Test Loss: 0.6029359
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.3997161
	speed: 0.0138s/iter; left time: 22.1110s
	iters: 200, epoch: 3 | loss: 0.4558371
	speed: 0.0134s/iter; left time: 20.1435s
Epoch: 3 cost time: 2.969392776489258
Epoch: 3, Steps: 213 | Train Loss: 0.4521897 Vali Loss: 0.5434842 Test Loss: 0.6631531
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.4014488
	speed: 0.0159s/iter; left time: 22.0908s
	iters: 200, epoch: 4 | loss: 0.4200570
	speed: 0.0161s/iter; left time: 20.8053s
Epoch: 4 cost time: 3.718209981918335
Epoch: 4, Steps: 213 | Train Loss: 0.4304178 Vali Loss: 0.5464041 Test Loss: 0.6634786
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.4547762
	speed: 0.0186s/iter; left time: 21.8886s
	iters: 200, epoch: 5 | loss: 0.3919311
	speed: 0.0173s/iter; left time: 18.6366s
Epoch: 5 cost time: 3.8117129802703857
Epoch: 5, Steps: 213 | Train Loss: 0.4184039 Vali Loss: 0.5396487 Test Loss: 0.6568314
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.4197296
	speed: 0.0174s/iter; left time: 16.8326s
	iters: 200, epoch: 6 | loss: 0.4031777
	speed: 0.0157s/iter; left time: 13.5619s
Epoch: 6 cost time: 3.4068615436553955
Epoch: 6, Steps: 213 | Train Loss: 0.4134302 Vali Loss: 0.5404459 Test Loss: 0.6636866
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.8_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.5945461392402649, mae:0.6082891225814819
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.8_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.5307609
	speed: 0.0120s/iter; left time: 24.3748s
	iters: 200, epoch: 1 | loss: 0.5754811
	speed: 0.0112s/iter; left time: 21.5954s
Epoch: 1 cost time: 2.4846670627593994
Epoch: 1, Steps: 213 | Train Loss: 0.5252921 Vali Loss: 0.4986655 Test Loss: 0.5970270
Validation loss decreased (inf --> 0.498666).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.4448981
	speed: 0.0162s/iter; left time: 29.4725s
	iters: 200, epoch: 2 | loss: 0.4130242
	speed: 0.0164s/iter; left time: 28.2292s
Epoch: 2 cost time: 3.5460145473480225
Epoch: 2, Steps: 213 | Train Loss: 0.4886797 Vali Loss: 0.5044186 Test Loss: 0.6105838
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.4985152
	speed: 0.0193s/iter; left time: 30.9600s
	iters: 200, epoch: 3 | loss: 0.4415681
	speed: 0.0171s/iter; left time: 25.7560s
Epoch: 3 cost time: 3.7285423278808594
Epoch: 3, Steps: 213 | Train Loss: 0.4597092 Vali Loss: 0.4849931 Test Loss: 0.6317138
Validation loss decreased (0.498666 --> 0.484993).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.4694137
	speed: 0.0201s/iter; left time: 27.9223s
	iters: 200, epoch: 4 | loss: 0.3952164
	speed: 0.0175s/iter; left time: 22.6457s
Epoch: 4 cost time: 3.8077774047851562
Epoch: 4, Steps: 213 | Train Loss: 0.4409784 Vali Loss: 0.5023767 Test Loss: 0.6437970
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.3830028
	speed: 0.0167s/iter; left time: 19.6776s
	iters: 200, epoch: 5 | loss: 0.4309025
	speed: 0.0138s/iter; left time: 14.9221s
Epoch: 5 cost time: 2.9992239475250244
Epoch: 5, Steps: 213 | Train Loss: 0.4286554 Vali Loss: 0.5180683 Test Loss: 0.6305415
EarlyStopping counter: 2 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.4243408
	speed: 0.0163s/iter; left time: 15.7706s
	iters: 200, epoch: 6 | loss: 0.3843342
	speed: 0.0182s/iter; left time: 15.7839s
Epoch: 6 cost time: 4.048863649368286
Epoch: 6, Steps: 213 | Train Loss: 0.4242350 Vali Loss: 0.5139447 Test Loss: 0.6354920
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.4101445
	speed: 0.0197s/iter; left time: 14.8382s
	iters: 200, epoch: 7 | loss: 0.3981527
	speed: 0.0156s/iter; left time: 10.1627s
Epoch: 7 cost time: 3.3672847747802734
Epoch: 7, Steps: 213 | Train Loss: 0.4208689 Vali Loss: 0.5191382 Test Loss: 0.6402680
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.3872992
	speed: 0.0228s/iter; left time: 12.2979s
	iters: 200, epoch: 8 | loss: 0.3903417
	speed: 0.0184s/iter; left time: 8.1110s
Epoch: 8 cost time: 4.002341985702515
Epoch: 8, Steps: 213 | Train Loss: 0.4190069 Vali Loss: 0.5131796 Test Loss: 0.6411985
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.8_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.6317139863967896, mae:0.6261582374572754
Seed: 42
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.95_alpha0.8_96_192Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.95_alpha0.8.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           192                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.8_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.7752222
	speed: 0.0380s/iter; left time: 76.0418s
	iters: 200, epoch: 1 | loss: 0.6450700
	speed: 0.0267s/iter; left time: 50.7651s
Epoch: 1 cost time: 5.565509557723999
Epoch: 1, Steps: 210 | Train Loss: 0.6540501 Vali Loss: 0.6259448 Test Loss: 0.9174073
Validation loss decreased (inf --> 0.625945).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.6930652
	speed: 0.0134s/iter; left time: 24.0681s
	iters: 200, epoch: 2 | loss: 0.5566582
	speed: 0.0133s/iter; left time: 22.5175s
Epoch: 2 cost time: 2.8912477493286133
Epoch: 2, Steps: 210 | Train Loss: 0.6133302 Vali Loss: 0.6112784 Test Loss: 0.9250547
Validation loss decreased (0.625945 --> 0.611278).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.5344315
	speed: 0.0193s/iter; left time: 30.4902s
	iters: 200, epoch: 3 | loss: 0.5771818
	speed: 0.0182s/iter; left time: 26.9219s
Epoch: 3 cost time: 3.925426721572876
Epoch: 3, Steps: 210 | Train Loss: 0.5798141 Vali Loss: 0.6589319 Test Loss: 0.9887027
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.6173087
	speed: 0.0196s/iter; left time: 26.8310s
	iters: 200, epoch: 4 | loss: 0.5173985
	speed: 0.0214s/iter; left time: 27.2029s
Epoch: 4 cost time: 4.67353367805481
Epoch: 4, Steps: 210 | Train Loss: 0.5549929 Vali Loss: 0.6622139 Test Loss: 1.0001298
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.5416749
	speed: 0.0234s/iter; left time: 27.1832s
	iters: 200, epoch: 5 | loss: 0.5697495
	speed: 0.0187s/iter; left time: 19.8084s
Epoch: 5 cost time: 3.960259437561035
Epoch: 5, Steps: 210 | Train Loss: 0.5379755 Vali Loss: 0.6610447 Test Loss: 1.0242785
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.5385823
	speed: 0.0146s/iter; left time: 13.9034s
	iters: 200, epoch: 6 | loss: 0.5351469
	speed: 0.0118s/iter; left time: 10.0027s
Epoch: 6 cost time: 2.4890494346618652
Epoch: 6, Steps: 210 | Train Loss: 0.5279601 Vali Loss: 0.6651379 Test Loss: 1.0336080
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.5663217
	speed: 0.0165s/iter; left time: 12.2075s
	iters: 200, epoch: 7 | loss: 0.5051208
	speed: 0.0156s/iter; left time: 9.9806s
Epoch: 7 cost time: 3.3821628093719482
Epoch: 7, Steps: 210 | Train Loss: 0.5226237 Vali Loss: 0.6807196 Test Loss: 1.0388819
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.8_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.925054669380188, mae:0.7544670104980469
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.8_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.6793566
	speed: 0.0246s/iter; left time: 49.1949s
	iters: 200, epoch: 1 | loss: 0.6381993
	speed: 0.0235s/iter; left time: 44.5832s
Epoch: 1 cost time: 5.028983116149902
Epoch: 1, Steps: 210 | Train Loss: 0.6592438 Vali Loss: 0.5939238 Test Loss: 0.9408323
Validation loss decreased (inf --> 0.593924).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.6023859
	speed: 0.0203s/iter; left time: 36.3044s
	iters: 200, epoch: 2 | loss: 0.6561545
	speed: 0.0166s/iter; left time: 28.1526s
Epoch: 2 cost time: 3.5429086685180664
Epoch: 2, Steps: 210 | Train Loss: 0.6134739 Vali Loss: 0.5863917 Test Loss: 0.9398181
Validation loss decreased (0.593924 --> 0.586392).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.5300515
	speed: 0.0185s/iter; left time: 29.3229s
	iters: 200, epoch: 3 | loss: 0.5897493
	speed: 0.0178s/iter; left time: 26.3583s
Epoch: 3 cost time: 3.848609685897827
Epoch: 3, Steps: 210 | Train Loss: 0.5709625 Vali Loss: 0.6591541 Test Loss: 1.0154263
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.4671861
	speed: 0.0204s/iter; left time: 27.9017s
	iters: 200, epoch: 4 | loss: 0.5606127
	speed: 0.0179s/iter; left time: 22.7502s
Epoch: 4 cost time: 3.8873836994171143
Epoch: 4, Steps: 210 | Train Loss: 0.5443514 Vali Loss: 0.6505747 Test Loss: 1.0149949
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.5613375
	speed: 0.0234s/iter; left time: 27.1114s
	iters: 200, epoch: 5 | loss: 0.5794920
	speed: 0.0182s/iter; left time: 19.2789s
Epoch: 5 cost time: 3.841623544692993
Epoch: 5, Steps: 210 | Train Loss: 0.5316361 Vali Loss: 0.6581551 Test Loss: 1.0065247
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.5595886
	speed: 0.0146s/iter; left time: 13.9050s
	iters: 200, epoch: 6 | loss: 0.5703245
	speed: 0.0116s/iter; left time: 9.8866s
Epoch: 6 cost time: 2.5758132934570312
Epoch: 6, Steps: 210 | Train Loss: 0.5217725 Vali Loss: 0.6664169 Test Loss: 1.0258080
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.4797822
	speed: 0.0188s/iter; left time: 13.9313s
	iters: 200, epoch: 7 | loss: 0.4926400
	speed: 0.0177s/iter; left time: 11.3458s
Epoch: 7 cost time: 3.8336246013641357
Epoch: 7, Steps: 210 | Train Loss: 0.5193561 Vali Loss: 0.6593876 Test Loss: 1.0042828
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.8_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.9398180246353149, mae:0.7591456174850464
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.8_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.6912534
	speed: 0.0197s/iter; left time: 39.3891s
	iters: 200, epoch: 1 | loss: 0.6889348
	speed: 0.0178s/iter; left time: 33.8413s
Epoch: 1 cost time: 3.8373301029205322
Epoch: 1, Steps: 210 | Train Loss: 0.6575868 Vali Loss: 0.5882669 Test Loss: 0.9312221
Validation loss decreased (inf --> 0.588267).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.6304172
	speed: 0.0199s/iter; left time: 35.5826s
	iters: 200, epoch: 2 | loss: 0.5219640
	speed: 0.0209s/iter; left time: 35.3959s
Epoch: 2 cost time: 4.327126502990723
Epoch: 2, Steps: 210 | Train Loss: 0.6085211 Vali Loss: 0.6565650 Test Loss: 0.9486792
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.5977463
	speed: 0.0164s/iter; left time: 26.0041s
	iters: 200, epoch: 3 | loss: 0.5279707
	speed: 0.0139s/iter; left time: 20.5360s
Epoch: 3 cost time: 3.007899522781372
Epoch: 3, Steps: 210 | Train Loss: 0.5685202 Vali Loss: 0.6037874 Test Loss: 0.9109920
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.5419357
	speed: 0.0135s/iter; left time: 18.4714s
	iters: 200, epoch: 4 | loss: 0.4716522
	speed: 0.0124s/iter; left time: 15.7652s
Epoch: 4 cost time: 2.702576160430908
Epoch: 4, Steps: 210 | Train Loss: 0.5446405 Vali Loss: 0.6317384 Test Loss: 0.9415555
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.5285827
	speed: 0.0120s/iter; left time: 13.8950s
	iters: 200, epoch: 5 | loss: 0.5069317
	speed: 0.0141s/iter; left time: 14.9727s
Epoch: 5 cost time: 3.178313732147217
Epoch: 5, Steps: 210 | Train Loss: 0.5305455 Vali Loss: 0.6268772 Test Loss: 0.9339294
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.5540258
	speed: 0.0175s/iter; left time: 16.6426s
	iters: 200, epoch: 6 | loss: 0.4857713
	speed: 0.0172s/iter; left time: 14.6240s
Epoch: 6 cost time: 3.6876161098480225
Epoch: 6, Steps: 210 | Train Loss: 0.5230326 Vali Loss: 0.6340526 Test Loss: 0.9342315
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.8_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.9312220811843872, mae:0.7567710280418396
Seed: 42
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.95_alpha0.8_96_336Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.95_alpha0.8.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           336                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.8_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.8830596
	speed: 0.0309s/iter; left time: 60.6854s
	iters: 200, epoch: 1 | loss: 0.8246372
	speed: 0.0232s/iter; left time: 43.2056s
Epoch: 1 cost time: 4.8111512660980225
Epoch: 1, Steps: 206 | Train Loss: 0.8305413 Vali Loss: 0.7341490 Test Loss: 1.3035922
Validation loss decreased (inf --> 0.734149).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.8108581
	speed: 0.0205s/iter; left time: 36.0370s
	iters: 200, epoch: 2 | loss: 0.7219111
	speed: 0.0181s/iter; left time: 29.9204s
Epoch: 2 cost time: 3.8296310901641846
Epoch: 2, Steps: 206 | Train Loss: 0.7854858 Vali Loss: 0.7111780 Test Loss: 1.4048018
Validation loss decreased (0.734149 --> 0.711178).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.9529710
	speed: 0.0160s/iter; left time: 24.7198s
	iters: 200, epoch: 3 | loss: 0.6218065
	speed: 0.0151s/iter; left time: 21.8525s
Epoch: 3 cost time: 3.1491870880126953
Epoch: 3, Steps: 206 | Train Loss: 0.7443734 Vali Loss: 0.7120132 Test Loss: 1.4123071
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.5810041
	speed: 0.0132s/iter; left time: 17.7736s
	iters: 200, epoch: 4 | loss: 0.6095529
	speed: 0.0131s/iter; left time: 16.2296s
Epoch: 4 cost time: 2.7828385829925537
Epoch: 4, Steps: 206 | Train Loss: 0.7077347 Vali Loss: 0.7112544 Test Loss: 1.4952043
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.6222320
	speed: 0.0181s/iter; left time: 20.6197s
	iters: 200, epoch: 5 | loss: 0.6396841
	speed: 0.0162s/iter; left time: 16.8281s
Epoch: 5 cost time: 3.45586895942688
Epoch: 5, Steps: 206 | Train Loss: 0.6836095 Vali Loss: 0.7195387 Test Loss: 1.5275817
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.6511095
	speed: 0.0153s/iter; left time: 14.2880s
	iters: 200, epoch: 6 | loss: 0.7023102
	speed: 0.0173s/iter; left time: 14.4056s
Epoch: 6 cost time: 3.7653696537017822
Epoch: 6, Steps: 206 | Train Loss: 0.6704874 Vali Loss: 0.7350217 Test Loss: 1.5465766
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.5710564
	speed: 0.0150s/iter; left time: 10.8791s
	iters: 200, epoch: 7 | loss: 0.7198240
	speed: 0.0142s/iter; left time: 8.8831s
Epoch: 7 cost time: 3.051466941833496
Epoch: 7, Steps: 206 | Train Loss: 0.6631054 Vali Loss: 0.7353251 Test Loss: 1.5426720
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.8_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.4048017263412476, mae:0.9317325949668884
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.8_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.7042761
	speed: 0.0127s/iter; left time: 24.9382s
	iters: 200, epoch: 1 | loss: 0.8843168
	speed: 0.0132s/iter; left time: 24.5874s
Epoch: 1 cost time: 2.8347103595733643
Epoch: 1, Steps: 206 | Train Loss: 0.8318339 Vali Loss: 0.6954168 Test Loss: 1.3048915
Validation loss decreased (inf --> 0.695417).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.6818130
	speed: 0.0158s/iter; left time: 27.7880s
	iters: 200, epoch: 2 | loss: 0.6714909
	speed: 0.0155s/iter; left time: 25.6408s
Epoch: 2 cost time: 3.3086111545562744
Epoch: 2, Steps: 206 | Train Loss: 0.7838044 Vali Loss: 0.6760923 Test Loss: 1.3444442
Validation loss decreased (0.695417 --> 0.676092).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.6277602
	speed: 0.0187s/iter; left time: 28.9015s
	iters: 200, epoch: 3 | loss: 0.8280927
	speed: 0.0170s/iter; left time: 24.7025s
Epoch: 3 cost time: 3.5809531211853027
Epoch: 3, Steps: 206 | Train Loss: 0.7351497 Vali Loss: 0.6732116 Test Loss: 1.3616803
Validation loss decreased (0.676092 --> 0.673212).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.6721258
	speed: 0.0203s/iter; left time: 27.2832s
	iters: 200, epoch: 4 | loss: 0.6370231
	speed: 0.0199s/iter; left time: 24.7487s
Epoch: 4 cost time: 4.152288436889648
Epoch: 4, Steps: 206 | Train Loss: 0.6966483 Vali Loss: 0.6966260 Test Loss: 1.4114588
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.5957556
	speed: 0.0135s/iter; left time: 15.3360s
	iters: 200, epoch: 5 | loss: 0.7482973
	speed: 0.0128s/iter; left time: 13.2485s
Epoch: 5 cost time: 2.6745193004608154
Epoch: 5, Steps: 206 | Train Loss: 0.6697561 Vali Loss: 0.6811401 Test Loss: 1.4086447
EarlyStopping counter: 2 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.6558512
	speed: 0.0194s/iter; left time: 18.0894s
	iters: 200, epoch: 6 | loss: 0.7062811
	speed: 0.0181s/iter; left time: 15.0729s
Epoch: 6 cost time: 3.88267183303833
Epoch: 6, Steps: 206 | Train Loss: 0.6529601 Vali Loss: 0.6781234 Test Loss: 1.4333339
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.6309530
	speed: 0.0170s/iter; left time: 12.3549s
	iters: 200, epoch: 7 | loss: 0.6414795
	speed: 0.0161s/iter; left time: 10.0542s
Epoch: 7 cost time: 3.424221992492676
Epoch: 7, Steps: 206 | Train Loss: 0.6479724 Vali Loss: 0.6936506 Test Loss: 1.4383031
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.7429355
	speed: 0.0153s/iter; left time: 7.9363s
	iters: 200, epoch: 8 | loss: 0.6363339
	speed: 0.0143s/iter; left time: 5.9757s
Epoch: 8 cost time: 3.0436856746673584
Epoch: 8, Steps: 206 | Train Loss: 0.6433323 Vali Loss: 0.6939607 Test Loss: 1.4461845
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.8_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.361680269241333, mae:0.9189411401748657
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.8_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.7593355
	speed: 0.0140s/iter; left time: 27.4369s
	iters: 200, epoch: 1 | loss: 0.6963705
	speed: 0.0132s/iter; left time: 24.6424s
Epoch: 1 cost time: 2.7782485485076904
Epoch: 1, Steps: 206 | Train Loss: 0.8299331 Vali Loss: 0.6763588 Test Loss: 1.2910860
Validation loss decreased (inf --> 0.676359).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.8191823
	speed: 0.0176s/iter; left time: 30.9424s
	iters: 200, epoch: 2 | loss: 0.8027455
	speed: 0.0162s/iter; left time: 26.8004s
Epoch: 2 cost time: 3.4283125400543213
Epoch: 2, Steps: 206 | Train Loss: 0.7739799 Vali Loss: 0.6962035 Test Loss: 1.3762392
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.6582088
	speed: 0.0202s/iter; left time: 31.2398s
	iters: 200, epoch: 3 | loss: 0.6153486
	speed: 0.0181s/iter; left time: 26.1677s
Epoch: 3 cost time: 3.767730712890625
Epoch: 3, Steps: 206 | Train Loss: 0.7082194 Vali Loss: 0.7187491 Test Loss: 1.4424276
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.6453457
	speed: 0.0157s/iter; left time: 21.1467s
	iters: 200, epoch: 4 | loss: 0.6717581
	speed: 0.0140s/iter; left time: 17.3576s
Epoch: 4 cost time: 2.9411683082580566
Epoch: 4, Steps: 206 | Train Loss: 0.6572237 Vali Loss: 0.7423714 Test Loss: 1.4737930
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.6450907
	speed: 0.0174s/iter; left time: 19.7688s
	iters: 200, epoch: 5 | loss: 0.6754090
	speed: 0.0166s/iter; left time: 17.2012s
Epoch: 5 cost time: 3.476034641265869
Epoch: 5, Steps: 206 | Train Loss: 0.6292870 Vali Loss: 0.7435905 Test Loss: 1.5107232
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.5280716
	speed: 0.0210s/iter; left time: 19.5447s
	iters: 200, epoch: 6 | loss: 0.6463794
	speed: 0.0198s/iter; left time: 16.4453s
Epoch: 6 cost time: 4.128978490829468
Epoch: 6, Steps: 206 | Train Loss: 0.6156658 Vali Loss: 0.7285035 Test Loss: 1.5086586
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.8_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.2910860776901245, mae:0.8857553005218506
Seed: 42
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.95_alpha0.8_96_720Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.95_alpha0.8.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           720                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.8_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.1053358
	speed: 0.0272s/iter; left time: 50.0682s
Epoch: 1 cost time: 4.011959075927734
Epoch: 1, Steps: 194 | Train Loss: 1.2154237 Vali Loss: 0.6261477 Test Loss: 1.4671183
Validation loss decreased (inf --> 0.626148).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0828047
	speed: 0.0221s/iter; left time: 36.3383s
Epoch: 2 cost time: 3.6745617389678955
Epoch: 2, Steps: 194 | Train Loss: 1.1599013 Vali Loss: 0.7502125 Test Loss: 1.4790720
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.9092062
	speed: 0.0167s/iter; left time: 24.2378s
Epoch: 3 cost time: 2.8747522830963135
Epoch: 3, Steps: 194 | Train Loss: 1.0528339 Vali Loss: 0.9389813 Test Loss: 1.5903887
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0140340
	speed: 0.0266s/iter; left time: 33.4376s
Epoch: 4 cost time: 3.93855619430542
Epoch: 4, Steps: 194 | Train Loss: 0.9640813 Vali Loss: 1.0135977 Test Loss: 1.6811467
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.9415487
	speed: 0.0183s/iter; left time: 19.4885s
Epoch: 5 cost time: 3.430763006210327
Epoch: 5, Steps: 194 | Train Loss: 0.9251276 Vali Loss: 1.0779496 Test Loss: 1.6154102
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.9740531
	speed: 0.0223s/iter; left time: 19.4115s
Epoch: 6 cost time: 3.774129867553711
Epoch: 6, Steps: 194 | Train Loss: 0.8974373 Vali Loss: 1.0950434 Test Loss: 1.6276381
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.8_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.467118263244629, mae:0.9474582672119141
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.8_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.2230535
	speed: 0.0217s/iter; left time: 40.0037s
Epoch: 1 cost time: 3.5220413208007812
Epoch: 1, Steps: 194 | Train Loss: 1.2134631 Vali Loss: 0.5985429 Test Loss: 1.4310187
Validation loss decreased (inf --> 0.598543).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.1857555
	speed: 0.0184s/iter; left time: 30.2852s
Epoch: 2 cost time: 3.1006836891174316
Epoch: 2, Steps: 194 | Train Loss: 1.1436447 Vali Loss: 0.7382419 Test Loss: 1.4067423
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.9850770
	speed: 0.0206s/iter; left time: 29.9152s
Epoch: 3 cost time: 3.367046594619751
Epoch: 3, Steps: 194 | Train Loss: 1.0349158 Vali Loss: 0.8098016 Test Loss: 1.3327602
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0257905
	speed: 0.0170s/iter; left time: 21.3926s
Epoch: 4 cost time: 3.044163703918457
Epoch: 4, Steps: 194 | Train Loss: 0.9510389 Vali Loss: 0.9010842 Test Loss: 1.4193113
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.8972607
	speed: 0.0169s/iter; left time: 18.0272s
Epoch: 5 cost time: 2.8918211460113525
Epoch: 5, Steps: 194 | Train Loss: 0.9117274 Vali Loss: 0.9122398 Test Loss: 1.4118659
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.8386258
	speed: 0.0179s/iter; left time: 15.5957s
Epoch: 6 cost time: 2.955805540084839
Epoch: 6, Steps: 194 | Train Loss: 0.8954742 Vali Loss: 0.9134152 Test Loss: 1.4096642
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.8_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.4310188293457031, mae:0.9356032013893127
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.8_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.9118600
	speed: 0.0252s/iter; left time: 46.4533s
Epoch: 1 cost time: 3.959282159805298
Epoch: 1, Steps: 194 | Train Loss: 1.2132465 Vali Loss: 0.6131009 Test Loss: 1.4297191
Validation loss decreased (inf --> 0.613101).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.1250244
	speed: 0.0230s/iter; left time: 37.8652s
Epoch: 2 cost time: 4.11626672744751
Epoch: 2, Steps: 194 | Train Loss: 1.1306937 Vali Loss: 0.7072988 Test Loss: 1.3790932
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0950953
	speed: 0.0274s/iter; left time: 39.8438s
Epoch: 3 cost time: 4.062020778656006
Epoch: 3, Steps: 194 | Train Loss: 0.9962846 Vali Loss: 0.9606411 Test Loss: 1.4371403
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.8621830
	speed: 0.0203s/iter; left time: 25.5050s
Epoch: 4 cost time: 3.1756203174591064
Epoch: 4, Steps: 194 | Train Loss: 0.9070915 Vali Loss: 0.9568269 Test Loss: 1.4566892
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.7955665
	speed: 0.0199s/iter; left time: 21.2449s
Epoch: 5 cost time: 3.895740032196045
Epoch: 5, Steps: 194 | Train Loss: 0.8637738 Vali Loss: 1.0276688 Test Loss: 1.4798805
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0130961
	speed: 0.0201s/iter; left time: 17.4704s
Epoch: 6 cost time: 3.5552852153778076
Epoch: 6, Steps: 194 | Train Loss: 0.8444224 Vali Loss: 1.0277256 Test Loss: 1.4957582
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.8_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_42<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.4297189712524414, mae:0.9371098279953003
Seed: 123
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.95_alpha0.8_96_96Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.95_alpha0.8.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           96                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.8_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.4519632
	speed: 0.0217s/iter; left time: 44.0128s
	iters: 200, epoch: 1 | loss: 0.5349725
	speed: 0.0197s/iter; left time: 38.0030s
Epoch: 1 cost time: 4.142176866531372
Epoch: 1, Steps: 213 | Train Loss: 0.5261703 Vali Loss: 0.4965929 Test Loss: 0.5888888
Validation loss decreased (inf --> 0.496593).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.4490753
	speed: 0.0173s/iter; left time: 31.5200s
	iters: 200, epoch: 2 | loss: 0.5221471
	speed: 0.0164s/iter; left time: 28.1344s
Epoch: 2 cost time: 3.520094871520996
Epoch: 2, Steps: 213 | Train Loss: 0.4921138 Vali Loss: 0.4909942 Test Loss: 0.5900854
Validation loss decreased (0.496593 --> 0.490994).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.4201205
	speed: 0.0157s/iter; left time: 25.1506s
	iters: 200, epoch: 3 | loss: 0.4307131
	speed: 0.0157s/iter; left time: 23.6889s
Epoch: 3 cost time: 3.3652548789978027
Epoch: 3, Steps: 213 | Train Loss: 0.4623886 Vali Loss: 0.5000588 Test Loss: 0.6269910
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.5032291
	speed: 0.0144s/iter; left time: 20.0565s
	iters: 200, epoch: 4 | loss: 0.4689147
	speed: 0.0128s/iter; left time: 16.5602s
Epoch: 4 cost time: 2.732053756713867
Epoch: 4, Steps: 213 | Train Loss: 0.4419855 Vali Loss: 0.5160254 Test Loss: 0.6310156
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.4159569
	speed: 0.0162s/iter; left time: 19.1313s
	iters: 200, epoch: 5 | loss: 0.4411635
	speed: 0.0176s/iter; left time: 18.9730s
Epoch: 5 cost time: 3.823087215423584
Epoch: 5, Steps: 213 | Train Loss: 0.4311772 Vali Loss: 0.5082644 Test Loss: 0.6317500
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.4142814
	speed: 0.0186s/iter; left time: 17.9223s
	iters: 200, epoch: 6 | loss: 0.4606194
	speed: 0.0166s/iter; left time: 14.3681s
Epoch: 6 cost time: 3.597776412963867
Epoch: 6, Steps: 213 | Train Loss: 0.4265960 Vali Loss: 0.5057750 Test Loss: 0.6372283
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.3962647
	speed: 0.0201s/iter; left time: 15.1424s
	iters: 200, epoch: 7 | loss: 0.4379100
	speed: 0.0186s/iter; left time: 12.1451s
Epoch: 7 cost time: 3.9694173336029053
Epoch: 7, Steps: 213 | Train Loss: 0.4227503 Vali Loss: 0.5087428 Test Loss: 0.6351347
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.8_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.5900853872299194, mae:0.6068108081817627
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.8_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.5235595
	speed: 0.0127s/iter; left time: 25.8840s
	iters: 200, epoch: 1 | loss: 0.4575996
	speed: 0.0117s/iter; left time: 22.5738s
Epoch: 1 cost time: 2.565607786178589
Epoch: 1, Steps: 213 | Train Loss: 0.5261939 Vali Loss: 0.4906426 Test Loss: 0.6073741
Validation loss decreased (inf --> 0.490643).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.4955015
	speed: 0.0188s/iter; left time: 34.1966s
	iters: 200, epoch: 2 | loss: 0.4335836
	speed: 0.0162s/iter; left time: 27.8930s
Epoch: 2 cost time: 3.564668655395508
Epoch: 2, Steps: 213 | Train Loss: 0.4917145 Vali Loss: 0.5109125 Test Loss: 0.5999144
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.4448946
	speed: 0.0163s/iter; left time: 26.2313s
	iters: 200, epoch: 3 | loss: 0.4659944
	speed: 0.0145s/iter; left time: 21.8242s
Epoch: 3 cost time: 3.1833083629608154
Epoch: 3, Steps: 213 | Train Loss: 0.4627889 Vali Loss: 0.5035315 Test Loss: 0.6413649
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.4101461
	speed: 0.0175s/iter; left time: 24.3585s
	iters: 200, epoch: 4 | loss: 0.4621113
	speed: 0.0155s/iter; left time: 20.0343s
Epoch: 4 cost time: 3.424262523651123
Epoch: 4, Steps: 213 | Train Loss: 0.4402850 Vali Loss: 0.5176655 Test Loss: 0.6386531
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.4336186
	speed: 0.0142s/iter; left time: 16.7868s
	iters: 200, epoch: 5 | loss: 0.4048257
	speed: 0.0156s/iter; left time: 16.8857s
Epoch: 5 cost time: 3.3562002182006836
Epoch: 5, Steps: 213 | Train Loss: 0.4284594 Vali Loss: 0.5185527 Test Loss: 0.6480959
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.4375374
	speed: 0.0235s/iter; left time: 22.7013s
	iters: 200, epoch: 6 | loss: 0.4107561
	speed: 0.0212s/iter; left time: 18.3417s
Epoch: 6 cost time: 4.501188516616821
Epoch: 6, Steps: 213 | Train Loss: 0.4235694 Vali Loss: 0.5197758 Test Loss: 0.6454142
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.8_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.6073741316795349, mae:0.6157886981964111
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.8_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.5829042
	speed: 0.0218s/iter; left time: 44.2363s
	iters: 200, epoch: 1 | loss: 0.5294569
	speed: 0.0189s/iter; left time: 36.5824s
Epoch: 1 cost time: 4.090484142303467
Epoch: 1, Steps: 213 | Train Loss: 0.5260123 Vali Loss: 0.4894477 Test Loss: 0.5975252
Validation loss decreased (inf --> 0.489448).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.5179035
	speed: 0.0149s/iter; left time: 27.0896s
	iters: 200, epoch: 2 | loss: 0.4900982
	speed: 0.0139s/iter; left time: 23.8749s
Epoch: 2 cost time: 3.079812526702881
Epoch: 2, Steps: 213 | Train Loss: 0.4904113 Vali Loss: 0.4759701 Test Loss: 0.5996140
Validation loss decreased (0.489448 --> 0.475970).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.5110221
	speed: 0.0153s/iter; left time: 24.6055s
	iters: 200, epoch: 3 | loss: 0.4044732
	speed: 0.0145s/iter; left time: 21.8282s
Epoch: 3 cost time: 3.1468253135681152
Epoch: 3, Steps: 213 | Train Loss: 0.4635996 Vali Loss: 0.5024623 Test Loss: 0.6140104
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.4421597
	speed: 0.0156s/iter; left time: 21.7419s
	iters: 200, epoch: 4 | loss: 0.4452839
	speed: 0.0169s/iter; left time: 21.8841s
Epoch: 4 cost time: 3.6203293800354004
Epoch: 4, Steps: 213 | Train Loss: 0.4473268 Vali Loss: 0.5132846 Test Loss: 0.6614868
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.4831703
	speed: 0.0167s/iter; left time: 19.6530s
	iters: 200, epoch: 5 | loss: 0.4486870
	speed: 0.0167s/iter; left time: 18.0550s
Epoch: 5 cost time: 3.853381872177124
Epoch: 5, Steps: 213 | Train Loss: 0.4381493 Vali Loss: 0.5172286 Test Loss: 0.6501934
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.4869328
	speed: 0.0170s/iter; left time: 16.4206s
	iters: 200, epoch: 6 | loss: 0.4345519
	speed: 0.0144s/iter; left time: 12.4768s
Epoch: 6 cost time: 3.182666301727295
Epoch: 6, Steps: 213 | Train Loss: 0.4338399 Vali Loss: 0.5280433 Test Loss: 0.6521698
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.4265483
	speed: 0.0167s/iter; left time: 12.5535s
	iters: 200, epoch: 7 | loss: 0.3979368
	speed: 0.0169s/iter; left time: 11.0284s
Epoch: 7 cost time: 3.752662181854248
Epoch: 7, Steps: 213 | Train Loss: 0.4313800 Vali Loss: 0.5159819 Test Loss: 0.6493916
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.8_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.5996138453483582, mae:0.6116501092910767
Seed: 123
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.95_alpha0.8_96_192Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.95_alpha0.8.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           192                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.8_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.6102238
	speed: 0.0252s/iter; left time: 50.3708s
	iters: 200, epoch: 1 | loss: 0.6101718
	speed: 0.0196s/iter; left time: 37.2588s
Epoch: 1 cost time: 4.168823480606079
Epoch: 1, Steps: 210 | Train Loss: 0.6554958 Vali Loss: 0.6079190 Test Loss: 0.9169335
Validation loss decreased (inf --> 0.607919).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.6976652
	speed: 0.0280s/iter; left time: 50.0863s
	iters: 200, epoch: 2 | loss: 0.5836673
	speed: 0.0210s/iter; left time: 35.4759s
Epoch: 2 cost time: 4.360474109649658
Epoch: 2, Steps: 210 | Train Loss: 0.6086549 Vali Loss: 0.6351731 Test Loss: 0.9342621
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.5519475
	speed: 0.0278s/iter; left time: 43.9380s
	iters: 200, epoch: 3 | loss: 0.5097771
	speed: 0.0216s/iter; left time: 31.9429s
Epoch: 3 cost time: 4.532931566238403
Epoch: 3, Steps: 210 | Train Loss: 0.5540801 Vali Loss: 0.6543715 Test Loss: 0.9483067
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.5662909
	speed: 0.0139s/iter; left time: 19.1023s
	iters: 200, epoch: 4 | loss: 0.5108813
	speed: 0.0115s/iter; left time: 14.6241s
Epoch: 4 cost time: 2.508394956588745
Epoch: 4, Steps: 210 | Train Loss: 0.5196500 Vali Loss: 0.6369334 Test Loss: 0.9577702
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.5010594
	speed: 0.0194s/iter; left time: 22.5066s
	iters: 200, epoch: 5 | loss: 0.5115344
	speed: 0.0164s/iter; left time: 17.3520s
Epoch: 5 cost time: 3.4816038608551025
Epoch: 5, Steps: 210 | Train Loss: 0.5048686 Vali Loss: 0.6571761 Test Loss: 0.9631680
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.4628634
	speed: 0.0187s/iter; left time: 17.8132s
	iters: 200, epoch: 6 | loss: 0.5265046
	speed: 0.0173s/iter; left time: 14.7083s
Epoch: 6 cost time: 3.7159245014190674
Epoch: 6, Steps: 210 | Train Loss: 0.4974597 Vali Loss: 0.6625795 Test Loss: 0.9607158
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.8_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.9169335961341858, mae:0.7497053742408752
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.8_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.6984103
	speed: 0.0244s/iter; left time: 48.7727s
	iters: 200, epoch: 1 | loss: 0.5798241
	speed: 0.0195s/iter; left time: 37.0127s
Epoch: 1 cost time: 4.072458744049072
Epoch: 1, Steps: 210 | Train Loss: 0.6595970 Vali Loss: 0.5942299 Test Loss: 0.9280542
Validation loss decreased (inf --> 0.594230).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.6069378
	speed: 0.0182s/iter; left time: 32.5921s
	iters: 200, epoch: 2 | loss: 0.6312922
	speed: 0.0136s/iter; left time: 23.0813s
Epoch: 2 cost time: 2.8649983406066895
Epoch: 2, Steps: 210 | Train Loss: 0.6158830 Vali Loss: 0.6213136 Test Loss: 1.0251546
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.6280822
	speed: 0.0171s/iter; left time: 26.9939s
	iters: 200, epoch: 3 | loss: 0.5391032
	speed: 0.0137s/iter; left time: 20.2166s
Epoch: 3 cost time: 3.041311502456665
Epoch: 3, Steps: 210 | Train Loss: 0.5805920 Vali Loss: 0.6201592 Test Loss: 1.0287347
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.6373653
	speed: 0.0157s/iter; left time: 21.5878s
	iters: 200, epoch: 4 | loss: 0.5342643
	speed: 0.0141s/iter; left time: 17.8947s
Epoch: 4 cost time: 3.0512678623199463
Epoch: 4, Steps: 210 | Train Loss: 0.5559327 Vali Loss: 0.6591624 Test Loss: 0.9727830
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.5013762
	speed: 0.0172s/iter; left time: 19.9511s
	iters: 200, epoch: 5 | loss: 0.5136750
	speed: 0.0160s/iter; left time: 17.0186s
Epoch: 5 cost time: 3.4491469860076904
Epoch: 5, Steps: 210 | Train Loss: 0.5377450 Vali Loss: 0.6545305 Test Loss: 0.9676330
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.5853624
	speed: 0.0164s/iter; left time: 15.5585s
	iters: 200, epoch: 6 | loss: 0.5547003
	speed: 0.0136s/iter; left time: 11.5860s
Epoch: 6 cost time: 2.9284634590148926
Epoch: 6, Steps: 210 | Train Loss: 0.5293232 Vali Loss: 0.6630977 Test Loss: 0.9855583
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.8_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.9280540943145752, mae:0.7551310658454895
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.8_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.6602213
	speed: 0.0143s/iter; left time: 28.6462s
	iters: 200, epoch: 1 | loss: 0.6284058
	speed: 0.0139s/iter; left time: 26.4130s
Epoch: 1 cost time: 3.042141914367676
Epoch: 1, Steps: 210 | Train Loss: 0.6555160 Vali Loss: 0.5864348 Test Loss: 0.9203593
Validation loss decreased (inf --> 0.586435).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.6123083
	speed: 0.0137s/iter; left time: 24.5994s
	iters: 200, epoch: 2 | loss: 0.6692928
	speed: 0.0137s/iter; left time: 23.1391s
Epoch: 2 cost time: 3.027451515197754
Epoch: 2, Steps: 210 | Train Loss: 0.6119593 Vali Loss: 0.5765485 Test Loss: 0.9818044
Validation loss decreased (0.586435 --> 0.576548).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.6201547
	speed: 0.0157s/iter; left time: 24.8960s
	iters: 200, epoch: 3 | loss: 0.5800829
	speed: 0.0145s/iter; left time: 21.4933s
Epoch: 3 cost time: 3.1555023193359375
Epoch: 3, Steps: 210 | Train Loss: 0.5664139 Vali Loss: 0.6210058 Test Loss: 0.9782898
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.4691382
	speed: 0.0184s/iter; left time: 25.2881s
	iters: 200, epoch: 4 | loss: 0.5711508
	speed: 0.0167s/iter; left time: 21.1750s
Epoch: 4 cost time: 3.5098953247070312
Epoch: 4, Steps: 210 | Train Loss: 0.5323125 Vali Loss: 0.6336046 Test Loss: 0.9327461
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.4720727
	speed: 0.0159s/iter; left time: 18.4457s
	iters: 200, epoch: 5 | loss: 0.5276880
	speed: 0.0153s/iter; left time: 16.2515s
Epoch: 5 cost time: 3.3107972145080566
Epoch: 5, Steps: 210 | Train Loss: 0.5158062 Vali Loss: 0.6509383 Test Loss: 0.9643243
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.5781450
	speed: 0.0199s/iter; left time: 18.9454s
	iters: 200, epoch: 6 | loss: 0.4885124
	speed: 0.0161s/iter; left time: 13.6878s
Epoch: 6 cost time: 3.5499496459960938
Epoch: 6, Steps: 210 | Train Loss: 0.5082591 Vali Loss: 0.6625388 Test Loss: 0.9553851
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.4838046
	speed: 0.0175s/iter; left time: 12.9787s
	iters: 200, epoch: 7 | loss: 0.5049950
	speed: 0.0157s/iter; left time: 10.0524s
Epoch: 7 cost time: 3.3859527111053467
Epoch: 7, Steps: 210 | Train Loss: 0.5055733 Vali Loss: 0.6552334 Test Loss: 0.9468616
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.8_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.9818044304847717, mae:0.7780161499977112
Seed: 123
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.95_alpha0.8_96_336Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.95_alpha0.8.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           336                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.8_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.8544901
	speed: 0.0413s/iter; left time: 81.0089s
	iters: 200, epoch: 1 | loss: 0.9211231
	speed: 0.0266s/iter; left time: 49.5463s
Epoch: 1 cost time: 5.557627439498901
Epoch: 1, Steps: 206 | Train Loss: 0.8330641 Vali Loss: 0.6598601 Test Loss: 1.2907006
Validation loss decreased (inf --> 0.659860).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.6959175
	speed: 0.0175s/iter; left time: 30.7587s
	iters: 200, epoch: 2 | loss: 0.7219604
	speed: 0.0150s/iter; left time: 24.8354s
Epoch: 2 cost time: 3.1323366165161133
Epoch: 2, Steps: 206 | Train Loss: 0.7756297 Vali Loss: 0.6995526 Test Loss: 1.3573136
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.6146026
	speed: 0.0210s/iter; left time: 32.4684s
	iters: 200, epoch: 3 | loss: 0.6942749
	speed: 0.0168s/iter; left time: 24.3724s
Epoch: 3 cost time: 3.5683460235595703
Epoch: 3, Steps: 206 | Train Loss: 0.7077379 Vali Loss: 0.6679566 Test Loss: 1.4528226
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.6796834
	speed: 0.0294s/iter; left time: 39.4267s
	iters: 200, epoch: 4 | loss: 0.6664915
	speed: 0.0218s/iter; left time: 27.0786s
Epoch: 4 cost time: 4.517842769622803
Epoch: 4, Steps: 206 | Train Loss: 0.6582965 Vali Loss: 0.6855474 Test Loss: 1.4802730
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.5737364
	speed: 0.0203s/iter; left time: 23.0678s
	iters: 200, epoch: 5 | loss: 0.6328347
	speed: 0.0146s/iter; left time: 15.1189s
Epoch: 5 cost time: 3.064145565032959
Epoch: 5, Steps: 206 | Train Loss: 0.6318434 Vali Loss: 0.6792310 Test Loss: 1.5451075
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.5919710
	speed: 0.0171s/iter; left time: 15.9049s
	iters: 200, epoch: 6 | loss: 0.7106383
	speed: 0.0134s/iter; left time: 11.1041s
Epoch: 6 cost time: 2.825674295425415
Epoch: 6, Steps: 206 | Train Loss: 0.6191402 Vali Loss: 0.6719933 Test Loss: 1.5466783
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.8_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.2907004356384277, mae:0.8862679600715637
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.8_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.8870325
	speed: 0.0153s/iter; left time: 29.9318s
	iters: 200, epoch: 1 | loss: 0.8696541
	speed: 0.0146s/iter; left time: 27.1830s
Epoch: 1 cost time: 3.0878024101257324
Epoch: 1, Steps: 206 | Train Loss: 0.8331516 Vali Loss: 0.6622506 Test Loss: 1.3357840
Validation loss decreased (inf --> 0.662251).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.8202142
	speed: 0.0156s/iter; left time: 27.3666s
	iters: 200, epoch: 2 | loss: 0.7232551
	speed: 0.0158s/iter; left time: 26.1958s
Epoch: 2 cost time: 3.4105165004730225
Epoch: 2, Steps: 206 | Train Loss: 0.7850133 Vali Loss: 0.6389039 Test Loss: 1.3799853
Validation loss decreased (0.662251 --> 0.638904).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.7063304
	speed: 0.0194s/iter; left time: 30.0877s
	iters: 200, epoch: 3 | loss: 0.8992181
	speed: 0.0159s/iter; left time: 22.9972s
Epoch: 3 cost time: 3.3023600578308105
Epoch: 3, Steps: 206 | Train Loss: 0.7269015 Vali Loss: 0.6564062 Test Loss: 1.4419332
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.6059489
	speed: 0.0154s/iter; left time: 20.6348s
	iters: 200, epoch: 4 | loss: 0.7410238
	speed: 0.0129s/iter; left time: 15.9909s
Epoch: 4 cost time: 2.683664321899414
Epoch: 4, Steps: 206 | Train Loss: 0.6764691 Vali Loss: 0.6419520 Test Loss: 1.3789631
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.6030014
	speed: 0.0194s/iter; left time: 22.0398s
	iters: 200, epoch: 5 | loss: 0.5698633
	speed: 0.0173s/iter; left time: 17.8891s
Epoch: 5 cost time: 3.6050500869750977
Epoch: 5, Steps: 206 | Train Loss: 0.6526902 Vali Loss: 0.6509285 Test Loss: 1.4226640
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.5824801
	speed: 0.0186s/iter; left time: 17.3631s
	iters: 200, epoch: 6 | loss: 0.5884469
	speed: 0.0170s/iter; left time: 14.1571s
Epoch: 6 cost time: 3.5789690017700195
Epoch: 6, Steps: 206 | Train Loss: 0.6409875 Vali Loss: 0.6554343 Test Loss: 1.4304782
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.5779315
	speed: 0.0203s/iter; left time: 14.6886s
	iters: 200, epoch: 7 | loss: 0.6189606
	speed: 0.0182s/iter; left time: 11.3479s
Epoch: 7 cost time: 3.8603320121765137
Epoch: 7, Steps: 206 | Train Loss: 0.6352174 Vali Loss: 0.6597944 Test Loss: 1.4333615
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.8_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.3799853324890137, mae:0.923566460609436
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.8_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.7860159
	speed: 0.0131s/iter; left time: 25.7566s
	iters: 200, epoch: 1 | loss: 0.7416900
	speed: 0.0129s/iter; left time: 23.9912s
Epoch: 1 cost time: 2.7708992958068848
Epoch: 1, Steps: 206 | Train Loss: 0.8342283 Vali Loss: 0.6977967 Test Loss: 1.3140031
Validation loss decreased (inf --> 0.697797).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.8534426
	speed: 0.0195s/iter; left time: 34.1505s
	iters: 200, epoch: 2 | loss: 0.7197360
	speed: 0.0170s/iter; left time: 28.1350s
Epoch: 2 cost time: 3.635267734527588
Epoch: 2, Steps: 206 | Train Loss: 0.7717526 Vali Loss: 0.6695304 Test Loss: 1.3772105
Validation loss decreased (0.697797 --> 0.669530).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.7150133
	speed: 0.0155s/iter; left time: 24.0677s
	iters: 200, epoch: 3 | loss: 0.6395052
	speed: 0.0150s/iter; left time: 21.6800s
Epoch: 3 cost time: 3.2068400382995605
Epoch: 3, Steps: 206 | Train Loss: 0.6857703 Vali Loss: 0.6340812 Test Loss: 1.4491626
Validation loss decreased (0.669530 --> 0.634081).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.6571855
	speed: 0.0182s/iter; left time: 24.4034s
	iters: 200, epoch: 4 | loss: 0.6492170
	speed: 0.0166s/iter; left time: 20.6334s
Epoch: 4 cost time: 3.5089643001556396
Epoch: 4, Steps: 206 | Train Loss: 0.6277839 Vali Loss: 0.6651999 Test Loss: 1.5700328
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.6290653
	speed: 0.0162s/iter; left time: 18.4502s
	iters: 200, epoch: 5 | loss: 0.6280252
	speed: 0.0127s/iter; left time: 13.1855s
Epoch: 5 cost time: 2.67549729347229
Epoch: 5, Steps: 206 | Train Loss: 0.6000690 Vali Loss: 0.6689801 Test Loss: 1.4980955
EarlyStopping counter: 2 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.5264663
	speed: 0.0171s/iter; left time: 15.9378s
	iters: 200, epoch: 6 | loss: 0.5871339
	speed: 0.0148s/iter; left time: 12.2574s
Epoch: 6 cost time: 3.145540475845337
Epoch: 6, Steps: 206 | Train Loss: 0.5868546 Vali Loss: 0.6584907 Test Loss: 1.5002747
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.6076015
	speed: 0.0182s/iter; left time: 13.1971s
	iters: 200, epoch: 7 | loss: 0.5534206
	speed: 0.0167s/iter; left time: 10.4181s
Epoch: 7 cost time: 3.5641493797302246
Epoch: 7, Steps: 206 | Train Loss: 0.5809239 Vali Loss: 0.6586650 Test Loss: 1.4804389
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.5162170
	speed: 0.0222s/iter; left time: 11.5047s
	iters: 200, epoch: 8 | loss: 0.5061019
	speed: 0.0182s/iter; left time: 7.6294s
Epoch: 8 cost time: 3.8524513244628906
Epoch: 8, Steps: 206 | Train Loss: 0.5810432 Vali Loss: 0.6546919 Test Loss: 1.4871335
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.8_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.4491627216339111, mae:0.9416054487228394
Seed: 123
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.95_alpha0.8_96_720Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.95_alpha0.8.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           720                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.8_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.1655657
	speed: 0.0284s/iter; left time: 52.3713s
Epoch: 1 cost time: 4.314977169036865
Epoch: 1, Steps: 194 | Train Loss: 1.2130927 Vali Loss: 0.6388034 Test Loss: 1.4720324
Validation loss decreased (inf --> 0.638803).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.3404405
	speed: 0.0257s/iter; left time: 42.3247s
Epoch: 2 cost time: 4.098812818527222
Epoch: 2, Steps: 194 | Train Loss: 1.1386065 Vali Loss: 0.6807822 Test Loss: 1.4324321
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.9991422
	speed: 0.0203s/iter; left time: 29.5508s
Epoch: 3 cost time: 3.1477506160736084
Epoch: 3, Steps: 194 | Train Loss: 1.0285931 Vali Loss: 0.7601311 Test Loss: 1.4618818
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.8588588
	speed: 0.0153s/iter; left time: 19.2907s
Epoch: 4 cost time: 2.7906928062438965
Epoch: 4, Steps: 194 | Train Loss: 0.9468888 Vali Loss: 0.8666861 Test Loss: 1.5156738
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.8376762
	speed: 0.0149s/iter; left time: 15.8246s
Epoch: 5 cost time: 2.8621978759765625
Epoch: 5, Steps: 194 | Train Loss: 0.8975311 Vali Loss: 0.9026785 Test Loss: 1.5531279
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.9854696
	speed: 0.0196s/iter; left time: 17.0516s
Epoch: 6 cost time: 3.3033957481384277
Epoch: 6, Steps: 194 | Train Loss: 0.8755829 Vali Loss: 0.8816357 Test Loss: 1.5707362
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.8_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.4720325469970703, mae:0.9494885206222534
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.8_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.2093573
	speed: 0.0185s/iter; left time: 33.9990s
Epoch: 1 cost time: 3.303900718688965
Epoch: 1, Steps: 194 | Train Loss: 1.2159304 Vali Loss: 0.6122048 Test Loss: 1.4325346
Validation loss decreased (inf --> 0.612205).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.2856958
	speed: 0.0245s/iter; left time: 40.3546s
Epoch: 2 cost time: 4.20418643951416
Epoch: 2, Steps: 194 | Train Loss: 1.1662615 Vali Loss: 0.7066085 Test Loss: 1.4087116
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.2178094
	speed: 0.0186s/iter; left time: 27.0143s
Epoch: 3 cost time: 3.0085182189941406
Epoch: 3, Steps: 194 | Train Loss: 1.0805611 Vali Loss: 0.9284930 Test Loss: 1.3920141
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 1.0090240
	speed: 0.0200s/iter; left time: 25.2290s
Epoch: 4 cost time: 3.455956220626831
Epoch: 4, Steps: 194 | Train Loss: 0.9795277 Vali Loss: 0.8811041 Test Loss: 1.5436958
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.8975268
	speed: 0.0193s/iter; left time: 20.5504s
Epoch: 5 cost time: 3.101621627807617
Epoch: 5, Steps: 194 | Train Loss: 0.9279784 Vali Loss: 0.9162241 Test Loss: 1.5153253
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.9678255
	speed: 0.0242s/iter; left time: 21.0923s
Epoch: 6 cost time: 3.6981732845306396
Epoch: 6, Steps: 194 | Train Loss: 0.9014843 Vali Loss: 0.9584891 Test Loss: 1.5916207
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.8_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.4325344562530518, mae:0.9385223984718323
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.8_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.1185418
	speed: 0.0136s/iter; left time: 25.0690s
Epoch: 1 cost time: 2.6651062965393066
Epoch: 1, Steps: 194 | Train Loss: 1.2168844 Vali Loss: 0.5972334 Test Loss: 1.4378599
Validation loss decreased (inf --> 0.597233).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.3280611
	speed: 0.0184s/iter; left time: 30.2944s
Epoch: 2 cost time: 3.3855960369110107
Epoch: 2, Steps: 194 | Train Loss: 1.1360340 Vali Loss: 0.6903527 Test Loss: 1.4260544
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.1876726
	speed: 0.0188s/iter; left time: 27.3582s
Epoch: 3 cost time: 3.5029075145721436
Epoch: 3, Steps: 194 | Train Loss: 1.0131848 Vali Loss: 0.7476142 Test Loss: 1.5225612
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.8304960
	speed: 0.0188s/iter; left time: 23.6114s
Epoch: 4 cost time: 3.4451076984405518
Epoch: 4, Steps: 194 | Train Loss: 0.9274149 Vali Loss: 0.7733212 Test Loss: 1.5053593
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.8749895
	speed: 0.0191s/iter; left time: 20.3655s
Epoch: 5 cost time: 3.0255649089813232
Epoch: 5, Steps: 194 | Train Loss: 0.8847890 Vali Loss: 0.7985637 Test Loss: 1.5332272
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.9712759
	speed: 0.0140s/iter; left time: 12.1624s
Epoch: 6 cost time: 2.447314500808716
Epoch: 6, Steps: 194 | Train Loss: 0.8652113 Vali Loss: 0.8094832 Test Loss: 1.5294656
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.8_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_123<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.4378597736358643, mae:0.9362199306488037
Seed: 456
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.95_alpha0.8_96_96Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.95_alpha0.8.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           96                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.8_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.5790750
	speed: 0.0337s/iter; left time: 68.5327s
	iters: 200, epoch: 1 | loss: 0.5160432
	speed: 0.0236s/iter; left time: 45.6477s
Epoch: 1 cost time: 5.007300138473511
Epoch: 1, Steps: 213 | Train Loss: 0.5300938 Vali Loss: 0.5011333 Test Loss: 0.5859633
Validation loss decreased (inf --> 0.501133).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.5351375
	speed: 0.0150s/iter; left time: 27.3600s
	iters: 200, epoch: 2 | loss: 0.5669762
	speed: 0.0134s/iter; left time: 23.0417s
Epoch: 2 cost time: 2.961442708969116
Epoch: 2, Steps: 213 | Train Loss: 0.4915275 Vali Loss: 0.5140625 Test Loss: 0.5791034
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.4917201
	speed: 0.0129s/iter; left time: 20.7319s
	iters: 200, epoch: 3 | loss: 0.4363666
	speed: 0.0124s/iter; left time: 18.6095s
Epoch: 3 cost time: 2.757037878036499
Epoch: 3, Steps: 213 | Train Loss: 0.4571804 Vali Loss: 0.5336645 Test Loss: 0.6418654
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.4533284
	speed: 0.0147s/iter; left time: 20.4700s
	iters: 200, epoch: 4 | loss: 0.3984848
	speed: 0.0132s/iter; left time: 17.0826s
Epoch: 4 cost time: 2.9418468475341797
Epoch: 4, Steps: 213 | Train Loss: 0.4370248 Vali Loss: 0.5134413 Test Loss: 0.6327523
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.4102819
	speed: 0.0195s/iter; left time: 22.9563s
	iters: 200, epoch: 5 | loss: 0.4371281
	speed: 0.0172s/iter; left time: 18.5788s
Epoch: 5 cost time: 3.7519805431365967
Epoch: 5, Steps: 213 | Train Loss: 0.4258638 Vali Loss: 0.5090134 Test Loss: 0.6300622
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.4351596
	speed: 0.0197s/iter; left time: 19.0670s
	iters: 200, epoch: 6 | loss: 0.4138503
	speed: 0.0142s/iter; left time: 12.2955s
Epoch: 6 cost time: 3.0277512073516846
Epoch: 6, Steps: 213 | Train Loss: 0.4209109 Vali Loss: 0.5127782 Test Loss: 0.6368647
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.8_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.5859631896018982, mae:0.6047728657722473
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.8_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.6369104
	speed: 0.0159s/iter; left time: 32.3004s
	iters: 200, epoch: 1 | loss: 0.4768127
	speed: 0.0162s/iter; left time: 31.2128s
Epoch: 1 cost time: 3.5674543380737305
Epoch: 1, Steps: 213 | Train Loss: 0.5274105 Vali Loss: 0.4892608 Test Loss: 0.5911511
Validation loss decreased (inf --> 0.489261).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.4488417
	speed: 0.0217s/iter; left time: 39.4375s
	iters: 200, epoch: 2 | loss: 0.4970481
	speed: 0.0200s/iter; left time: 34.4126s
Epoch: 2 cost time: 4.2760279178619385
Epoch: 2, Steps: 213 | Train Loss: 0.4910060 Vali Loss: 0.5369710 Test Loss: 0.6566271
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.4369406
	speed: 0.0198s/iter; left time: 31.7294s
	iters: 200, epoch: 3 | loss: 0.5333986
	speed: 0.0168s/iter; left time: 25.3229s
Epoch: 3 cost time: 3.633892774581909
Epoch: 3, Steps: 213 | Train Loss: 0.4623523 Vali Loss: 0.5198210 Test Loss: 0.6065891
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.4825690
	speed: 0.0158s/iter; left time: 21.9347s
	iters: 200, epoch: 4 | loss: 0.4220981
	speed: 0.0161s/iter; left time: 20.7930s
Epoch: 4 cost time: 3.5647470951080322
Epoch: 4, Steps: 213 | Train Loss: 0.4451727 Vali Loss: 0.5027766 Test Loss: 0.6445749
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.4815796
	speed: 0.0212s/iter; left time: 24.9701s
	iters: 200, epoch: 5 | loss: 0.4370039
	speed: 0.0194s/iter; left time: 20.9437s
Epoch: 5 cost time: 4.132533073425293
Epoch: 5, Steps: 213 | Train Loss: 0.4349249 Vali Loss: 0.4795587 Test Loss: 0.6352278
Validation loss decreased (0.489261 --> 0.479559).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.4268195
	speed: 0.0187s/iter; left time: 18.1009s
	iters: 200, epoch: 6 | loss: 0.3876567
	speed: 0.0175s/iter; left time: 15.1313s
Epoch: 6 cost time: 3.8336262702941895
Epoch: 6, Steps: 213 | Train Loss: 0.4295892 Vali Loss: 0.4929257 Test Loss: 0.6348640
EarlyStopping counter: 1 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.4621324
	speed: 0.0181s/iter; left time: 13.6239s
	iters: 200, epoch: 7 | loss: 0.4277372
	speed: 0.0149s/iter; left time: 9.7056s
Epoch: 7 cost time: 3.2535791397094727
Epoch: 7, Steps: 213 | Train Loss: 0.4273698 Vali Loss: 0.4970112 Test Loss: 0.6318830
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.3952889
	speed: 0.0140s/iter; left time: 7.5637s
	iters: 200, epoch: 8 | loss: 0.4328020
	speed: 0.0136s/iter; left time: 5.9674s
Epoch: 8 cost time: 3.0060672760009766
Epoch: 8, Steps: 213 | Train Loss: 0.4258082 Vali Loss: 0.4940253 Test Loss: 0.6340492
EarlyStopping counter: 3 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.4664287
	speed: 0.0185s/iter; left time: 6.0372s
	iters: 200, epoch: 9 | loss: 0.4152668
	speed: 0.0201s/iter; left time: 4.5611s
Epoch: 9 cost time: 4.45958685874939
Epoch: 9, Steps: 213 | Train Loss: 0.4249423 Vali Loss: 0.4926457 Test Loss: 0.6345450
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 0.4284844
	speed: 0.0177s/iter; left time: 2.0142s
	iters: 200, epoch: 10 | loss: 0.4171856
	speed: 0.0160s/iter; left time: 0.2241s
Epoch: 10 cost time: 3.4992048740386963
Epoch: 10, Steps: 213 | Train Loss: 0.4253082 Vali Loss: 0.4918865 Test Loss: 0.6339720
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.8_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.6352277994155884, mae:0.6321322321891785
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.8_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.4993900
	speed: 0.0151s/iter; left time: 30.7690s
	iters: 200, epoch: 1 | loss: 0.4972138
	speed: 0.0132s/iter; left time: 25.5010s
Epoch: 1 cost time: 2.893812656402588
Epoch: 1, Steps: 213 | Train Loss: 0.5258059 Vali Loss: 0.4966199 Test Loss: 0.5867736
Validation loss decreased (inf --> 0.496620).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.4738808
	speed: 0.0181s/iter; left time: 32.9311s
	iters: 200, epoch: 2 | loss: 0.4867534
	speed: 0.0159s/iter; left time: 27.3163s
Epoch: 2 cost time: 3.447305679321289
Epoch: 2, Steps: 213 | Train Loss: 0.4888818 Vali Loss: 0.4869736 Test Loss: 0.5984102
Validation loss decreased (0.496620 --> 0.486974).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.4134369
	speed: 0.0229s/iter; left time: 36.7785s
	iters: 200, epoch: 3 | loss: 0.4455398
	speed: 0.0189s/iter; left time: 28.4856s
Epoch: 3 cost time: 4.040613412857056
Epoch: 3, Steps: 213 | Train Loss: 0.4590104 Vali Loss: 0.4937169 Test Loss: 0.6159328
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.4327258
	speed: 0.0165s/iter; left time: 22.9068s
	iters: 200, epoch: 4 | loss: 0.3954872
	speed: 0.0140s/iter; left time: 18.0890s
Epoch: 4 cost time: 3.1852424144744873
Epoch: 4, Steps: 213 | Train Loss: 0.4388823 Vali Loss: 0.4902835 Test Loss: 0.6446688
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.3916161
	speed: 0.0175s/iter; left time: 20.6265s
	iters: 200, epoch: 5 | loss: 0.4227313
	speed: 0.0136s/iter; left time: 14.6523s
Epoch: 5 cost time: 2.9574103355407715
Epoch: 5, Steps: 213 | Train Loss: 0.4288957 Vali Loss: 0.5028294 Test Loss: 0.6402095
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.4268982
	speed: 0.0159s/iter; left time: 15.3971s
	iters: 200, epoch: 6 | loss: 0.4432501
	speed: 0.0140s/iter; left time: 12.1624s
Epoch: 6 cost time: 3.0070314407348633
Epoch: 6, Steps: 213 | Train Loss: 0.4242816 Vali Loss: 0.5072722 Test Loss: 0.6567374
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.4494851
	speed: 0.0140s/iter; left time: 10.5719s
	iters: 200, epoch: 7 | loss: 0.3902385
	speed: 0.0144s/iter; left time: 9.3979s
Epoch: 7 cost time: 3.10892391204834
Epoch: 7, Steps: 213 | Train Loss: 0.4203208 Vali Loss: 0.5008923 Test Loss: 0.6545565
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.8_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.5984101891517639, mae:0.611311674118042
Seed: 456
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.95_alpha0.8_96_192Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.95_alpha0.8.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           192                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.8_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.7864975
	speed: 0.0309s/iter; left time: 61.7345s
	iters: 200, epoch: 1 | loss: 0.5771133
	speed: 0.0215s/iter; left time: 40.8319s
Epoch: 1 cost time: 4.56240177154541
Epoch: 1, Steps: 210 | Train Loss: 0.6561751 Vali Loss: 0.5975295 Test Loss: 0.9261233
Validation loss decreased (inf --> 0.597529).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.6399482
	speed: 0.0216s/iter; left time: 38.7347s
	iters: 200, epoch: 2 | loss: 0.5560426
	speed: 0.0167s/iter; left time: 28.2484s
Epoch: 2 cost time: 3.5099074840545654
Epoch: 2, Steps: 210 | Train Loss: 0.6097090 Vali Loss: 0.6073852 Test Loss: 0.9124930
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.5193019
	speed: 0.0188s/iter; left time: 29.7186s
	iters: 200, epoch: 3 | loss: 0.5780683
	speed: 0.0166s/iter; left time: 24.5203s
Epoch: 3 cost time: 3.5581462383270264
Epoch: 3, Steps: 210 | Train Loss: 0.5639681 Vali Loss: 0.6474500 Test Loss: 0.9543691
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.5257682
	speed: 0.0171s/iter; left time: 23.4575s
	iters: 200, epoch: 4 | loss: 0.5591291
	speed: 0.0150s/iter; left time: 19.0937s
Epoch: 4 cost time: 3.354363441467285
Epoch: 4, Steps: 210 | Train Loss: 0.5321316 Vali Loss: 0.6637607 Test Loss: 0.9498008
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.4789054
	speed: 0.0257s/iter; left time: 29.7845s
	iters: 200, epoch: 5 | loss: 0.5470899
	speed: 0.0192s/iter; left time: 20.4020s
Epoch: 5 cost time: 4.087559700012207
Epoch: 5, Steps: 210 | Train Loss: 0.5140202 Vali Loss: 0.6651019 Test Loss: 0.9487985
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.4949004
	speed: 0.0182s/iter; left time: 17.2800s
	iters: 200, epoch: 6 | loss: 0.4823550
	speed: 0.0148s/iter; left time: 12.6065s
Epoch: 6 cost time: 3.149094581604004
Epoch: 6, Steps: 210 | Train Loss: 0.5052161 Vali Loss: 0.6697962 Test Loss: 0.9499044
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.8_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.926123321056366, mae:0.7553441524505615
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.8_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.5938948
	speed: 0.0156s/iter; left time: 31.1572s
	iters: 200, epoch: 1 | loss: 0.7636219
	speed: 0.0142s/iter; left time: 26.9982s
Epoch: 1 cost time: 3.0859787464141846
Epoch: 1, Steps: 210 | Train Loss: 0.6551056 Vali Loss: 0.6079525 Test Loss: 0.9242625
Validation loss decreased (inf --> 0.607953).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.6294963
	speed: 0.0214s/iter; left time: 38.3506s
	iters: 200, epoch: 2 | loss: 0.7231050
	speed: 0.0177s/iter; left time: 29.9399s
Epoch: 2 cost time: 3.779467821121216
Epoch: 2, Steps: 210 | Train Loss: 0.6128433 Vali Loss: 0.6276212 Test Loss: 0.9435747
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.6784837
	speed: 0.0206s/iter; left time: 32.5711s
	iters: 200, epoch: 3 | loss: 0.5348265
	speed: 0.0174s/iter; left time: 25.7345s
Epoch: 3 cost time: 3.6681063175201416
Epoch: 3, Steps: 210 | Train Loss: 0.5759572 Vali Loss: 0.6161999 Test Loss: 0.9896772
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.5910644
	speed: 0.0174s/iter; left time: 23.8395s
	iters: 200, epoch: 4 | loss: 0.6235307
	speed: 0.0156s/iter; left time: 19.8328s
Epoch: 4 cost time: 3.3697597980499268
Epoch: 4, Steps: 210 | Train Loss: 0.5439798 Vali Loss: 0.6484067 Test Loss: 0.9313433
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.5204257
	speed: 0.0159s/iter; left time: 18.4586s
	iters: 200, epoch: 5 | loss: 0.5741017
	speed: 0.0134s/iter; left time: 14.2535s
Epoch: 5 cost time: 2.912806749343872
Epoch: 5, Steps: 210 | Train Loss: 0.5272435 Vali Loss: 0.6569003 Test Loss: 0.9266863
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.5004392
	speed: 0.0195s/iter; left time: 18.5692s
	iters: 200, epoch: 6 | loss: 0.4580808
	speed: 0.0202s/iter; left time: 17.2036s
Epoch: 6 cost time: 4.302470922470093
Epoch: 6, Steps: 210 | Train Loss: 0.5181337 Vali Loss: 0.6592318 Test Loss: 0.9586482
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.8_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.9242624640464783, mae:0.7532749772071838
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.8_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.6408770
	speed: 0.0220s/iter; left time: 44.0577s
	iters: 200, epoch: 1 | loss: 0.6498274
	speed: 0.0189s/iter; left time: 35.8553s
Epoch: 1 cost time: 4.027475118637085
Epoch: 1, Steps: 210 | Train Loss: 0.6539838 Vali Loss: 0.6060745 Test Loss: 0.9484413
Validation loss decreased (inf --> 0.606074).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.6264848
	speed: 0.0177s/iter; left time: 31.7578s
	iters: 200, epoch: 2 | loss: 0.7180847
	speed: 0.0156s/iter; left time: 26.4632s
Epoch: 2 cost time: 3.3258750438690186
Epoch: 2, Steps: 210 | Train Loss: 0.6119003 Vali Loss: 0.6033965 Test Loss: 0.9412246
Validation loss decreased (0.606074 --> 0.603396).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.5257773
	speed: 0.0147s/iter; left time: 23.2979s
	iters: 200, epoch: 3 | loss: 0.5840202
	speed: 0.0116s/iter; left time: 17.1693s
Epoch: 3 cost time: 2.486356019973755
Epoch: 3, Steps: 210 | Train Loss: 0.5723504 Vali Loss: 0.6440627 Test Loss: 0.9691386
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.4935291
	speed: 0.0177s/iter; left time: 24.2801s
	iters: 200, epoch: 4 | loss: 0.5798423
	speed: 0.0154s/iter; left time: 19.5180s
Epoch: 4 cost time: 3.255634307861328
Epoch: 4, Steps: 210 | Train Loss: 0.5409251 Vali Loss: 0.6329612 Test Loss: 0.9379949
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.4829524
	speed: 0.0175s/iter; left time: 20.2938s
	iters: 200, epoch: 5 | loss: 0.4788277
	speed: 0.0152s/iter; left time: 16.1736s
Epoch: 5 cost time: 3.2161757946014404
Epoch: 5, Steps: 210 | Train Loss: 0.5208646 Vali Loss: 0.6440524 Test Loss: 0.9542485
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.4755151
	speed: 0.0180s/iter; left time: 17.1510s
	iters: 200, epoch: 6 | loss: 0.5167670
	speed: 0.0164s/iter; left time: 13.9506s
Epoch: 6 cost time: 3.5609054565429688
Epoch: 6, Steps: 210 | Train Loss: 0.5098495 Vali Loss: 0.6568339 Test Loss: 0.9533998
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.5014073
	speed: 0.0170s/iter; left time: 12.5744s
	iters: 200, epoch: 7 | loss: 0.5299797
	speed: 0.0139s/iter; left time: 8.9062s
Epoch: 7 cost time: 2.9432761669158936
Epoch: 7, Steps: 210 | Train Loss: 0.5049055 Vali Loss: 0.6508490 Test Loss: 0.9604741
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.8_96_192_iTransformer_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:0.9412245750427246, mae:0.7615864276885986
Seed: 456
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.95_alpha0.8_96_336Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.95_alpha0.8.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           336                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.8_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.8120434
	speed: 0.0299s/iter; left time: 58.5578s
	iters: 200, epoch: 1 | loss: 0.8659810
	speed: 0.0235s/iter; left time: 43.8088s
Epoch: 1 cost time: 4.86972451210022
Epoch: 1, Steps: 206 | Train Loss: 0.8315312 Vali Loss: 0.6572956 Test Loss: 1.3836526
Validation loss decreased (inf --> 0.657296).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.7697393
	speed: 0.0125s/iter; left time: 21.9807s
	iters: 200, epoch: 2 | loss: 0.8002750
	speed: 0.0113s/iter; left time: 18.6932s
Epoch: 2 cost time: 2.4471728801727295
Epoch: 2, Steps: 206 | Train Loss: 0.7749962 Vali Loss: 0.6376931 Test Loss: 1.3163571
Validation loss decreased (0.657296 --> 0.637693).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.8508241
	speed: 0.0162s/iter; left time: 25.1033s
	iters: 200, epoch: 3 | loss: 0.7466226
	speed: 0.0145s/iter; left time: 20.9601s
Epoch: 3 cost time: 3.086547613143921
Epoch: 3, Steps: 206 | Train Loss: 0.7169880 Vali Loss: 0.6577275 Test Loss: 1.3854104
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.6570488
	speed: 0.0199s/iter; left time: 26.6871s
	iters: 200, epoch: 4 | loss: 0.6976730
	speed: 0.0157s/iter; left time: 19.4805s
Epoch: 4 cost time: 3.2736971378326416
Epoch: 4, Steps: 206 | Train Loss: 0.6685398 Vali Loss: 0.6421386 Test Loss: 1.4649341
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.6048152
	speed: 0.0250s/iter; left time: 28.4091s
	iters: 200, epoch: 5 | loss: 0.6478142
	speed: 0.0193s/iter; left time: 19.9903s
Epoch: 5 cost time: 4.005332946777344
Epoch: 5, Steps: 206 | Train Loss: 0.6417130 Vali Loss: 0.6478344 Test Loss: 1.4711387
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.6894068
	speed: 0.0183s/iter; left time: 17.0431s
	iters: 200, epoch: 6 | loss: 0.5358858
	speed: 0.0154s/iter; left time: 12.8315s
Epoch: 6 cost time: 3.25016188621521
Epoch: 6, Steps: 206 | Train Loss: 0.6235771 Vali Loss: 0.6460407 Test Loss: 1.4959558
EarlyStopping counter: 4 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.6609729
	speed: 0.0175s/iter; left time: 12.6524s
	iters: 200, epoch: 7 | loss: 0.5434331
	speed: 0.0144s/iter; left time: 9.0155s
Epoch: 7 cost time: 3.065248966217041
Epoch: 7, Steps: 206 | Train Loss: 0.6190861 Vali Loss: 0.6505793 Test Loss: 1.4897891
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.8_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.3163573741912842, mae:0.8991645574569702
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.8_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.8709033
	speed: 0.0211s/iter; left time: 41.3097s
	iters: 200, epoch: 1 | loss: 0.7295582
	speed: 0.0171s/iter; left time: 31.9136s
Epoch: 1 cost time: 3.626315116882324
Epoch: 1, Steps: 206 | Train Loss: 0.8339949 Vali Loss: 0.6730853 Test Loss: 1.2997435
Validation loss decreased (inf --> 0.673085).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.7819586
	speed: 0.0171s/iter; left time: 30.0936s
	iters: 200, epoch: 2 | loss: 0.7513017
	speed: 0.0158s/iter; left time: 26.1639s
Epoch: 2 cost time: 3.3722660541534424
Epoch: 2, Steps: 206 | Train Loss: 0.7821477 Vali Loss: 0.6307527 Test Loss: 1.3415626
Validation loss decreased (0.673085 --> 0.630753).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.8212951
	speed: 0.0171s/iter; left time: 26.4841s
	iters: 200, epoch: 3 | loss: 0.7484607
	speed: 0.0152s/iter; left time: 22.0513s
Epoch: 3 cost time: 3.1788246631622314
Epoch: 3, Steps: 206 | Train Loss: 0.7282046 Vali Loss: 0.6099867 Test Loss: 1.3711658
Validation loss decreased (0.630753 --> 0.609987).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.7750671
	speed: 0.0136s/iter; left time: 18.2627s
	iters: 200, epoch: 4 | loss: 0.6304567
	speed: 0.0136s/iter; left time: 16.9121s
Epoch: 4 cost time: 2.8789217472076416
Epoch: 4, Steps: 206 | Train Loss: 0.6725223 Vali Loss: 0.6437095 Test Loss: 1.3889873
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.7325718
	speed: 0.0164s/iter; left time: 18.6534s
	iters: 200, epoch: 5 | loss: 0.5715142
	speed: 0.0177s/iter; left time: 18.3899s
Epoch: 5 cost time: 3.7540886402130127
Epoch: 5, Steps: 206 | Train Loss: 0.6435279 Vali Loss: 0.6341136 Test Loss: 1.3858962
EarlyStopping counter: 2 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.6809489
	speed: 0.0190s/iter; left time: 17.6743s
	iters: 200, epoch: 6 | loss: 0.6869081
	speed: 0.0167s/iter; left time: 13.8433s
Epoch: 6 cost time: 3.5540502071380615
Epoch: 6, Steps: 206 | Train Loss: 0.6286180 Vali Loss: 0.6286787 Test Loss: 1.3892140
EarlyStopping counter: 3 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.6114222
	speed: 0.0263s/iter; left time: 19.0764s
	iters: 200, epoch: 7 | loss: 0.6261239
	speed: 0.0183s/iter; left time: 11.4470s
Epoch: 7 cost time: 3.7989585399627686
Epoch: 7, Steps: 206 | Train Loss: 0.6199436 Vali Loss: 0.6272986 Test Loss: 1.3893782
EarlyStopping counter: 4 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.6514234
	speed: 0.0230s/iter; left time: 11.9521s
	iters: 200, epoch: 8 | loss: 0.6091029
	speed: 0.0178s/iter; left time: 7.4604s
Epoch: 8 cost time: 3.714055299758911
Epoch: 8, Steps: 206 | Train Loss: 0.6174330 Vali Loss: 0.6310483 Test Loss: 1.3947129
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.8_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.3711658716201782, mae:0.9222511053085327
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.8_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 0.7320063
	speed: 0.0185s/iter; left time: 36.2420s
	iters: 200, epoch: 1 | loss: 0.7466663
	speed: 0.0176s/iter; left time: 32.8192s
Epoch: 1 cost time: 3.6962592601776123
Epoch: 1, Steps: 206 | Train Loss: 0.8295878 Vali Loss: 0.7113650 Test Loss: 1.3171108
Validation loss decreased (inf --> 0.711365).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.8483127
	speed: 0.0163s/iter; left time: 28.5332s
	iters: 200, epoch: 2 | loss: 0.8040375
	speed: 0.0156s/iter; left time: 25.7377s
Epoch: 2 cost time: 3.3312876224517822
Epoch: 2, Steps: 206 | Train Loss: 0.7619919 Vali Loss: 0.7205325 Test Loss: 1.3517034
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.6085857
	speed: 0.0218s/iter; left time: 33.8141s
	iters: 200, epoch: 3 | loss: 0.6158759
	speed: 0.0172s/iter; left time: 24.8943s
Epoch: 3 cost time: 3.591529130935669
Epoch: 3, Steps: 206 | Train Loss: 0.6819717 Vali Loss: 0.6582293 Test Loss: 1.4738666
Validation loss decreased (0.711365 --> 0.658229).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.5921767
	speed: 0.0184s/iter; left time: 24.6753s
	iters: 200, epoch: 4 | loss: 0.6713783
	speed: 0.0162s/iter; left time: 20.1278s
Epoch: 4 cost time: 3.39273738861084
Epoch: 4, Steps: 206 | Train Loss: 0.6261322 Vali Loss: 0.6573148 Test Loss: 1.4803931
Validation loss decreased (0.658229 --> 0.657315).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.5990292
	speed: 0.0215s/iter; left time: 24.4610s
	iters: 200, epoch: 5 | loss: 0.5408789
	speed: 0.0171s/iter; left time: 17.6969s
Epoch: 5 cost time: 3.590273141860962
Epoch: 5, Steps: 206 | Train Loss: 0.5984904 Vali Loss: 0.6642694 Test Loss: 1.4916201
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.5927864
	speed: 0.0199s/iter; left time: 18.5464s
	iters: 200, epoch: 6 | loss: 0.5580150
	speed: 0.0174s/iter; left time: 14.5000s
Epoch: 6 cost time: 3.594170570373535
Epoch: 6, Steps: 206 | Train Loss: 0.5874293 Vali Loss: 0.6605506 Test Loss: 1.4934670
EarlyStopping counter: 2 out of 5
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.5968857
	speed: 0.0289s/iter; left time: 20.9664s
	iters: 200, epoch: 7 | loss: 0.6681880
	speed: 0.0212s/iter; left time: 13.2776s
Epoch: 7 cost time: 4.437166213989258
Epoch: 7, Steps: 206 | Train Loss: 0.5807050 Vali Loss: 0.6603301 Test Loss: 1.4891131
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.5622362
	speed: 0.0148s/iter; left time: 7.6939s
	iters: 200, epoch: 8 | loss: 0.5597677
	speed: 0.0114s/iter; left time: 4.7927s
Epoch: 8 cost time: 2.4372689723968506
Epoch: 8, Steps: 206 | Train Loss: 0.5769577 Vali Loss: 0.6598568 Test Loss: 1.4872737
EarlyStopping counter: 4 out of 5
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.5783501
	speed: 0.0258s/iter; left time: 8.0849s
	iters: 200, epoch: 9 | loss: 0.5984157
	speed: 0.0200s/iter; left time: 4.2706s
Epoch: 9 cost time: 4.2030861377716064
Epoch: 9, Steps: 206 | Train Loss: 0.5773644 Vali Loss: 0.6595688 Test Loss: 1.4875015
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.8_96_336_iTransformer_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.4803929328918457, mae:0.9454054236412048
Seed: 456
GPU available True
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           gamma0.95_alpha0.8_96_720Model:              iTransformer        

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/synthetic/
  Data Path:          gamma0.95_alpha0.8.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            96                  Label Len:          48                  
  Pred Len:           720                 Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             10                  Dec In:             10                  
  C Out:              10                  d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                3                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           5                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Task: long_term_forecast
Long Term Forecast
Exp: <class 'exp.exp_long_term_forecasting.Exp_Long_Term_Forecast'>
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.8_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.2196621
	speed: 0.0255s/iter; left time: 46.9476s
Epoch: 1 cost time: 3.661482810974121
Epoch: 1, Steps: 194 | Train Loss: 1.2118144 Vali Loss: 0.6460422 Test Loss: 1.4506618
Validation loss decreased (inf --> 0.646042).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.1414845
	speed: 0.0168s/iter; left time: 27.7271s
Epoch: 2 cost time: 2.7669146060943604
Epoch: 2, Steps: 194 | Train Loss: 1.1259126 Vali Loss: 0.9084933 Test Loss: 1.4510238
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.0619141
	speed: 0.0208s/iter; left time: 30.1649s
Epoch: 3 cost time: 3.4629099369049072
Epoch: 3, Steps: 194 | Train Loss: 0.9886251 Vali Loss: 0.9461930 Test Loss: 1.4411761
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.9017791
	speed: 0.0189s/iter; left time: 23.7812s
Epoch: 4 cost time: 3.3374054431915283
Epoch: 4, Steps: 194 | Train Loss: 0.9130447 Vali Loss: 1.0293574 Test Loss: 1.4735235
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.9824648
	speed: 0.0251s/iter; left time: 26.6796s
Epoch: 5 cost time: 3.7931346893310547
Epoch: 5, Steps: 194 | Train Loss: 0.8784415 Vali Loss: 1.0507960 Test Loss: 1.5203515
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.8710660
	speed: 0.0179s/iter; left time: 15.6187s
Epoch: 6 cost time: 2.8461568355560303
Epoch: 6, Steps: 194 | Train Loss: 0.8538012 Vali Loss: 1.0883650 Test Loss: 1.5177275
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.8_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.4506617784500122, mae:0.9453269839286804
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.8_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.2239971
	speed: 0.0154s/iter; left time: 28.4296s
Epoch: 1 cost time: 3.2024686336517334
Epoch: 1, Steps: 194 | Train Loss: 1.2156168 Vali Loss: 0.5890463 Test Loss: 1.4031549
Validation loss decreased (inf --> 0.589046).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.2450238
	speed: 0.0173s/iter; left time: 28.5007s
Epoch: 2 cost time: 3.1874704360961914
Epoch: 2, Steps: 194 | Train Loss: 1.1402637 Vali Loss: 0.7010475 Test Loss: 1.3815322
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 1.1443733
	speed: 0.0179s/iter; left time: 26.0662s
Epoch: 3 cost time: 3.1921656131744385
Epoch: 3, Steps: 194 | Train Loss: 1.0098380 Vali Loss: 0.8292020 Test Loss: 1.4039415
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.9683404
	speed: 0.0178s/iter; left time: 22.3521s
Epoch: 4 cost time: 3.290466785430908
Epoch: 4, Steps: 194 | Train Loss: 0.9418824 Vali Loss: 0.8137566 Test Loss: 1.3703150
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 1.0016185
	speed: 0.0164s/iter; left time: 17.5058s
Epoch: 5 cost time: 2.9320030212402344
Epoch: 5, Steps: 194 | Train Loss: 0.9011748 Vali Loss: 0.7868838 Test Loss: 1.3784634
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.8921291
	speed: 0.0144s/iter; left time: 12.5508s
Epoch: 6 cost time: 2.846461057662964
Epoch: 6, Steps: 194 | Train Loss: 0.8849029 Vali Loss: 0.7751890 Test Loss: 1.4013851
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.8_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.403154730796814, mae:0.9266448020935059
Use GPU: cuda:0
no_skip False
add a fuse layer of decoder
>>>>>>>start training : long_term_forecast_gamma0.95_alpha0.8_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456>>>>>>>>>>>>>>>>>>>>>>>>>>
data (10000, 2)
data (10000, 2)
data (10000, 2)
	iters: 100, epoch: 1 | loss: 1.0217906
	speed: 0.0213s/iter; left time: 39.2258s
Epoch: 1 cost time: 3.816483736038208
Epoch: 1, Steps: 194 | Train Loss: 1.2176620 Vali Loss: 0.6160217 Test Loss: 1.4195501
Validation loss decreased (inf --> 0.616022).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 1.0778986
	speed: 0.0266s/iter; left time: 43.7697s
Epoch: 2 cost time: 4.127097845077515
Epoch: 2, Steps: 194 | Train Loss: 1.1625743 Vali Loss: 0.6167843 Test Loss: 1.4067118
EarlyStopping counter: 1 out of 5
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.9266660
	speed: 0.0146s/iter; left time: 21.2798s
Epoch: 3 cost time: 2.571869373321533
Epoch: 3, Steps: 194 | Train Loss: 1.0529532 Vali Loss: 0.7626957 Test Loss: 1.3886096
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.9809989
	speed: 0.0140s/iter; left time: 17.6013s
Epoch: 4 cost time: 2.8854568004608154
Epoch: 4, Steps: 194 | Train Loss: 0.9694420 Vali Loss: 0.7970355 Test Loss: 1.4079088
EarlyStopping counter: 3 out of 5
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.8705407
	speed: 0.0177s/iter; left time: 18.8775s
Epoch: 5 cost time: 3.0546610355377197
Epoch: 5, Steps: 194 | Train Loss: 0.9283974 Vali Loss: 0.8214963 Test Loss: 1.4384952
EarlyStopping counter: 4 out of 5
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 1.0608757
	speed: 0.0157s/iter; left time: 13.6529s
Epoch: 6 cost time: 2.6393418312072754
Epoch: 6, Steps: 194 | Train Loss: 0.9107192 Vali Loss: 0.8026239 Test Loss: 1.4432098
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_gamma0.95_alpha0.8_96_720_iTransformer_custom_ftM_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_noSkipFalse_FDCTrue_conv2d_456<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mse:1.4195499420166016, mae:0.9331536293029785
